{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2019 17:59:16 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/xd/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', '##ae', '##m']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-935e665621ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Mask a token that we will try to predict back with `BertForMaskedLM`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmasked_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'[CLS]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'who'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'was'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'henson'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[SEP]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[MASK]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'was'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'puppet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'##eer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[SEP]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Convert token to vocabulary indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenized input\n",
    "text = \"draem\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tall', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG\n",
    "words = nltk.word_tokenize(\"I don't like the flower.\")\n",
    "word_tag = nltk.pos_tag(['tall'])\n",
    "print(word_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.word_tokenize（text）：对指定的句子进行分词，返回单词列表\n",
    "\n",
    "nltk.pos_tag(words)：对指定的单词列表进行词性标记，返回标记列表\n",
    "\n",
    "CC  coordinating conjunction\n",
    "CD  cardinal digit\n",
    "DT  determiner\n",
    "EX  existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW  foreign word\n",
    "IN  preposition/subordinating conjunction\n",
    "JJ  adjective   'big'\n",
    "JJR adjective, comparative  'bigger'\n",
    "JJS adjective, superlative  'biggest'\n",
    "LS  list marker 1)\n",
    "MD  modal   could, will\n",
    "NN  noun, singular 'desk'\n",
    "NNS noun plural 'desks'\n",
    "NNP proper noun, singular   'Harrison'\n",
    "NNPS    proper noun, plural 'Americans'\n",
    "PDT predeterminer   'all the kids'\n",
    "POS possessive ending   parent's\n",
    "PRP personal pronoun    I, he, she\n",
    "PRP$    possessive pronoun  my, his, hers\n",
    "RB  adverb  very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP  particle    give up\n",
    "TO  to  go 'to' the store.\n",
    "UH  interjection    errrrrrrrm\n",
    "VB  verb, base form take\n",
    "VBD verb, past tense    took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle   taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present  takes\n",
    "WDT wh-determiner   which\n",
    "WP  wh-pronoun  who, what\n",
    "WP$ possessive wh-pronoun   whose\n",
    "WRB wh-abverb   where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "cactus\n",
      "good\n",
      "rock\n",
      "python\n",
      "friendly\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"my\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\"))#pos只能是a，v，r，n\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"friendly\", pos=\"n\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    taller | JJR       \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"taller\")\n",
    "for i in range(0,len(doc)):\n",
    "    print('{: >10} | {: <10}'.format(doc[i].text, doc[i].tag_,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'apples', 'appling', 'appled']\n",
      "putts\n"
     ]
    }
   ],
   "source": [
    "from pattern import en\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE \n",
    "#print (lemma('better','a'))\n",
    "list0 = lexeme('apples')\n",
    "\n",
    "print(list0)\n",
    "#print (lexeme('had'))\n",
    "word = \"give\"\n",
    "#print( conjugate('purred', '3sg'))\n",
    "print (conjugate(verb='putting',tense=PRESENT,person = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verb conjugation\n",
    "The pattern.en module has a lexicon of 8,500 common English verbs and their conjugated forms (infinitive, 3rd singular present, present participle, past and past participle – verbs such as be may have more forms). Some verbs can also be negated, including be, can, do, will, must, have, may, need, dare, ought.\n",
    "\n",
    "conjugate(verb, \n",
    "    tense = PRESENT,        # INFINITIVE, PRESENT, PAST, FUTURE\n",
    "   person = 3,              # 1, 2, 3 or None\n",
    "   number = SINGULAR,       # SG, PL\n",
    "     mood = INDICATIVE,     # INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE\n",
    "   aspect = IMPERFECTIVE,   # IMPERFECTIVE, PERFECTIVE, PROGRESSIVE \n",
    "  negated = False,          # True or False\n",
    "    parse = True)\n",
    "lemma(verb)                 # Base form, e.g., are => be.\n",
    "lexeme(verb)                # List of possible forms: be => is, was, ...\n",
    "tenses(verb)                # List of possible tenses of the given form.\n",
    "The conjugate() function takes the following optional parameters:\n",
    "\n",
    "Tense\tPerson\tNumber\tMood\tAspect\tAlias\tTag\tExample\n",
    "INFINITIVE\tNone\tNone\tNone\tNone\t\"inf\"\tVB\tbe\n",
    "PRESENT\t1\tSG\tINDICATIVE\tIMPERFECTIVE\t\"1sg\"\tVBP\tI am\n",
    "PRESENT\t2\tSG\tINDICATIVE\tIMPERFECTIVE\t\"2sg\"\t ·\tyou are\n",
    "PRESENT\t3\tSG\tINDICATIVE\tIMPERFECTIVE\t\"3sg\"\tVBZ\the is\n",
    "PRESENT\tNone\tPL\tINDICATIVE\tIMPERFECTIVE\t\"pl\"\t ·\tare\n",
    "PRESENT\tNone\tNone\tINDICATIVE\tPROGRESSIVE\t\"part\"\tVBG\tbeing\n",
    " \n",
    "PAST\tNone\tNone\tNone\tNone\t\"p\"\tVBD\twere\n",
    "PAST\t1\tPL\tINDICATIVE\tIMPERFECTIVE\t\"1sgp\"\t ·\tI was\n",
    "PAST\t2\tPL\tINDICATIVE\tIMPERFECTIVE\t\"2sgp\"\t ·\tyou were\n",
    "PAST\t3\tPL\tINDICATIVE\tIMPERFECTIVE\t\"3gp\"\t ·\the was\n",
    "PAST\tNone\tPL\tINDICATIVE\tIMPERFECTIVE\t\"ppl\"\t ·\twere\n",
    "PAST\tNone\tNone\tINDICATIVE\tPROGRESSIVE\t\"ppart\"\tVBN\tbeen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = [[1,2,3],[4,5,6]]\n",
    "\n",
    "torch.tensor(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "article(word, function=INDEFINITE)   # DEFINITE | INDEFINITE,限定性冠词the或者非限定性冠词a/an\n",
    "referenced(word, article=INDEFINITE) # Returns article + word. 返回冠词 + word\n",
    "pluralize(word, pos=NOUN, custom={}, classical=True)\n",
    "singularize(word, pos=NOUN, custom={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a university\n",
      "an\n",
      "suppers\n",
      "supper\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import article,referenced,pluralize, singularize\n",
    "print(referenced('university'))\n",
    "print(article('hour'))\n",
    "\n",
    "print(pluralize('supper'))\n",
    "print(singularize('supper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pattern.en' has no attribute 'adjective'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-719df3f4e5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print( en.is_number(12))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"anxious\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboolean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pattern.en' has no attribute 'adjective'"
     ]
    }
   ],
   "source": [
    "from pattern import en\n",
    "#print( en.is_number(12))\n",
    "print(en.adjective.is_emotion(\"anxious\", boolean=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-382-a7ff61981f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basil'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "from pattern.en import wordnet\n",
    "\n",
    "a = wordnet.synsets('basement')[0]\n",
    "b = wordnet.synsets('base')[0]\n",
    "c = wordnet.synsets('basil')[0]\n",
    "\n",
    "t = path_similarity('basic','base')\n",
    "print(t)\n",
    "print( wordnet.similarity(a, a)) \n",
    "print (wordnet.similarity(a, b))\n",
    "print( wordnet.similarity(a, c))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loudlier\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import comparative, superlative,grade\n",
    " \n",
    "print (comparative('loudly'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        if univ_pos in (NOUN, 'NOUN', 'noun'):\n",
    "            univ_pos = 'noun'\n",
    "        elif univ_pos in (VERB, 'VERB', 'verb'):\n",
    "            univ_pos = 'verb'\n",
    "        elif univ_pos in (ADJ, 'ADJ', 'adj'):\n",
    "            univ_pos = 'adj'\n",
    "        elif univ_pos in (PUNCT, 'PUNCT', 'punct'):\n",
    "            univ_pos = 'punct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tagger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-c3ce8a6cf57c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"I went to countryside with my family.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tagger'"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import Tagger\n",
    "tagger = Tagger(nlp.vocab)\n",
    "doc = nlp(u\"I went to countryside with my family.\")\n",
    "processed = tagger(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I i\n",
      "went go\n",
      "to to\n",
      "countryside countryside\n",
      "with with\n",
      "my my\n",
      "family family\n",
      ". .\n",
      "hurrily hurrily\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "print(nlp(u''))\n",
    "for tok in nlp(u'I went to countryside with my family.'):\n",
    "    print (tok, tok.lemma_)\n",
    "    \n",
    "for tok in nlp(u'He tried his best to run hurrily'):\n",
    "    if tok.text == 'hurrily':\n",
    "        print (tok, tok.lemma_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better 0 \n",
      ". 0 \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "doc = nlp(u\"better.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ducks'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "lemmas = lemmatizer(u'ducks', u'NOUN')\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('summarise', 0.6666666666666666), ('summarises', 0.3333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "word = 'darkment'\n",
    "print (suggest('summeries'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这里包含中文字符.!?\n"
     ]
    }
   ],
   "source": [
    "def C_trans_to_E(string):\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(C_pun,E_pun)}\n",
    "    return string.translate(table)\n",
    "\n",
    "s1 = '这里包含中文字符。！？'\n",
    "s2 = C_trans_to_E(s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauty\n",
      "beauti\n",
      "beauti\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmers=[]\n",
    "stemmers.append(LancasterStemmer()) \n",
    "stemmers.append(SnowballStemmer(\"english\"))\n",
    "stemmers.append(PorterStemmer())\n",
    "for stemmer in stemmers:\n",
    "    word = stemmer.stem(\"beautiful\")\n",
    "    print(word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "useful\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmers=[]\n",
    "stemmers.append(LancasterStemmer()) \n",
    "stemmers.append(SnowballStemmer(\"english\"))\n",
    "stemmers.append(PorterStemmer())\n",
    "\n",
    "def adj_to_adv(word):\n",
    "    suggest_word = None\n",
    "    if(word == \"good\"):\n",
    "        return \"well\"\n",
    "    else:\n",
    "        word_stem = LancasterStemmer().stem(word)\n",
    "        #print(word_stem)\n",
    "        suggest_ = word + 'ly'\n",
    "        #print(suggest_)\n",
    "        suggest_list = suggest(suggest_)\n",
    "        #print(suggest_list)\n",
    "        for word_ in suggest_list:\n",
    "            stem_list = []\n",
    "            #print(word_[0])\n",
    "            for stemmer in stemmers:\n",
    "                stem_list.append(stemmer.stem(word_[0]))\n",
    "                #print(stem_list)\n",
    "            if word_stem in stem_list and word != word_[0]:\n",
    "                suggest_word = word_[0]\n",
    "                break\n",
    "        return suggest_word\n",
    "\n",
    "def adv_to_adj(word):\n",
    "    suggest_word = None\n",
    "    if(word == \"well\"):\n",
    "        return \"good\"    \n",
    "    else:\n",
    "        word_stem = PorterStemmer().stem(word)\n",
    "        #print(\"词根\" + word_stem)\n",
    "        suggest_ = word[:-2]\n",
    "        #print(word)\n",
    "        suggest_list = suggest(suggest_)\n",
    "        #print(suggest_list)\n",
    "        for word_ in suggest_list:\n",
    "            stem_list = []\n",
    "            #print(word_[0])\n",
    "            for stemmer in stemmers:\n",
    "                stem_list.append(stemmer.stem(word_[0]))\n",
    "                #print(stem_list)\n",
    "            if word_stem in stem_list and word != word_[0]:\n",
    "                suggest_word = word_[0]\n",
    "                break\n",
    "        return suggest_word\n",
    "\n",
    "print(adj_to_adv(\"difficult\"))\n",
    "print(adv_to_adj(\"usefully\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c8ba8ac1d827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_word2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_like_word_adj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"angry\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_like_word_adj2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"angry\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_like_word_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"however\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c8ba8ac1d827>\u001b[0m in \u001b[0;36mbuild_like_word_adj\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlist_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlist_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomparative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlist_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuperlative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mword_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_to_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comparative' is not defined"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from pattern.en import suggest\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "#lemmas = lemmatizer(u'best', u'adj')\n",
    "\n",
    "def build_like_word_adj(word): #创建类似形容词列表\n",
    "    list_word = []\n",
    "    lemmas = lemmatizer(word, u'adj')\n",
    "    #print(lemmas)\n",
    "    for i in lemmas:\n",
    "        list_word.append(i)\n",
    "        list_word.append(comparative(i))\n",
    "        list_word.append(superlative(i))\n",
    "        word_adv = adj_to_adv(i)\n",
    "        if word_adv != None:\n",
    "            list_word.append(word_adv)\n",
    "    return list_word\n",
    "\n",
    "def build_like_word_adv(word): #创建类似形容词列表\n",
    "    list_word = []\n",
    "    lemmas = lemmatizer(word, u'adj')\n",
    "    #print(lemmas)\n",
    "    for i in lemmas:\n",
    "        list_word.append(i)\n",
    "        list_word.append(comparative(i))\n",
    "        list_word.append(superlative(i))\n",
    "        word_adj = adv_to_adj(i)\n",
    "        if word_adj != None:\n",
    "            list_word.append(word_adj)\n",
    "    return list_word\n",
    "def build_like_word_adj2(word): #创建类似形容词列表\n",
    "    list_word = []\n",
    "    list_word2 = [] #把比较级最高级带more的放在这里\n",
    "    lemmas = lemmatizer(word, u'adj')\n",
    "    #print(lemmas)\n",
    "    for i in lemmas:\n",
    "        list_word.append(i)\n",
    "        word_er = comparative(i)\n",
    "        if \"more\" in word_er:\n",
    "            list_word2.append(word_er)\n",
    "        else:\n",
    "            list_word.append(word_er)\n",
    "        word_est = superlative(i)\n",
    "        if \"most\" in word_est:\n",
    "            list_word2.append(word_est)\n",
    "        else:\n",
    "            list_word.append(word_est)\n",
    "        word_adv = adj_to_adv(i)\n",
    "        if word_adv != None:\n",
    "            list_word.append(word_adv)\n",
    "    return list_word,list_word2\n",
    "\n",
    "print(build_like_word_adj(\"angry\"))\n",
    "print(build_like_word_adj2(\"angry\"))\n",
    "print(build_like_word_adv(\"however\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 xiaofang -13\n",
      "18 xiaofang -13\n"
     ]
    }
   ],
   "source": [
    "name = 'Tim' #全局变量\n",
    "ids = 130\n",
    "def f1():\n",
    "    age = 18 #局部变量\n",
    "    print(age,name,ids)\n",
    "\n",
    "    \n",
    "def f2():\n",
    "    age=19 #局部变量\n",
    "    global name,ids\n",
    "    name = 'xiaofang'\n",
    "    ids = -13\n",
    "    print(age,name,ids)\n",
    "    f1()\n",
    "\n",
    "\n",
    "f2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2019 16:58:49 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/xd/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import tokenization, BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, BertConfig\n",
    "from examples.extract_features import *\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')#do_lower_case：在标记化时将文本转换为小写。默认= True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-dbac78bb9a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlist_word_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'是错误的key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "list_word_id = []\n",
    "list_word = ['angry', 'angrier', 'angriest', 'angrily']\n",
    "for word in list_word:\n",
    "    try:\n",
    "        list_word_id.append(tokenizer.vocab[word])\n",
    "    except KeyError:\n",
    "        print(word + '是错误的key')\n",
    "print(list_word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = [2, 3]\n",
    "\n",
    "def func():\n",
    "    del b[1]\n",
    "\n",
    "func()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'man', 'is', 'a', 'Chinese', '.', 'He', 'is', \"n't\", 'a', 'bitch', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenize = nltk.word_tokenize\n",
    "text = tokenize(\"The man is a Chinese . He isn't a bitch.\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'xiaofang', 13: 'xiaoheimao'}\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "dictionary.update({1:\"xiaofang\"})\n",
    "dictionary.update({13:\"xiaoheimao\"})\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3、词干化\n",
      "you are best. it is lemmatize test for spacy. I love these books\n",
      "you -PRON- 757862\n",
      "are be 536\n",
      "best best 902\n",
      ". . 453\n",
      "it it 519\n",
      "is is 513\n",
      "lemmatize lemmatize 1138934\n",
      "test test 1877\n",
      "for for 531\n",
      "spacy spacy 857539\n",
      ". . 453\n",
      "I -PRON- 757862\n",
      "love love 949\n",
      "these these 742\n",
      "books book 1300\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3、词干化\")\n",
    "test_doc = nlp(u\"you are best. it is lemmatize test for spacy. I love these books\")\n",
    "print(test_doc)\n",
    "for token in test_doc:\n",
    "    print(token, token.lemma_, token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic basic 0.9999999284398302\n",
      "basic as 0.3952775975639635\n",
      "basic base 0.44361194055627906\n",
      "as basic 0.3952775975639635\n",
      "as as 0.9999999975007859\n",
      "as base 0.3801193503113012\n",
      "base basic 0.44361194055627906\n",
      "base as 0.3801193503113012\n",
      "base base 0.9999999536640364\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()  # make sure to use larger model!\n",
    "tokens = nlp(u'basic as base')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[('increasing', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "from pattern.en import suggest\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "print(d.check(\"cream\"))\n",
    "print(suggest(\"increasing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cream', 0.5384615384615384), ('crew', 0.4230769230769231), ('cret', 0.038461538461538464)]\n",
      "pattern的suggest time cost 0.0010383129119873047 s\n",
      "['rem', 'creme', 'cream', 'cram', 'chem', 'crew', 'c rem', 'Cree', 'Rem', 'crime', 'crimp']\n",
      "enchant的suggest time cost 0.019581079483032227 s\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "import time\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "time_start=time.time()\n",
    "print(suggest(\"crem\"))\n",
    "time_end=time.time()\n",
    "print('pattern的suggest time cost',time_end-time_start,'s')\n",
    "\n",
    "time_start=time.time()\n",
    "print(d.suggest(\"crem\"))\n",
    "time_end=time.time()\n",
    "print('enchant的suggest time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beauteous', 0.5), ('dishy', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "from convert_pos import convert\n",
    " \n",
    "print(convert(\"beauty\", 'n', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02'), Synset('happy.s.04')]\n"
     ]
    }
   ],
   "source": [
    "word = 'happy'\n",
    "from_pos = 'a'\n",
    "synsets = wn.synsets(word, pos=from_pos)\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'monday', 'morning', ',', 'and', 'the', 'writing', 'class', 'had', 'just', 'begin', '.', 'everyone', 'was', 'silent', ',', 'wait', 'to', 'see', 'who', 'would', 'be', 'called', 'upon', 'to', 'read', 'his', 'and', 'her', 'paragraph', 'aloud', '.', 'some', 'of', 'us', 'were', 'confident', 'and', 'eager', 'take', 'part', 'in', 'the', 'class', 'activity', ',', 'others', 'were', 'nervous', 'and', 'anxious', '.', 'i', 'had', 'done', 'myself', 'homework', 'but', 'i', 'was', 'shy', '.', 'i', 'was', 'afraid', 'that', 'to', 'speak', 'in', 'front', 'of', 'a', 'larger', 'group', 'of', 'people', '.', 'at', 'that', 'moment', ',', 'i', 'remembered', 'that', 'my', 'father', 'once', 'said', ',', '``', 'the', 'classroom', 'is', 'a', 'place', 'for', 'learning', 'and', 'that', 'include', 'leaning', 'from', 'textbooks', ',', 'and', 'mistake', 'as', 'well', '.', \"''\", 'immediate', ',', 'i', 'raised', 'my', 'huuuand', '.']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize('It was Monday morning, and the writing class had just begin. Everyone was silent, wait to see who would be called upon to read his and her paragraph aloud. Some of us were confident and eager take part in the class activity, others were nervous and anxious. I had done myself homework but I was shy. I was afraid that to speak in front of a larger group of people. At that moment, I remembered that my father once said, \"The classroom is a place for learning and that include leaning from textbooks, and mistake as well.\" Immediate, I raised my huuuand.'.lower())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook likunlin_草稿.ipynb to python\n",
      "[NbConvertApp] Writing 14963 bytes to likunlin_草稿.py\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    !jupyter nbconvert --to python likunlin_草稿.ipynb\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n"
     ]
    }
   ],
   "source": [
    "def C_trans_to_E(string): #标点符号转换函数\n",
    "    E_pun = u',.!?[]()<>\"\\''\n",
    "    C_pun = u'，。！？【】（）《》“‘'\n",
    "    table= {ord(f):ord(t) for f,t in zip(C_pun,E_pun)}\n",
    "    return string.translate(table)\n",
    "\n",
    "print(C_trans_to_E(\"“\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "print(wn.morphy('taller', \"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_词性(token,tag):\n",
    "    if 'VB' in tag: #如果是动词的各种时态\n",
    "        tag0 = \"v\"\n",
    "    elif \"JJ\" in tag : #形容词\n",
    "        tag0 = \"a\"\n",
    "    elif \"RB\" in tag: #副词\n",
    "        tag0 = \"r\"\n",
    "    elif \"NN\" in tag: #名词\n",
    "        tag0 = \"n\"\n",
    "    else:\n",
    "        return tag\n",
    "    if wn.morphy(token, tag0)==None:\n",
    "        tag = nltk.pos_tag([token])[0][1]\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('countryside', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tokens = ['I','went','to','countryside','.']\n",
    "tokens = ['countryside']\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
