{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import tokenization, BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, BertConfig\n",
    "from examples.extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/10/2019 08:14:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt\n",
      "06/10/2019 08:14:45 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased/\n",
      "06/10/2019 08:14:45 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "args = Args()\n",
    "args.no_cuda = True\n",
    "\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "# BERT_DIR = '/nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/'\n",
    "BERT_DIR = '/nas/pretrain-bert/pretrain-pytorch/bert-base-uncased/'\n",
    "config_file = os.path.join(BERT_DIR, CONFIG_NAME)\n",
    "config = BertConfig.from_json_file(config_file)\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(os.path.join(BERT_DIR, 'vocab.txt'))\n",
    "tokenizer = BertTokenizer.from_pretrained('/nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt')\n",
    "model = BertForPreTraining.from_pretrained(BERT_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_text_to_examples(text):\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    if True:\n",
    "        for line in text:\n",
    "            line = line.strip()\n",
    "            text_a = None\n",
    "            text_b = None\n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "            if m is None:\n",
    "                text_a = line\n",
    "            else:\n",
    "                text_a = m.group(1)\n",
    "                text_b = m.group(2)\n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            unique_id += 1\n",
    "    return examples\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, append_special_tokens=True, replace_mask=True, print_info=False):\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        tokens = []\n",
    "        input_type_ids = []\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            if replace_mask and token == '_':  # XD\n",
    "                token = \"[MASK]\"\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                if replace_mask and token == '_':  # XD\n",
    "                    token = \"[MASK]\"\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(1)\n",
    "            if append_special_tokens:\n",
    "                tokens.append(\"[SEP]\")\n",
    "                input_type_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "#             logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\n",
    "#                 \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=example.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids))\n",
    "    return features\n",
    "\n",
    "def copy_and_mask_feature(feature, masked_tokens=None):\n",
    "    import copy\n",
    "    tokens = feature.tokens\n",
    "    masked_positions = [tokens.index(t) for t in masked_tokens if t in tokens] \\\n",
    "        if masked_tokens is not None else range(len(tokens))\n",
    "    assert len(masked_positions) > 0\n",
    "    masked_feature_copies = []\n",
    "    for masked_pos in masked_positions:\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        feature_copy.input_ids[masked_pos] = tokenizer.vocab[\"[MASK]\"]\n",
    "        masked_feature_copies.append(feature_copy)\n",
    "    return masked_feature_copies, masked_positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lm_probs(tokens, input_ids, probs, topk=5, firstk=20):\n",
    "    def print_pair(token, prob, end_str='', hit_mark=' '):\n",
    "        if i < firstk:\n",
    "            # token = token.replace('</w>', '').replace('\\n', '/n')\n",
    "            print('{}{: >3} | {: <12}'.format(hit_mark, int(round(prob*100)), token), end=end_str)\n",
    "    \n",
    "    ret = None\n",
    "    for i in range(len(tokens)):\n",
    "        ind_ = input_ids[i].item() if input_ids is not None else tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item()\n",
    "        print_pair(tokens[i], prob_, end_str='\\t')\n",
    "        values, indices = probs[i].topk(topk)\n",
    "        top_pairs = []\n",
    "        for j in range(topk):\n",
    "            ind, prob = indices[j].item(), values[j].item()\n",
    "            hit_mark = '*' if ind == ind_ else ' '\n",
    "            token = tokenizer.ids_to_tokens[ind]\n",
    "            print_pair(token, prob, hit_mark=hit_mark, end_str='' if j < topk - 1 else '\\n')\n",
    "            top_pairs.append((token, prob))\n",
    "        if tokens[i] == \"[MASK]\":\n",
    "            ret = top_pairs\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "def show_abnormals(tokens, probs, show_suggestions=False):\n",
    "    def gap2color(gap):\n",
    "        if gap <= 5:\n",
    "            return 'yellow_1'\n",
    "        elif gap <= 10:\n",
    "            return 'orange_1'\n",
    "        else:\n",
    "            return 'red_1'\n",
    "        \n",
    "    def print_token(token, suggestion, gap):\n",
    "        if gap == 0:\n",
    "            print(stylize(token + ' ', colored.fg('white') + colored.bg('black')), end='')\n",
    "        else:\n",
    "            print(stylize(token, colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "            if show_suggestions and gap > 5:\n",
    "                print(stylize('/' + suggestion + ' ', colored.fg('green' if gap > 10 else 'cyan') + colored.bg('black')), end='')\n",
    "            else:\n",
    "                print(stylize(' ', colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "                # print('/' + suggestion, end=' ')\n",
    "            # print('%.2f' % gap, end=' ')\n",
    "        \n",
    "    avg_gap = 0.\n",
    "    for i in range(1, len(tokens) - 1):  # skip first [CLS] and last [SEP]\n",
    "        ind_ = tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item()\n",
    "        top_prob = probs[i].max().item()\n",
    "        top_ind = probs[i].argmax().item()\n",
    "        gap = math.log(top_prob) - math.log(prob_)\n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        print_token(tokens[i], suggestion, gap)\n",
    "        avg_gap += gap\n",
    "    avg_gap /= (len(tokens) - 2)\n",
    "    print()\n",
    "    print(avg_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_cache = {}\n",
    "\n",
    "def analyze_text(text, masked_tokens=None, show_suggestions=False, show_firstk_probs=20):\n",
    "    if text[0] in analyzed_cache:\n",
    "        features, mlm_probs = analyzed_cache[text[0]]\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "        tokens = features[0].tokens\n",
    "    else:\n",
    "        examples = convert_text_to_examples(text)\n",
    "        features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "        if not given_mask or masked_tokens is not None:\n",
    "            assert len(features) == 1\n",
    "            features, masked_positions = copy_and_mask_feature(features[0], masked_tokens=masked_tokens)\n",
    "\n",
    "        input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long)\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_type_ids = input_type_ids.to(device)\n",
    "\n",
    "        mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "        mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "\n",
    "        tokens = features[0].tokens\n",
    "        if not given_mask or masked_tokens is not None:\n",
    "            bsz, seq_len, vocab_size = mlm_probs.size()\n",
    "            assert bsz == len(masked_positions)\n",
    "            # reduced_mlm_probs = torch.Tensor(1, seq_len, vocab_size)\n",
    "            # for i in range(seq_len):\n",
    "            #    reduced_mlm_probs[0, i] = mlm_probs[i, i]\n",
    "            reduced_mlm_probs = torch.Tensor(1, len(masked_positions), vocab_size)\n",
    "            for i, pos in enumerate(masked_positions):\n",
    "                reduced_mlm_probs[0, i] = mlm_probs[i, pos]\n",
    "            mlm_probs = reduced_mlm_probs\n",
    "            tokens = [tokens[i] for i in masked_positions]\n",
    "        \n",
    "        analyzed_cache[text[0]] = (features, mlm_probs)\n",
    "        \n",
    "    top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=show_firstk_probs)\n",
    "    if not given_mask:\n",
    "        show_abnormals(tokens, mlm_probs[0], show_suggestions=show_suggestions)\n",
    "    return top_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 | [CLS]       \t   3 | .              1 | the            1 | ,              1 | )              1 | \"           \n",
      " 100 | \"           \t*100 | \"              0 | '              0 | and            0 | so             0 | did         \n",
      " 100 | is          \t*100 | is             0 | was            0 | does           0 | isn            0 | has         \n",
      "  97 | tom         \t* 97 | tom            2 | he             0 | thomas         0 | you            0 | she         \n",
      " 100 | taller      \t*100 | taller         0 | tall           0 | shorter        0 | height         0 | tallest     \n",
      " 100 | than        \t*100 | than           0 | then           0 | as             0 | that           0 | to          \n",
      " 100 | mary        \t*100 | mary           0 | tom            0 | you            0 | barbara        0 | maria       \n",
      " 100 | ?           \t*100 | ?              0 | .              0 | !              0 | ...            0 | -           \n",
      " 100 | \"           \t*100 | \"              0 | '              0 | !              0 | *              0 | )           \n",
      " 100 | \"           \t*100 | \"              0 | no             0 | '              0 | oh             0 | that        \n",
      " 100 | no          \t*100 | no             0 | yes            0 | nope           0 | yeah           0 | oh          \n",
      " 100 | ,           \t*100 | ,              0 | .              0 | ;              0 | -              0 | no          \n",
      "   0 | [MASK]      \t  80 | tom           10 | he             4 | mary           2 | she            1 | thomas      \n",
      " 100 | is          \t*100 | is             0 | was            0 | does           0 | has            0 | no          \n",
      " 100 | taller      \t*100 | taller         0 | shorter        0 | tall           0 | larger         0 | smaller     \n",
      " 100 | .           \t*100 | .              0 | ;              0 | ,              0 | !              0 | )           \n",
      " 100 | \"           \t*100 | \"              0 | '              0 | .              0 | !              0 | ;           \n",
      "   0 | [SEP]       \t  86 | .              4 | ,              3 | he             2 | \"              1 | she         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tom', 0.7961671352386475),\n",
       " ('he', 0.09765198826789856),\n",
       " ('mary', 0.04068772494792938),\n",
       " ('she', 0.022535543888807297),\n",
       " ('thomas', 0.0058586327359080315)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"_ was the greatest physicist who developed theory of relativity.\"]\n",
    "text = [\"The trophy doesn't fit into the brown suitcase because the _ is too large.\"] # relational adj\n",
    "text = ['\"Is Tom taller than Mary?\" \"No, _ is taller.\"']  # yes/no\n",
    "text = [ \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.\"]  # compare \n",
    "text = ['John is taller/shorter than Mary because/although _ is older/younger.']  # causality\n",
    "text = [\"Jennifer is older than James . Jennifer younger than Robert . _ is the oldest.\"]  # transitive inference\n",
    "\n",
    "analyze_text(text, show_firstk_probs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2heads(attns, tokens, words):\n",
    "    positions = [tokens.index(word) for word in words]\n",
    "\n",
    "    for layer in range(config.num_hidden_layers):\n",
    "        for head in range(config.num_attention_heads):\n",
    "            for pos_indices in [(0, 1), (1, 0)]:\n",
    "                from_pos, to_pos = positions[pos_indices[0]], positions[pos_indices[1]]\n",
    "                if attns[layer][head][from_pos].max(0)[1].item() == to_pos:\n",
    "                    print('Layer %d, head %d: %s -> %s' % (layer, head, tokens[from_pos], tokens[to_pos]), end='\\t')\n",
    "                    print(attns[layer][head][from_pos].topk(5)[0].data)\n",
    "\n",
    "def head2words(attns, tokens, layer, head):\n",
    "    for from_pos in range(len(tokens)):\n",
    "        to_pos = attns[layer][head][from_pos].max(0)[1].item()\n",
    "        from_word, to_word = tokens[from_pos], tokens[to_pos]\n",
    "        if from_word.isalpha() and to_word.isalpha():\n",
    "            print('%s @ %d -> %s @ %d' % (from_word, from_pos, to_word, to_pos), end='\\t')\n",
    "            print(attns[layer][head][from_pos].topk(5)[0].data)\n",
    "      \n",
    "special_tokens = ['[CLS]', '[SEP]']\n",
    "\n",
    "def get_salient_heads(attns, tokens, attn_thld=0.5):\n",
    "    for layer in range(config.num_hidden_layers):\n",
    "        for head in range(config.num_attention_heads):\n",
    "            pos_pairs = []\n",
    "            for from_pos in range(1, len(tokens) - 1):  # skip [CLS] and [SEP]\n",
    "                top_attn, to_pos = attns[layer][head][from_pos].max(0)\n",
    "                top_attn, to_pos = top_attn.item(), to_pos.item()\n",
    "                from_word, to_word = tokens[from_pos], tokens[to_pos]\n",
    "#                 if from_word.isalpha() and to_word.isalpha() and top_attn >= attn_thld:\n",
    "                if abs(from_pos - to_pos) <= 1:\n",
    "#                     print('Layer %d, head %d: %s @ %d -> %s @ %d' % (layer, head, from_word, from_pos, to_word, to_pos), end='\\t')\n",
    "#                     print(attns[layer][head][from_pos].topk(5)[0].data)\n",
    "                    pos_pairs.append((from_pos, to_pos))\n",
    "    \n",
    "            ratio = len(pos_pairs) / (len(tokens) - 2)\n",
    "            if ratio > 0.5:\n",
    "                print(ratio)\n",
    "                for from_pos, to_pos in pos_pairs:\n",
    "                    print('Layer %d, head %d: %s @ %d -> %s @ %d' % (layer, head, tokens[from_pos], from_pos, tokens[to_pos], to_pos), end='\\t')\n",
    "                    print(attns[layer][head][from_pos].topk(5)[0].data)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2019 21:46:20 - INFO - examples.extract_features -   tokens: [CLS] jim laughed because he was so happy . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jim @ 1 -> jim @ 1\ttensor([0.7248, 0.0842, 0.0656, 0.0407, 0.0319], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# text, words = [\"The trophy doesn't fit into the brown suitcase because the it is too large.\"], ['fit', 'large']\n",
    "# text, words = [\"Mary couldn't beat John in the match because he was too strong.\"], ['beat', 'strong']\n",
    "text, words = [\"John is taller than Mary because he is older.\"], ['taller', 'older']\n",
    "# text, words = [\"The red ball is heavier than the blue ball because the red ball is bigger.\"], ['heavier', 'bigger']\n",
    "text, words = [\"Jim laughed because he was so happy.\"], ['cried', 'sad']\n",
    "# text, words = [\"Jim ate the cake quickly because he was so hungry.\"], ['ate', 'hungry']\n",
    "# text, words = [\"Jim drank the juice quickly because he was so thirsty.\"], ['drank', 'thirsty']\n",
    "# text, words = [\"Tom's drawing hangs high. It is above Susan's drawing\"], ['high', 'above']\n",
    "# text, words = [\"Tom's drawing hangs low. It is below Susan's drawing\"], ['low', 'below']\n",
    "# text, words = [\"John is taller than Mary . Mary is shorter than John.\"], ['taller', 'shorter']\n",
    "# text, words = [\"The drawing is above the cabinet. The cabinet is below the drawing\"], ['above', 'below']\n",
    "# text, words = [\"Jim is very thin . He is not fat.\"], ['thin', 'fat']\n",
    "\n",
    "features = convert_examples_to_features(convert_text_to_examples(text), tokenizer, print_info=False)\n",
    "input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long).to(device)\n",
    "input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long).to(device)\n",
    "mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "tokens = features[0].tokens\n",
    "# top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=100)\n",
    "\n",
    "attn_name = 'enc_self_attns'\n",
    "hypo = {attn_name: [model.bert.encoder.layer[i].attention.self.attention_probs[0] for i in range(config.num_hidden_layers)]}\n",
    "key_labels = query_labels = tokens\n",
    "labels_dict = {attn_name: (key_labels, query_labels)}\n",
    "result_tuple = (hypo, config.num_attention_heads, labels_dict)\n",
    "# plot_layer_attn(result_tuple, attn_name=attn_name, layer=10, heads=None)\n",
    "\n",
    "attns = hypo[attn_name]\n",
    "    \n",
    "# words2heads(attns, tokens, words)\n",
    "head2words(attns, tokens, 2, 10)\n",
    "# get_salient_heads(attns, tokens, attn_thld=0.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0，2\t-1\n",
    "0，3\t-1\n",
    "0，10\t+1  动宾\n",
    "1，1\t+1  动介\n",
    "1，4\t-1\n",
    "1，11\t0\n",
    "2，0\t+1**\n",
    "2，6\t0**\n",
    "2，9\t+1**\n",
    "3，5\t-1\n",
    "7，4\t-1\n",
    "11，8\t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = config.hidden_size // config.num_attention_heads\n",
    "layer = 1\n",
    "head = 1 # 2, 3, 10\n",
    "wq = model.bert.encoder.layer[layer].attention.self.query.weight.data.view(-1, config.num_attention_heads, head_size).permute(1, 0, 2)\n",
    "wk = model.bert.encoder.layer[layer].attention.self.key.weight.data.view(-1, config.num_attention_heads, head_size).permute(1, 0, 2)\n",
    "\n",
    "wqk = torch.bmm(wq, wk.transpose(-1, -2))\n",
    "# (wqk * wqk.transpose(-1, -2)).sum((1, 2)) / (wqk * wqk).sum((1, 2))\n",
    "# plt.imshow(wqk[head]*wqk[head])\n",
    "# plt.show()\n",
    "\n",
    "# q = torch.matmul(pos_emb, wq)\n",
    "# k = torch.matmul(pos_emb_prev, wk)\n",
    "# (q * k).sum((-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb = model.bert.embeddings.position_embeddings.weight.data\n",
    "pos_emb_prev = torch.zeros_like(pos_emb)\n",
    "pos_emb_next = torch.zeros_like(pos_emb)\n",
    "pos_emb_prev[1:] = pos_emb[:-1]\n",
    "pos_emb_next[:-1] = pos_emb[1:]\n",
    "pos_emb, pos_emb_prev, pos_emb_next = pos_emb[1:-1], pos_emb_prev[1:-1], pos_emb_next[1:-1]\n",
    "\n",
    "# pos_q = torch.matmul(pos_emb, wk[head])\n",
    "# plt.imshow(pos_q[:32])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.',\n",
       " 'Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    # same / different\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.\",\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.\",\n",
    "    \"Tom has yellow hair. Mary has black hair. John has black hair. Mary and _ have the same hair color.\",\n",
    "    # because / although\n",
    "    \"John is taller/shorter than Mary because/although _ is older/younger.\",\n",
    "    \"The red ball is heavier/lighter than the blue ball because/although the _ ball is bigger/smaller.\",\n",
    "    \"Charles did a lot better/worse than his good friend Nancy on the test because/although _ had/hadn't studied so hard.\",\n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thought that he would arrive earlier than Susan, but/and indeed _ was the first to arrive.\",\n",
    "    # reverse\n",
    "    \"John came then Mary came. They left in reverse order. _ left then _ left.\",\n",
    "    \"John came after Mary. They left in reverse order. _ left after _ .\",\n",
    "    \"John came first, then came Mary. They left in reverse order: _ left first, then left _ .\",\n",
    "    # compare sentences with same / opposite meaning, 2nd order\n",
    "    \"Though John is tall, Tom is taller than John. So John is _ than Tom.\",\n",
    "    \"Tom is taller than John. So _ is shorter than _.\",\n",
    "    # WSC-style: before /after\n",
    "    # \"Mary came before/after John. _ was late/early .\",\n",
    "    # yes / no, 2nd order\n",
    "    \"Was Tom taller than Susan? Yes, _ was taller.\",\n",
    "    # right / wrong, epistemic modality, 2nd order\n",
    "    \"John said/thought that the red ball was heavier than the blue ball. He was wrong. The _ ball was heavier\",\n",
    "    \"John was wrong in saying/thinking that the red ball was heavier than the blue ball. The _ ball was heavier\",\n",
    "    \"John said the rain was about to stop. Mary said the rain would continue. Later the rain stopped. _ was wrong/right.\",\n",
    "    \n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thanked Mary because  _ had given help to _ . \",\n",
    "    \"John felt vindicated/crushed when his longtime rival Mary revealed that _ was the winner of the competition.\",\n",
    "    \"John couldn't see the stage with Mary in front of him because _ is so short/tall.\",\n",
    "    \"Although they ran at about the same speed, John beat Sally because _ had such a bad start.\",\n",
    "    \"The fish ate the worm. The _ was hungry/tasty.\",\n",
    "    \n",
    "    \"John beat Mary. _ won the game/e winner.\",\n",
    "]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_switched_label.json') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_child_problem.json') as f:\n",
    "    cexamples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    for s in ce['sentences']:\n",
    "        for a in s['answer0'] + s['answer1']:\n",
    "            a = a.lower()\n",
    "#             if a not in tokenizer.vocab:\n",
    "#                 ce\n",
    "#                 print(a, 'not in vocab!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    if len(ce['sentences']) > 0:\n",
    "        e = examples[ce['index']]\n",
    "        assert ce['index'] == e['index']\n",
    "        e['score'] = all([s['score'] for s in ce['sentences']])\n",
    "        assert len(set([s['adjacent_ref'] for s in ce['sentences']])) == 1, 'adjcent_refs are different!'\n",
    "        e['adjacent_ref'] = ce['sentences'][0]['adjacent_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for e in examples:\n",
    "    if 'score' in e:\n",
    "        index = e['index']\n",
    "        if index < 252:\n",
    "            if index % 2 == 1:\n",
    "                index -= 1\n",
    "        elif index in [252, 253, 254]:\n",
    "            index = 252\n",
    "        else:\n",
    "            if index % 2 == 0:\n",
    "                index -= 1\n",
    "        groups[index].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  \"The trophy doesn't fit into the brown suitcase because [it] is too large.\",\n",
       "  'fit into:large/small'),\n",
       " (4,\n",
       "  'Joan made sure to thank Susan for all the help [she] had recieved.',\n",
       "  'thank:receive/give'),\n",
       " (10,\n",
       "  'The delivery truck zoomed by the school bus because [it] was going so fast.',\n",
       "  'zoom by:fast/slow'),\n",
       " (12,\n",
       "  'Frank felt vindicated when his longtime rival Bill revealed that [he] was the winner of the competition.',\n",
       "  'vindicated/crushed:be the winner'),\n",
       " (16,\n",
       "  'The large ball crashed right through the table because [it] was made of steel.',\n",
       "  'crash through:[hard]/[soft]'),\n",
       " (18,\n",
       "  \"John couldn't see the stage with Billy in front of him because [he] is so short.\",\n",
       "  '[block]:short/tall'),\n",
       " (20,\n",
       "  'Tom threw his schoolbag down to Ray after [he] reached the top of the stairs.',\n",
       "  'down to:top/bottom'),\n",
       " (22,\n",
       "  'Although they ran at about the same speed, Sue beat Sally because [she] had such a good start.',\n",
       "  'beat:good/bad'),\n",
       " (26,\n",
       "  \"Sam's drawing was hung just above Tina's and [it] did look much better with another one below it.\",\n",
       "  'above/below'),\n",
       " (28,\n",
       "  'Anna did a lot better than her good friend Lucy on the test because [she] had studied so hard.',\n",
       "  'better/worse:study hard'),\n",
       " (30,\n",
       "  'The firemen arrived after the police because [they] were coming from so far away.',\n",
       "  'after/before:far away'),\n",
       " (32,\n",
       "  \"Frank was upset with Tom because the toaster [he] had bought from him didn't work.\",\n",
       "  'be upset with:buy from not work/sell not work'),\n",
       " (36,\n",
       "  'The sack of potatoes had been placed above the bag of flour, so [it] had to be moved first.',\n",
       "  'above/below:moved first'),\n",
       " (38,\n",
       "  'Pete envies Martin although [he] is very successful.',\n",
       "  'although/because'),\n",
       " (42,\n",
       "  'I poured water from the bottle into the cup until [it] was empty.',\n",
       "  'pour:empty/full'),\n",
       " (46,\n",
       "  \"Sid explained his theory to Mark but [he] couldn't convince him.\",\n",
       "  'explain:convince/understand'),\n",
       " (48,\n",
       "  \"Susan knew that Ann's son had been in a car accident, so [she] told her about it.\",\n",
       "  '?know tell:so/because'),\n",
       " (50,\n",
       "  \"Joe's uncle can still beat him at tennis, even though [he] is 30 years younger.\",\n",
       "  'beat:younger/older'),\n",
       " (64,\n",
       "  'In the middle of the outdoor concert, the rain started falling, but [it] continued until 10.',\n",
       "  'but/and'),\n",
       " (68,\n",
       "  'Ann asked Mary what time the library closes, because [she] had forgotten.',\n",
       "  'because/but'),\n",
       " (84,\n",
       "  'If the con artist has succeeded in fooling Sam, [he] would have gotten a lot of money.',\n",
       "  'fool:get/lose'),\n",
       " (92,\n",
       "  'Alice tried frantically to stop her daughter from chatting at the party, leaving us to wonder why [she] was behaving so strangely.',\n",
       "  '?stop normal/stop abnormal:strange'),\n",
       " (98,\n",
       "  \"I was trying to open the lock with the key, but someone had filled the  keyhole with chewing gum, and I couldn't get [it] in.\",\n",
       "  'put ... into filled with ... :get in/get out'),\n",
       " (100,\n",
       "  'The dog chased the cat, which ran up a tree. [It] waited at the bottom.',\n",
       "  'up:at the bottom/at the top'),\n",
       " (106,\n",
       "  'John was doing research in the library when he heard a man humming and  whistling. [He] was very annoyed.',\n",
       "  'hear ... humming and whistling:annoyed/annoying'),\n",
       " (108,\n",
       "  'John was jogging through the park when he saw a man juggling watermelons. [He] was very impressed.',\n",
       "  'see ... juggling watermelons:impressed/impressive'),\n",
       " (132,\n",
       "  'Jane knocked on the door, and Susan answered it. [She] invited her to come out.',\n",
       "  'visit:invite come out/invite come in'),\n",
       " (150,\n",
       "  'Jackson was greatly influenced by Arnold, though [he] lived two centuries later.',\n",
       "  'influence:later/earlier'),\n",
       " (160,\n",
       "  'The actress used to be named Terpsichore, but she changed it to Tina a  few years ago, because she figured [it] was too hard to pronounce.',\n",
       "  'change:hard/easy'),\n",
       " (166,\n",
       "  'Fred is the only man still alive who remembers my great-grandfather. [He] is a remarkable man.',\n",
       "  'alive:is/was'),\n",
       " (170,\n",
       "  \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much  better equipped and ten times larger, [they] were defeated within weeks.\",\n",
       "  'better equipped and large:defeated/victorious'),\n",
       " (186,\n",
       "  'When the sponsors of the bill got to the town hall, they were surprised to find that the room was full of opponents. [They] were very much in the minority.',\n",
       "  'be full of:minority/majority'),\n",
       " (188,\n",
       "  'Everyone really loved the oatmeal cookies; only a few people liked the  chocolate chip cookies. Next time, we should make more of [them] .',\n",
       "  'like over:more/fewer'),\n",
       " (190,\n",
       "  'We had hoped to place copies of our newsletter on all the chairs in the auditorium, but there were simply not enough of [them] .',\n",
       "  'place on all:not enough/too many'),\n",
       " (196,\n",
       "  \"Steve follows Fred's example in everything. [He] admires him hugely.\",\n",
       "  'follow:admire/influence'),\n",
       " (198,\n",
       "  \"The table won't fit through the doorway because [it] is too wide.\",\n",
       "  'fit through:wide/narrow'),\n",
       " (200,\n",
       "  'Grace was happy to trade me her sweater for my jacket. She thinks [it] looks dowdy on her.',\n",
       "  'trade:dowdy/great'),\n",
       " (202,\n",
       "  'John hired Bill to take care of [him] .',\n",
       "  'hire/hire oneself to:take care of'),\n",
       " (204,\n",
       "  'John promised Bill to leave, so an hour later [he] left.',\n",
       "  'promise/order'),\n",
       " (210,\n",
       "  \"Jane knocked on Susan's door but [she] did not get an answer.\",\n",
       "  'knock:get an answer/answer'),\n",
       " (212,\n",
       "  'Joe paid the detective after [he] received the final report on the case.',\n",
       "  'pay:receive/deliver'),\n",
       " (226,\n",
       "  'Bill passed the half-empty plate to John because [he] was full.',\n",
       "  'pass the plate:full/hungry'),\n",
       " (252,\n",
       "  'George got free tickets to the play, but he gave them to Eric, even though [he] was particularly eager to see it.',\n",
       "  'even though/because/not'),\n",
       " (255,\n",
       "  \"Jane gave Joan candy because [she] wasn't hungry.\",\n",
       "  'give:not hungry/hungry'),\n",
       " (259,\n",
       "  'James asked Robert for a favor but [he] was refused.',\n",
       "  'ask for a favor:refuse/be refused`'),\n",
       " (261,\n",
       "  'Kirilov ceded the presidency to Shatov because [he] was less popular.',\n",
       "  'cede:less popular/more popular'),\n",
       " (263,\n",
       "  'Emma did not pass the ball to Janie although [she] saw that she was open.',\n",
       "  'not pass although:see open/open')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_dict(d, keys=['index', 'sentence', 'correct_answer', 'relational_word', 'is_associative', 'score']):\n",
    "    return {k: d[k] for k in d if k in keys}\n",
    "\n",
    "# ([[filter_dict(e) for e in eg] for eg in groups.values() if eg[0]['relational_word'] != 'none' and all([e['score'] for e in eg])])# / len([eg for eg in groups.values() if eg[0]['relational_word'] != 'none'])\n",
    "# [(index, eg[0]['relational_word'], all([e['score'] for e in eg])) for index, eg in groups.items() if eg[0]['relational_word'] != 'none']\n",
    "# len([filter_dict(e) for e in examples if 'score' in e and not e['score'] and e['adjacent_ref']])\n",
    "# for e in examples:\n",
    "#     if e['index'] % 2 == 0:\n",
    "#         print(e['sentence'])\n",
    "[(eg[0]['index'], eg[0]['sentence'], eg[0]['relational_word']) for index, eg in groups.items() if '/' in eg[0]['relational_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['because' in e['sentence'] for e in examples]) + \\\n",
    "sum(['so ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['but ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['though' in e['sentence'] for e in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('WSC_switched_label.json', 'w') as f:\n",
    "#     json.dump(examples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_attn_topk = 3\n",
    "\n",
    "def has_chinese_label(labels):\n",
    "    labels = [label.split('->')[0].strip() for label in labels]\n",
    "    r = sum([len(label) > 1 for label in labels if label not in ['BOS', 'EOS']]) * 1. / (len(labels) - 1)\n",
    "    return 0 < r < 0.5  # r == 0 means empty query labels used in self attention\n",
    "\n",
    "def _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col, color='b'):\n",
    "    assert len(query_labels) == attn.size(0)\n",
    "    assert len(key_labels) == attn.size(1)\n",
    "\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax2 = ax1.twinx()\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    pos = range(nlabels)\n",
    "    \n",
    "    if 'self' in attn_name and col < ncols - 1:\n",
    "        query_labels = ['' for _ in query_labels]\n",
    "\n",
    "    for ax, labels in [(ax1, key_labels), (ax2, query_labels)]:\n",
    "        ax.set_yticks(pos)\n",
    "        if has_chinese_label(labels):\n",
    "            ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "        else:\n",
    "            ax.set_yticklabels(labels)\n",
    "        ax.set_ylim([nlabels - 1, 0])\n",
    "        ax.tick_params(width=0, labelsize='xx-large')\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "#     mask, attn = filter_attn(attn)\n",
    "    for qi in range(attn.size(0)):\n",
    "#         if not mask[qi]:\n",
    "#             continue\n",
    "#         for ki in range(attn.size(1)):\n",
    "        for ki in attn[qi].topk(vis_attn_topk)[1]:\n",
    "            a = attn[qi, ki]\n",
    "            ax1.plot((-1, 1), (ki, qi), color, alpha=a)\n",
    "#     print(attn.mean(dim=0).topk(5)[0])\n",
    "#     ax1.barh(pos, attn.mean(dim=0).data.cpu().numpy())\n",
    "\n",
    "def plot_layer_attn(result_tuple, attn_name='dec_self_attns', layer=0, heads=None):\n",
    "    hypo, nheads, labels_dict = result_tuple\n",
    "    key_labels, query_labels = labels_dict[attn_name]\n",
    "    if heads is None:\n",
    "        heads = range(nheads)\n",
    "    else:\n",
    "        nheads = len(heads)\n",
    "    \n",
    "    stride = 2 if attn_name == 'dec_enc_attns' else 1\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    rcParams['figure.figsize'] = 20, int(round(nlabels * stride * nheads / 8 * 1.0))\n",
    "    \n",
    "    rows = nheads // ncols * stride\n",
    "    fig, axes = plt.subplots(rows, ncols)\n",
    "    \n",
    "    # for head in range(nheads):\n",
    "    for head_i, head in enumerate(heads):\n",
    "        row, col = head_i * stride // ncols, head_i * stride % ncols\n",
    "        ax1 = axes[row, col]\n",
    "        attn = hypo[attn_name][layer][head]\n",
    "        _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col)\n",
    "        if attn_name == 'dec_enc_attns':\n",
    "            col = col + 1\n",
    "            axes[row, col].axis('off')  # next subfig acts as blank place holder\n",
    "    # plt.suptitle('%s with %d heads, Layer %d' % (attn_name, nheads, layer), fontsize=20)\n",
    "    plt.show()  \n",
    "            \n",
    "ncols = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
