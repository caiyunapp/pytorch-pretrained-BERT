{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import tokenization, BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, BertConfig\n",
    "from examples.extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 09:11:27 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/xd/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/16/2019 09:11:27 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/\n",
      "04/16/2019 09:11:27 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "args = Args()\n",
    "args.no_cuda = True\n",
    "\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "BERT_DIR = '/nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/'\n",
    "config_file = os.path.join(BERT_DIR, CONFIG_NAME)\n",
    "config = BertConfig.from_json_file(config_file)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')#do_lower_case：在标记化时将文本转换为小写。默认= True\n",
    "model = BertForPreTraining.from_pretrained(BERT_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13579\n",
      "['i', 'ari', '##ve', 'home', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab['doubts'])\n",
    "print(tokenizer.tokenize(\"I arive home.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertForPreTraining：\n",
    "Outputs:\n",
    "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
    "            sentence classification loss.\n",
    "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
    "            Outputs a tuple comprising\n",
    "            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n",
    "            - the next sentence classification logits of shape [batch_size, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from_pretrained：\n",
    "Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
    "Download and cache the pre-trained model file if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2019 09:34:51 - INFO - examples.extract_features -   tokens: [CLS] i love you . hello everybody . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103, 1045, 2293, 103, 1012, 7592, 103, 1012, 102]\n",
      "[101, 103, 2293, 2017, 103, 7592, 7955, 103, 102]\n",
      "[101, 1045, 103, 2017, 1012, 103, 7955, 1012, 103]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4f0a1b5a596b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mmasked_feature_copies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_and_mask_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_feature_copies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#结果[101, 1045, 2293, 103, 102]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def convert_text_to_examples(text): #把每一行的句子变成一个实例，一个实例中包含text_a,text_b(text_b目前是没用的)\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    if True:\n",
    "        for line in text:\n",
    "            line = line.strip()\n",
    "            text_a = None\n",
    "            text_b = None\n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line) #想要匹配这样的字符串'You are my sunshine. ||| I love you.'\n",
    "            \n",
    "            if m is None:\n",
    "                text_a = line\n",
    "            else:\n",
    "                text_a = m.group(1) #匹配的第一句,比如You are my sunshine,my only sunshine.\n",
    "                text_b = m.group(2) #匹配的第二句，比如I love you.\n",
    "            \n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            unique_id += 1\n",
    "    return examples\n",
    "#疑问，当text是一行的时候，line是一个个字母 -> text是[\"***\"]的形式\n",
    "#print(convert_text_to_examples({\"I love you.\",\"hello everybody.\"})[0].text_a)\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, append_special_tokens=True, replace_mask=True, print_info=False):\n",
    "    #把实例变成一个特征\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a) #tokenizer的作用是\n",
    "        #print(example.unique_id) #*****************************\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        tokens = []\n",
    "        input_type_ids = [] #segment embedding\n",
    "        if append_special_tokens: #输入参数中默认为true\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            if replace_mask and token == '_':  # XD\n",
    "                token = \"[MASK]\"\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                if replace_mask and token == '_':  # XD\n",
    "                    token = \"[MASK]\"\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(1)\n",
    "            if append_special_tokens:\n",
    "                tokens.append(\"[SEP]\")\n",
    "                input_type_ids.append(1)\n",
    "        #print(tokens) #*******************************\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) #把原来句子中的词语编成在字典中的编号\n",
    "        input_mask = [1] * len(input_ids) \n",
    "        #print(input_ids)#***********************************\n",
    "        if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "#             logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\n",
    "#                 \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=example.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,#字符串中的每个单词在词典中的index序列\n",
    "                input_mask=input_mask, #一堆1\n",
    "                input_type_ids=input_type_ids)) #第0类和第1类，对text_a,text_b的区分，本代码中全都是零\n",
    "    return features\n",
    "                \n",
    "\n",
    "\n",
    "def copy_and_mask_feature(feature, step, masked_tokens=None): #step参数用来表示每多少个单词mask一次\n",
    "    import copy\n",
    "    tokens = feature.tokens\n",
    "    len_token = len(tokens)\n",
    "    if len_token<step:\n",
    "        batches = range(0,len(tokens))\n",
    "    else:\n",
    "        batches = range(0,step)\n",
    "    \n",
    "    assert len_token > 0\n",
    "    masked_feature_copies = []\n",
    "    for i in batches: #用[mask]依次掩盖每一个位置\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        masked_pos = i\n",
    "        while masked_pos < len_token:\n",
    "            feature_copy.input_ids[masked_pos] = tokenizer.vocab[\"[MASK]\"]\n",
    "            masked_pos = masked_pos + step\n",
    "        masked_feature_copies.append(feature_copy)\n",
    "    return masked_feature_copies, batches\n",
    "\n",
    "#examples = convert_text_to_examples({\"I love you.Hello everybody.\"})\n",
    "#features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "#masked_feature_copies, batches = copy_and_mask_feature(features[0],3)\n",
    "#for i in range(0,5):\n",
    "#    print(masked_feature_copies[i].input_ids) #结果[101, 1045, 2293, 103, 102]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG\n",
    "\n",
    "def show_lm_probs(tokens, input_ids, probs, topk=5, firstk=20): #输出结果的函数，要最高概率topk个输出\n",
    "    def print_pair(token, prob, end_str='', hit_mark=' '):\n",
    "        if i < firstk:\n",
    "            # token = token.replace('</w>', '').replace('\\n', '/n')\n",
    "            print('{}{: >3} | {: <12}'.format(hit_mark, int(round(prob*100)), token), end=end_str)\n",
    "    \n",
    "    ret = None\n",
    "    for i in range(len(tokens)):\n",
    "        ind_ = input_ids[i].item() if input_ids is not None else tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item() #这个probs是该字符串第i个位置上填上词典上各个词的概率，prob_是词典上原来天的这个词的概率\n",
    "        print_pair(tokens[i], prob_, end_str='\\t')\n",
    "        values, indices = probs[i].topk(topk)\n",
    "        #print(values, indices)\n",
    "        #print(\"****************************************************************************************************\")\n",
    "        top_pairs = []\n",
    "        for j in range(topk):\n",
    "            ind, prob = indices[j].item(), values[j].item()\n",
    "            hit_mark = '*' if ind == ind_ else ' '\n",
    "            token = tokenizer.ids_to_tokens[ind]\n",
    "            print_pair(token, prob, hit_mark=hit_mark, end_str='' if j < topk - 1 else '\\n')\n",
    "            top_pairs.append((token, prob))\n",
    "        if tokens[i] == \"[MASK]\":\n",
    "            ret = top_pairs\n",
    "    return ret #返回的这是个啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "def show_abnormals(tokens, probs, show_suggestions=False):\n",
    "    def gap2color(gap):\n",
    "        if gap <= 5:\n",
    "            return 'yellow_1'\n",
    "        elif gap <= 10:\n",
    "            return 'orange_1'\n",
    "        else:\n",
    "            return 'red_1'\n",
    "        \n",
    "    def print_token(token, suggestion, gap):\n",
    "        if gap == 0:\n",
    "            print(stylize(token + ' ', colored.fg('white') + colored.bg('black')), end='')\n",
    "        else:\n",
    "            print(stylize(token, colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "            if show_suggestions and gap > 5:\n",
    "                print(stylize('/' + suggestion + ' ', colored.fg('green' if gap > 10 else 'cyan') + colored.bg('black')), end='')\n",
    "            else:\n",
    "                print(stylize(' ', colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "                # print('/' + suggestion, end=' ')\n",
    "            # print('%.2f' % gap, end=' ')\n",
    "        \n",
    "    avg_gap = 0.\n",
    "    for i in range(1, len(tokens) - 1):  # skip first [CLS] and last [SEP]\n",
    "        ind_ = tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item()\n",
    "        top_prob = probs[i].max().item()\n",
    "        top_ind = probs[i].argmax().item()\n",
    "        gap = math.log(top_prob) - math.log(prob_) #计算两个词之间的差距\n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        print_token(tokens[i], suggestion, gap)\n",
    "        avg_gap += gap\n",
    "    avg_gap /= (len(tokens) - 2)\n",
    "    print()\n",
    "    print('平均gap:'+ str(avg_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'last', 'week', 'i', 'went', 'to', 'the', 'theatre', '.', 'i', 'had', 'a', 'very', 'good', 'seat', '.', 'the', 'play', 'was', 'very', 'interesting', '.', '[SEP]']\n",
      "[[101, 2197, 2733, 1045, 2253, 2000, 1996, 3004, 1012, 102], [101, 1045, 2018, 1037, 2200, 2204, 2835, 1012, 102], [101, 1996, 2377, 2001, 2200, 5875, 1012, 102]]\n",
      "[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "['Last week I went to the theatre.', ' I had a very good seat.', ' The play was very interesting.']\n"
     ]
    }
   ],
   "source": [
    "analyzed_cache = {}\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG\n",
    "#print (lemma('gave'))\n",
    "#print (lexeme('production'))\n",
    "#print (conjugate(verb='give',tense=PRESENT,number=SG))\n",
    "def process_text(text): \n",
    "#处理输入文本，包括将文本按句子分成若干token，得出原来text中index位置的单词在x句子的y位置，还得出各个句子类别码\n",
    "    token =[]\n",
    "    token0 = tokenizer.tokenize(text)\n",
    "    token.append('[CLS]')\n",
    "    for i in token0:\n",
    "        token.append(i)\n",
    "    token.append('[SEP]')\n",
    "    print(token)\n",
    "    in_sentence = [[0,0]] \n",
    "    sentence_n = 0\n",
    "    index = 1\n",
    "    for i in range(1,len(token)-1):\n",
    "        in_sentence.append([sentence_n,index])  #每个token中的词在所在句中的位置表示出来，以及该位置在哪一句中\n",
    "        index = index + 1                           #比如，位置i这个词在第sentence句的index位置上\n",
    "        if token[i] == '.':\n",
    "            sentence_n = sentence_n + 1\n",
    "            index = 1\n",
    "    sentences = text.split(\".\")\n",
    "    sentences.remove('')\n",
    "\n",
    "    sen_token = []\n",
    "    input_ids_sen = []\n",
    "    input_type_ids_sen = []\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentence = sentence + '.'\n",
    "        sentences[i] = sentences[i] + '.'\n",
    "        token = []\n",
    "        input_type_ids = []\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token.append('[CLS]')\n",
    "        input_type_ids.append(0) \n",
    "        for i in tokens:\n",
    "            token.append(i)\n",
    "            input_type_ids.append(0)        \n",
    "        token.append('[SEP]')        \n",
    "        input_type_ids.append(0)\n",
    "        input_ids_sen.append(tokenizer.convert_tokens_to_ids(token))\n",
    "        input_type_ids_sen.append(input_type_ids)\n",
    "    #input_ids_sen = torch.tensor(input_ids_sen)\n",
    "    #input_type_ids_sen = torch.tensor(input_type_ids_sen)\n",
    "    return input_ids_sen,input_type_ids_sen,in_sentence,sentences\n",
    "text = \"Last week I went to the theatre. I had a very good seat. The play was very interesting.\"\n",
    "input_ids_sen,input_type_ids_sen,in_sentence,sentences = process_text(text)\n",
    "print(input_ids_sen)\n",
    "print(in_sentence)\n",
    "print(input_type_ids_sen)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数是在该位置上的单词可能性很低时才使用，不会把原来就较为合理的面目全非"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否用不定式：\n",
      "用to的可能性0.00036304089007899165\n",
      "可能性最大的词概率0.23709583282470703\n",
      "是否用被动或进行时：\n",
      "[('was', 0.0002793713065329939), ('am', 4.049863855470903e-05), ('were', 1.306664398725843e-05), ('been', 4.840642304770881e-06), ('be', 1.453689151276194e-06), ('are', 7.996850399649702e-07), ('is', 5.958298174846277e-07), ('being', 9.706550230248467e-09)]\n",
      "had 0.8573063611984253\n",
      "was 0.0002793713065329939\n",
      "不是被动\n",
      "[('was', 0.9590925574302673), ('am', 0.006898669525980949), ('were', 0.0016424404457211494), ('been', 0.0004373548727016896), ('is', 0.00035717932041734457), ('be', 3.4134478482883424e-05), ('are', 2.2988733689999208e-05), ('being', 3.1775894626662193e-07)]\n",
      "was 0.9590925574302673\n",
      "was 0.9590925574302673\n",
      "判断其他语法：\n",
      "need_be == 1\n",
      "['go', 'goes', 'going', 'went', 'gone']\n",
      "[2175, 3632, 2183, 2253, 2908]\n",
      "{'go': 0.00043932811240665615, 'goes': 0.00012179886834928766, 'going': 0.6597349047660828, 'went': 0.00122930109500885, 'gone': 0.002755501540377736}\n",
      "going\n",
      "was going\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'was going'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE\n",
    "\n",
    "def analyse_V(index):\n",
    "#这是一个处理动词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "\n",
    "#******************************************初始数据处理**************************************************************************\n",
    "    need_to = 0 #表示是否需要变为不定式形式，0表示不需要，1表示需要\n",
    "    need_be = 0 #表示是否需要变为被动语态0表示不需要，1表示需要\n",
    "    \n",
    "    sentence_id = in_sentence[index][0]\n",
    "    id_in_sen = in_sentence[index][1]\n",
    "    wordV = input_ids_sen[sentence_id][id_in_sen]\n",
    "    wordV = tokenizer.ids_to_tokens[wordV]\n",
    "    \n",
    "    input_ids = copy.deepcopy(input_ids_sen[sentence_id])\n",
    "    input_type_ids = copy.deepcopy(input_type_ids_sen[sentence_id])\n",
    "#*****************************************判断语法应不应该是不定式抑或是被动语态**************************************************************\n",
    "    '''\n",
    "    input_ids1 = copy.deepcopy(input_ids)\n",
    "    input_ids1.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "    input_type_ids1 = copy.deepcopy(input_type_ids)\n",
    "    input_type_ids1.append(0)\n",
    "    \n",
    "    T_input_ids1 = torch.tensor([input_ids1], dtype=torch.long) #把input_ids增加了一个维度\n",
    "    T_input_ids1 = T_input_ids1.to(device) #拿去GPU\n",
    "\n",
    "    T_input_type_ids1 = torch.tensor([input_type_ids1], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    T_input_type_ids1 = T_input_type_ids1.to(device)    \n",
    "    \n",
    "    mlm_logits1, _ = model(T_input_ids1, T_input_type_ids1)\n",
    "    mlm_probs1 = F.softmax(mlm_logits1, dim=-1)\n",
    "    reduced_mlm_probs1 = mlm_probs1[0][id_in_sen]\n",
    "    '''\n",
    "#**************************************判断是不是不定式*********************  \n",
    "    input_ids1 = copy.deepcopy(input_ids)\n",
    "    input_ids1.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "    input_ids1[id_in_sen + 1]=tokenizer.vocab[conjugate(verb=wordV,tense=PRESENT,person = 1)]\n",
    "    input_type_ids1 = copy.deepcopy(input_type_ids)\n",
    "    input_type_ids1.append(0)\n",
    "    \n",
    "    T_input_ids1 = torch.tensor([input_ids1], dtype=torch.long) #把input_ids增加了一个维度\n",
    "    T_input_ids1 = T_input_ids1.to(device) #拿去GPU\n",
    "\n",
    "    T_input_type_ids1 = torch.tensor([input_type_ids1], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    T_input_type_ids1 = T_input_type_ids1.to(device)    \n",
    "    \n",
    "    mlm_logits1, _ = model(T_input_ids1, T_input_type_ids1)\n",
    "    mlm_probs1 = F.softmax(mlm_logits1, dim=-1)\n",
    "    reduced_mlm_probs1 = mlm_probs1[0][id_in_sen]\n",
    "    \n",
    "    prob_to = float(reduced_mlm_probs1[tokenizer.vocab[\"to\"]])\n",
    "    top_prob1 = reduced_mlm_probs1.max().item()\n",
    "    print(\"是否用不定式：\")\n",
    "    print(\"用to的可能性\"+str(prob_to))\n",
    "    print(\"可能性最大的词概率\"+str(top_prob1))\n",
    "    gap1 = math.log(top_prob1) - math.log(prob_to)\n",
    "    if gap1 < 1:\n",
    "        need_to = 1 \n",
    "#**************************************判断是不是被动语态或者进行时*******************   \n",
    "    print(\"是否用被动或进行时：\")\n",
    "    input_ids3 = copy.deepcopy(input_ids)\n",
    "    input_ids3.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "    input_ids3_ = copy.deepcopy(input_ids3)\n",
    "    input_ids3[id_in_sen + 1]=tokenizer.vocab[conjugate(verb=wordV,tense=PAST,aspect=PROGRESSIVE)]\n",
    "    input_ids3_[id_in_sen + 1]=tokenizer.vocab[conjugate(verb=wordV,tense=PRESENT,aspect=PROGRESSIVE)]\n",
    "    input_type_ids3 = copy.deepcopy(input_type_ids)\n",
    "    input_type_ids3.append(0)\n",
    "    \n",
    "    T_input_ids3 = torch.tensor([input_ids3], dtype=torch.long) #把input_ids增加了一个维度\n",
    "    T_input_ids3 = T_input_ids3.to(device) #拿去GPU\n",
    "    T_input_ids3_ = torch.tensor([input_ids3_], dtype=torch.long)\n",
    "    T_input_ids3_ = T_input_ids3_.to(device)\n",
    "\n",
    "    T_input_type_ids3 = torch.tensor([input_type_ids3], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    T_input_type_ids3 = T_input_type_ids3.to(device)    \n",
    "    \n",
    "    mlm_logits3, _ = model(T_input_ids3, T_input_type_ids3)\n",
    "    mlm_logits3_,_ = model(T_input_ids3_, T_input_type_ids3)\n",
    "    mlm_probs3 = F.softmax(mlm_logits3, dim=-1)\n",
    "    reduced_mlm_probs3 = mlm_probs3[0][id_in_sen]\n",
    "    mlm_probs3_= F.softmax(mlm_logits3_, dim=-1)\n",
    "    reduced_mlm_probs3_ = mlm_probs3_[0][id_in_sen]\n",
    "    \n",
    "    list_be = lexeme('be')\n",
    "    list_be = lexeme('be')[:8]\n",
    "\n",
    "    list_be_id = tokenizer.convert_tokens_to_ids(list_be)\n",
    "    list_be_prob = {}\n",
    "    for word,word_id in zip(list_be,list_be_id):\n",
    "        list_be_prob.update({word:float(reduced_mlm_probs3[word_id].data)})\n",
    "    prob_ord3 = sorted(list_be_prob.items(),key = lambda x:x[1],reverse = True)\n",
    "    print(prob_ord3)\n",
    "    top_ind3 = reduced_mlm_probs3.argmax().item()\n",
    "    top_prob3 = reduced_mlm_probs3.max().item()\n",
    "    print(tokenizer.ids_to_tokens[top_ind3],top_prob3)\n",
    "    print(prob_ord3[0][0],prob_ord3[0][1])\n",
    "    top_prob_be = prob_ord3[0][1]\n",
    "    gap3 = math.log(top_prob3) - math.log(top_prob_be)\n",
    "    if gap3 < 1:\n",
    "        need_be = 1 \n",
    "        be_ = prob_ord3[0][0]\n",
    "    else:\n",
    "        print('不是被动')\n",
    "#*******************************************是不是现在分词********************************        \n",
    "    list_be_prob = {}\n",
    "    for word,word_id in zip(list_be,list_be_id):\n",
    "        list_be_prob.update({word:float(reduced_mlm_probs3_[word_id].data)})\n",
    "    prob_ord3 = sorted(list_be_prob.items(),key = lambda x:x[1],reverse = True)\n",
    "    print(prob_ord3)\n",
    "    top_ind3 = reduced_mlm_probs3_.argmax().item()\n",
    "    top_prob3 = reduced_mlm_probs3_.max().item()\n",
    "    print(tokenizer.ids_to_tokens[top_ind3],top_prob3)\n",
    "    print(prob_ord3[0][0],prob_ord3[0][1])\n",
    "    top_prob_be = prob_ord3[0][1]\n",
    "    gap3 = math.log(top_prob3) - math.log(top_prob_be)\n",
    "    if gap3 < 1:\n",
    "        need_be = 1 \n",
    "        be_ = prob_ord3[0][0]    \n",
    "#*************************************************判断其他语法******************************************************************\n",
    "    print(\"判断其他语法：\")\n",
    "    if need_to == 0 and need_be == 0:\n",
    "        input_ids[id_in_sen] = tokenizer.vocab[\"[MASK]\"]\n",
    "        input_type_ids = copy.deepcopy(input_type_ids_sen[sentence_id])\n",
    "\n",
    "        T_input_ids = torch.tensor([input_ids], dtype=torch.long) #把input_ids增加了一个维度\n",
    "        T_input_type_ids = torch.tensor([input_type_ids], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "        T_input_ids = T_input_ids.to(device) #拿去GPU\n",
    "        T_input_type_ids = T_input_type_ids.to(device)\n",
    "\n",
    "        mlm_logits, _ = model(T_input_ids, T_input_type_ids)\n",
    "        mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "        reduced_mlm_probs = mlm_probs[0][id_in_sen]\n",
    "\n",
    "        list_word = lexeme(wordV)\n",
    "        #list_word = [word]\n",
    "\n",
    "        list_word_id = tokenizer.convert_tokens_to_ids(list_word)\n",
    "        print(list_word)\n",
    "        print(list_word_id)    \n",
    "        list_word_prob = {}\n",
    "        for word,word_id in zip(list_word,list_word_id):\n",
    "            list_word_prob.update({word:float(reduced_mlm_probs[word_id].data)})\n",
    "        print(list_word_prob)\n",
    "        prob_ord = sorted(list_word_prob.items(),key = lambda x:x[1],reverse = True)\n",
    "\n",
    "        top_ind = reduced_mlm_probs.argmax().item()\n",
    "        top_prob = reduced_mlm_probs.max().item()\n",
    "        top_prob_thisV = prob_ord[0][1]\n",
    "        gap = math.log(top_prob) - math.log(top_prob_thisV)\n",
    "        \n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        sentence = copy.deepcopy(sentences[sentence_id])\n",
    "        sentence = tokenizer.tokenize(sentence)\n",
    "        sentence[id_in_sen - 1] = suggestion\n",
    "        sentence_tag = nltk.pos_tag(sentence)\n",
    "        \n",
    "        suggestion_tag = sentence_tag[id_in_sen - 1][1]\n",
    "        #print(sentence_tag[id_in_sen - 1][0])\n",
    "        print(suggestion_tag)\n",
    "        \n",
    "        if gap < 5 or suggestion_tag.find(\"V\")==-1:\n",
    "            suggestion = prob_ord[0][0]\n",
    "        \n",
    "            \n",
    "\n",
    "        \"\"\"”values, indices = reduced_mlm_probs.topk(topk)\n",
    "        for j in range(topk):\n",
    "            ind, prob = indices[j].item(), values[j].item()\n",
    "            \n",
    "            token = tokenizer.ids_to_tokens[ind]\n",
    "            print(token,prob)\"\"\"\n",
    "    elif need_to == 1:\n",
    "        input_ids2 = copy.deepcopy(input_ids)\n",
    "        input_ids2.insert(id_in_sen,tokenizer.vocab[\"to\"])\n",
    "        input_ids2[id_in_sen + 1] = tokenizer.vocab[\"[MASK]\"]\n",
    "        T_input_ids2 = torch.tensor([input_ids2], dtype=torch.long) #把input_ids增加了一个维度\n",
    "        T_input_ids2 = T_input_ids2.to(device) #拿去GPU\n",
    "        \n",
    "        input_type_ids2 = copy.deepcopy(input_type_ids1)\n",
    "        T_input_type_ids2 = torch.tensor([input_type_ids2], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "        T_input_type_ids2 = T_input_type_ids2.to(device)   \n",
    "        mlm_logits2, _ = model(T_input_ids2, T_input_type_ids2)\n",
    "        mlm_probs2 = F.softmax(mlm_logits2, dim=-1)\n",
    "        reduced_mlm_probs2 = mlm_probs2[0][id_in_sen + 1]\n",
    "        \n",
    "        thisV = conjugate(verb = wordV,tense=PRESENT,person = 1)\n",
    "        print(thisV)\n",
    "        #list_word = [wordV]\n",
    "        thisV_id = tokenizer.vocab[thisV]\n",
    "   \n",
    "        top_ind2 = reduced_mlm_probs2.argmax().item()\n",
    "        top_prob2 = reduced_mlm_probs2.max().item()\n",
    "        prob_thisV2 = reduced_mlm_probs2[thisV_id]\n",
    "        gap = math.log(top_prob2) - math.log(prob_thisV2)\n",
    "        \n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind2]\n",
    "        sentence = copy.deepcopy(sentences[sentence_id])\n",
    "        sentence = tokenizer.tokenize(sentence)\n",
    "        sentence.insert(id_in_sen - 1,'to')\n",
    "        sentence[id_in_sen] = suggestion\n",
    "        print(\"sentence是：\",sentence)\n",
    "        sentence_tag = nltk.pos_tag(sentence)\n",
    "        \n",
    "        suggestion_tag = sentence_tag[id_in_sen][1]\n",
    "        if gap < 5 or suggestion_tag.find(\"V\")== -1:\n",
    "            suggestion = 'to '+ thisV\n",
    "        else:\n",
    "            suggestion = 'to '+ tokenizer.ids_to_tokens[top_ind2]\n",
    "    elif need_be == 1:#********************************处理需要be动词的时态*****************\n",
    "        print(\"need_be == 1\")\n",
    "        input_ids3 = copy.deepcopy(input_ids1)\n",
    "        input_ids3[id_in_sen] = tokenizer.vocab[be_]\n",
    "        input_ids3[id_in_sen + 1] = tokenizer.vocab[\"[MASK]\"]\n",
    "        T_input_ids3 = torch.tensor([input_ids3], dtype=torch.long) #把input_ids增加了一个维度\n",
    "        T_input_ids3 = T_input_ids3.to(device) #拿去GPU\n",
    "        \n",
    "        input_type_ids3 = copy.deepcopy(input_type_ids1)\n",
    "        T_input_type_ids3 = torch.tensor([input_type_ids3], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "        T_input_type_ids3 = T_input_type_ids3.to(device)\n",
    "        mlm_logits3, _ = model(T_input_ids3, T_input_type_ids3)\n",
    "        mlm_probs3 = F.softmax(mlm_logits3, dim=-1)\n",
    "        reduced_mlm_probs3 = mlm_probs3[0][id_in_sen + 1]\n",
    "        \n",
    "        list_word3 = lexeme(wordV)\n",
    "        #list_word = [wordV]\n",
    "        list_word_id3 = tokenizer.convert_tokens_to_ids(list_word3)\n",
    "        print(list_word3)\n",
    "        print(list_word_id3)    \n",
    "        list_word_prob3 = {}\n",
    "        for word,word_id in zip(list_word3,list_word_id3):\n",
    "            list_word_prob3.update({word:float(reduced_mlm_probs3[word_id].data)})\n",
    "        print(list_word_prob3)\n",
    "        prob_ord3 = sorted(list_word_prob3.items(),key = lambda x:x[1],reverse = True)\n",
    "\n",
    "        top_ind3 = reduced_mlm_probs3.argmax().item()\n",
    "        top_prob3 = reduced_mlm_probs3.max().item()\n",
    "        top_prob_thisV3 = prob_ord3[0][1]\n",
    "        gap = math.log(top_prob3) - math.log(top_prob_thisV3)\n",
    "        print(tokenizer.ids_to_tokens[top_ind3])\n",
    "        \n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind3]\n",
    "        sentence = copy.deepcopy(sentences[sentence_id])\n",
    "        sentence = tokenizer.tokenize(sentence)\n",
    "        sentence.insert(id_in_sen -1,be_)\n",
    "        sentence[id_in_sen] = suggestion\n",
    "        #print(\"sentence是：\",sentence)\n",
    "        sentence_tag = nltk.pos_tag(sentence)\n",
    "        \n",
    "        suggestion_tag = sentence_tag[id_in_sen][1]\n",
    "        if gap < 5 or suggestion_tag.find(\"VB\")== -1:\n",
    "            suggestion = be_ + ' ' + prob_ord3[0][0]\n",
    "        else:\n",
    "            suggestion = be_ + ' ' + tokenizer.ids_to_tokens[top_ind3]\n",
    "    print(suggestion)\n",
    "    return suggestion\n",
    "    \n",
    "analyse_V(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-17-ed6919504531>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-ed6919504531>\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    T_input_ids = torch.tensor([input_ids], dtype=torch.long) #把input_ids增加了一个维度\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import article,referenced,pluralize, singularize\n",
    "def analyse_N(index):\n",
    "#******************************************初始数据处理**************************************************************************\n",
    "    need_DT = 0 #表示是否需要在前面加冠词\n",
    "    prob_N = 0 #表示这个名词的单复数中最高的概率    \n",
    "    sentence_id = in_sentence[index][0]\n",
    "    id_in_sen = in_sentence[index][1]\n",
    "    wordN = input_ids_sen[sentence_id][id_in_sen]\n",
    "    wordN = tokenizer.ids_to_tokens[wordN]\n",
    "    \n",
    "    input_ids = copy.deepcopy(input_ids_sen[sentence_id])\n",
    "    input_type_ids = copy.deepcopy(input_type_ids_sen[sentence_id])\n",
    "#*****************************************若一个词有问题*************************************************************************    \n",
    "    input_ids[id_in_sen] = tokenizer.vocab[\"[MASK]\"]\n",
    "    input_type_ids = copy.deepcopy(input_type_ids_sen[sentence_id])\n",
    "\n",
    "    T_input_ids = torch.tensor([input_ids], dtype=torch.long) #把input_ids增加了一个维度\n",
    "    T_input_type_ids = torch.tensor([input_type_ids], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    T_input_ids = T_input_ids.to(device) #拿去GPU\n",
    "    T_input_type_ids = T_input_type_ids.to(device)\n",
    "\n",
    "    mlm_logits, _ = model(T_input_ids, T_input_type_ids)\n",
    "    mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "    reduced_mlm_probs = mlm_probs[0][id_in_sen]\n",
    "    \n",
    "    N_ = singularize(wordN)\n",
    "    N_s= pluralize(wordN)\n",
    "    N_id = tokenizer.vocab[N_]\n",
    "    N_s_id = tokenizer.vocab[N_s]\n",
    "    if(reduced_mlm_probs[N_id] > reduced_mlm_probs[N_s_id]):\n",
    "        suggestion = N_\n",
    "        prob_N = reduced_mlm_probs[N_id]\n",
    "    else:\n",
    "        suggestion = N_s\n",
    "        prob_N = reduced_mlm_probs[N_s_id]\n",
    "    \n",
    "    top_ind = reduced_mlm_probs.argmax().item()\n",
    "    top_prob = reduced_mlm_probs.max().item()\n",
    "    \n",
    "    gap = math.log(top_prob)- math.log(prob_N)\n",
    "    if gap > 6.5: #我觉得代词的阈值应该回比名词小一点\n",
    "        need_DT = 1 #不见棺材不落泪，认为缺冠词 \n",
    "        \n",
    "    \n",
    "    input_ids.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "    input_ids.insert[id_in_sen + 1] = tokenizer.vocab[suggestion]\n",
    "    input_type_ids.append(0)\n",
    "    \n",
    "        T_input_ids = torch.tensor([input_ids], dtype=torch.long) #把input_ids增加了一个维度\n",
    "        T_input_type_ids = torch.tensor([input_type_ids], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "        T_input_ids = T_input_ids.to(device) #拿去GPU\n",
    "        T_input_type_ids = T_input_type_ids.to(device)\n",
    "\n",
    "        mlm_logits, _ = model(T_input_ids, T_input_type_ids)\n",
    "        mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "        reduced_mlm_probs = mlm_probs[0][id_in_sen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_cache = {}\n",
    "\n",
    "def analyze_text(text, masked_tokens=None, show_suggestions=True, show_firstk_probs=20):\n",
    "    step = 7\n",
    "    if text[0] in analyzed_cache: #分析过的缓存\n",
    "        features, mlm_probs = analyzed_cache[text[0]]\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "        tokens = features[0].tokens \n",
    "    else:\n",
    "        examples = convert_text_to_examples(text)\n",
    "        features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "        if not given_mask or masked_tokens is not None:\n",
    "            assert len(features) == 1\n",
    "            features, batches = copy_and_mask_feature(features[0],step, masked_tokens=masked_tokens)\n",
    "            #print(len(features))\n",
    "\n",
    "        input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) #把input_ids增加了一个维度，变成[n_features,sequence_len]\n",
    "        #这里的n_features实际上是句子有多少批训练\n",
    "        \n",
    "        input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "        input_ids = input_ids.to(device) #拿去GPU\n",
    "        input_type_ids = input_type_ids.to(device)\n",
    "\n",
    "        mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "        mlm_probs = F.softmax(mlm_logits, dim=-1) #最后一维，也就是vocab 换算成概率和为百分之百\n",
    "        #print(mlm_probs.size())#这里实验的是torch.Size([5, 5, 30522])\n",
    "        tokens = features[0].tokens #为了输出，[mask]在input_ids里面表示出来，features的token都一样\n",
    "        #print(tokens)\n",
    "        if not given_mask or masked_tokens is not None:\n",
    "            bsz, seq_len, vocab_size = mlm_probs.size() #三个维度分别是batch_size, sequence_length, vocab_size\n",
    "            assert bsz == len(batches)\n",
    "            # reduced_mlm_probs = torch.Tensor(1, seq_len, vocab_size)\n",
    "            # for i in range(seq_len):\n",
    "            #    reduced_mlm_probs[0, i] = mlm_probs[i, i]\n",
    "            reduced_mlm_probs = torch.Tensor(1, len(tokens), vocab_size)\n",
    "            for i in batches:\n",
    "                pos = i\n",
    "                while pos < len(tokens):\n",
    "                    reduced_mlm_probs[0, pos] = mlm_probs[i, pos]\n",
    "                    pos = pos + step\n",
    "            mlm_probs = reduced_mlm_probs #压缩一下大小，节约不必要浪费的空间（只需要第i个batch里面[mask]位置的词汇表概率即可）\n",
    "            #tokens = [tokens[i] for i in masked_positions]\n",
    "        \n",
    "        analyzed_cache[text[0]] = (features, mlm_probs)\n",
    "        \n",
    "    top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=show_firstk_probs) #传入的probs是二维的\n",
    "    #print(top_pairs) #******************************\n",
    "    if not given_mask:\n",
    "        show_abnormals(tokens, mlm_probs[0], show_suggestions=show_suggestions)\n",
    "    #return top_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyze_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8bce9e3b3ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"I hate you.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#text =[\"Last week I go to the zoo. I had a very good seat. The play was very interesting.But I didn't enjoy it. A young man and a young woman were sitting behind me.They were talking loudly. I got very angry.\"]#因为外面有中括号，所以是二维的\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_firstk_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#print(analyzed_cache)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtime_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyze_text' is not defined"
     ]
    }
   ],
   "source": [
    "# text = [\"Who was Jim Henson? Jim Henson _ a puppeteer.\"]\n",
    "# text = [\"Last week I went to the theatre. I had a very good seat. The play was very interesting. But I didn't enjoy it. A young man and a young woman were sitting behind me. They were talking loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angrily. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "# text = [\"After the outbreak of the disease, the Ministry of Agriculture and rural areas immediately sent a supervision team to the local. Local Emergency Response Mechanism has been activated in accordance with the requirements, to take blockade, culling, harmless treatment, disinfection and other treatment measures to all disease and culling of pigs for harmless treatment. At the same time, all live pigs and their products are prohibited from transferring out of the blockade area, and live pigs are not allowed to be transported into the blockade area. At present, all the above measures have been implemented.\"]\n",
    "# text = [\"Early critics of Emily Dickinson's poetry mistook for simplemindedness the surface of artlessness that in fact she constructed with such innocence.\"]\n",
    "import time\n",
    "time_start=time.time()\n",
    "text = [\"I hate you.\"]\n",
    "#text =[\"Last week I go to the zoo. I had a very good seat. The play was very interesting.But I didn't enjoy it. A young man and a young woman were sitting behind me.They were talking loudly. I got very angry.\"]#因为外面有中括号，所以是二维的\n",
    "analyze_text(text, show_firstk_probs=100)\n",
    "#print(analyzed_cache)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/03/2019 17:10:45 - INFO - examples.extract_features -   tokens: [CLS] the trophy doesn ' t fit into the brown suitcase because the [MASK] is too large . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 | [CLS]       \t   2 | .              1 | )              1 | the            1 | ,              1 | \"           \n",
      " 100 | the         \t*100 | the            0 | his            0 | a              0 | its            0 | her         \n",
      "  97 | trophy      \t* 97 | trophy         0 | cup            0 | prize          0 | trophies       0 | competition \n",
      " 100 | doesn       \t*100 | doesn          0 | can            0 | does           0 | won            0 | didn        \n",
      " 100 | '           \t*100 | '              0 | t              0 | \"              0 | =              0 | `           \n",
      " 100 | t           \t*100 | t              0 | not            0 | s              0 | n              0 | to          \n",
      " 100 | fit         \t*100 | fit            0 | fits           0 | sit            0 | get            0 | fitting     \n",
      " 100 | into        \t*100 | into           0 | in             0 | inside         0 | onto           0 | within      \n",
      " 100 | the         \t*100 | the            0 | her            0 | his            0 | a              0 | my          \n",
      " 100 | brown       \t*100 | brown          0 | black          0 | green          0 | blue           0 | plastic     \n",
      "  95 | suitcase    \t* 95 | suitcase       3 | bag            1 | luggage        0 | backpack       0 | trunk       \n",
      " 100 | because     \t*100 | because        0 | as             0 | since          0 | due            0 | .           \n",
      " 100 | the         \t*100 | the            0 | its            0 | his            0 | it             0 | her         \n",
      "   0 | [MASK]      \t  21 | suitcase      19 | bag            6 | box            2 | luggage        2 | case        \n",
      "  99 | is          \t* 99 | is             1 | was            0 | being          0 | has            0 | it          \n",
      " 100 | too         \t*100 | too            0 | very           0 | extra          0 | overly         0 | more        \n",
      "  87 | large       \t* 87 | large         11 | big            1 | small          1 | huge           0 | larger      \n",
      " 100 | .           \t*100 | .              0 | ;              0 | ,              0 | !              0 | '           \n",
      "   0 | [SEP]       \t  35 | .              8 | )              5 | ,              4 | (              3 | it          \n"
     ]
    }
   ],
   "source": [
    "text = [\"The trophy doesn't fit into the brown suitcase because the _ is too large.\"]\n",
    "# text = [\"Mary beat John in the match because _ was very strong.\"]\n",
    "features = convert_examples_to_features(convert_text_to_examples(text), tokenizer, print_info=False)\n",
    "input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long).to(device)\n",
    "input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long).to(device)\n",
    "mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "tokens = features[0].tokens\n",
    "top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.',\n",
       " 'Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    # same / different\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.\",\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.\",\n",
    "    \"Tom has yellow hair. Mary has black hair. John has black hair. Mary and _ have the same hair color.\",\n",
    "    # because / although\n",
    "    \"John is taller/shorter than Mary because/although _ is older/younger.\",\n",
    "    \"The red ball is heavier/lighter than the blue ball because/although the _ ball is bigger/smaller.\",\n",
    "    \"Charles did a lot better/worse than his good friend Nancy on the test because/although _ had/hadn't studied so hard.\",\n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thought that he would arrive earlier than Susan, but/and indeed _ was the first to arrive.\",\n",
    "    # reverse\n",
    "    \"John came then Mary came. They left in reverse order. _ left then _ left.\",\n",
    "    \"John came after Mary. They left in reverse order. _ left after _ .\",\n",
    "    \"John came first, then came Mary. They left in reverse order: _ left first, then left _ .\",\n",
    "    # compare\n",
    "    \"Though John is tall, Tom is taller than John. So John is _ than Tom.\",\n",
    "    \"Tom is taller than John. So _ is shorter than _.\",\n",
    "    # WSC-style: before /after\n",
    "    \"Mary came before/after John. _ was late/early .\",\n",
    "    # yes / no\n",
    "    \"Was Tom taller than Susan? Yes, _ was taller.\",\n",
    "    # right / wrong, epistemic modality\n",
    "    \"John said the rain was about to stop. Mary said the rain would continue. Later the rain stopped. _ was wrong.\",\n",
    "    \n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thanked Mary because  _ had given help to _ . \",\n",
    "    \"John felt vindicated/crushed when his longtime rival Mary revealed that _ was the winner of the competition.\",\n",
    "    \"John couldn't see the stage with Mary in front of him because _ is so short/tall.\",\n",
    "    \"Although they ran at about the same speed, John beat Sally because _ had such a bad start.\",\n",
    "    \"The fish ate the worm. The _ was hungry/tasty.\",\n",
    "    \n",
    "    \"John beat Mary. _ won the game/e winner.\",\n",
    "]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_switched_label.json') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_child_problem.json') as f:\n",
    "    cexamples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    for s in ce['sentences']:\n",
    "        for a in s['answer0'] + s['answer1']:\n",
    "            a = a.lower()\n",
    "            if a not in tokenizer.vocab:\n",
    "                ce\n",
    "                print(a, 'not in vocab!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    if len(ce['sentences']) > 0:\n",
    "        e = examples[ce['index']]\n",
    "        assert ce['index'] == e['index']\n",
    "        e['score'] = all([s['score'] for s in ce['sentences']])\n",
    "        assert len(set([s['adjacent_ref'] for s in ce['sentences']])) == 1, 'adjcent_refs are different!'\n",
    "        e['adjacent_ref'] = ce['sentences'][0]['adjacent_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for e in examples:\n",
    "    if 'score' in e:\n",
    "        index = e['index']\n",
    "        if index < 252:\n",
    "            if index % 2 == 1:\n",
    "                index -= 1\n",
    "        elif index in [252, 253, 254]:\n",
    "            index = 252\n",
    "        else:\n",
    "            if index % 2 == 0:\n",
    "                index -= 1\n",
    "        groups[index].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'fit into:large/small', False),\n",
       " (4, 'thank:receive/give', False),\n",
       " (6, 'call:successful available', True),\n",
       " (8, 'ask:repeat answer', False),\n",
       " (10, 'zoom by:fast/slow', False),\n",
       " (12, 'vindicated/crushed:be the winner', False),\n",
       " (14, 'lift:weak heavy', False),\n",
       " (16, 'crash through:[hard]/[soft]', False),\n",
       " (18, '[block]:short/tall', False),\n",
       " (20, 'down to:top/bottom', False),\n",
       " (22, 'beat:good/bad', False),\n",
       " (24, 'roll off:anchored level', False),\n",
       " (26, 'above/below', False),\n",
       " (28, 'better/worse:study hard', False),\n",
       " (30, 'after/before:far away', False),\n",
       " (32, 'be upset with:buy from not work/sell not work', True),\n",
       " (34, '?yell at comfort:upset', False),\n",
       " (36, 'above/below:moved first', False),\n",
       " (38, 'although/because', False),\n",
       " (40, 'bully:punish rescue', False),\n",
       " (42, 'pour:empty/full', False),\n",
       " (44, 'know:nosy indiscreet', False),\n",
       " (46, 'explain:convince/understand', True),\n",
       " (48, '?know tell:so/because', True),\n",
       " (50, 'beat:younger/older', False),\n",
       " (56, 'clog:cleaned removed', True),\n",
       " (58, '?immediately follow:short delayed', False),\n",
       " (60, '?between:see see around', True),\n",
       " (64, 'but/and', False),\n",
       " (66, 'clean:put in the trash put in the drawer', False),\n",
       " (68, 'because/but', False),\n",
       " (70, 'out of:handy lighter', False),\n",
       " (72, 'put:tall high', False),\n",
       " (74, 'show:good famous', True),\n",
       " (76, 'pay for:generous grateful', False),\n",
       " (78, 'but', False),\n",
       " (80, 'if', False),\n",
       " (82, 'if', False),\n",
       " (84, 'fool:get/lose', False),\n",
       " (88, 'wait:impatient cautious', False),\n",
       " (90, 'give birth:woman baby', True),\n",
       " (92, '?stop normal/stop abnormal:strange', False),\n",
       " (96, 'eat:hungry tasty', False),\n",
       " (98, 'put ... into filled with ... :get in/get out', False),\n",
       " (100, 'up:at the bottom/at the top', False),\n",
       " (102, 'crash through:removed repaired', False),\n",
       " (104, 'stab:taken to the police station taken to the hospital', False),\n",
       " (106, 'hear ... humming and whistling:annoyed/annoying', True),\n",
       " (108, 'see ... juggling watermelons:impressed/impressive', True),\n",
       " (114, 'tell lies: truthful skeptical', True),\n",
       " (130, 'but:disappointed', True),\n",
       " (132, 'visit:invite come out/invite come in', True),\n",
       " (134, 'take classes from:eager known to speak it fluently', False),\n",
       " (138, 'cover:out gone', True),\n",
       " (144, 'tuck:work sleep', True),\n",
       " (150, 'influence:later/earlier', False),\n",
       " (152, 'can not cut:thick small', False),\n",
       " (154, 'attack:kill guard', False),\n",
       " (156, 'attack:bold nervous', False),\n",
       " (160, 'change:hard:easy', False),\n",
       " (166, 'alive:is/was', False),\n",
       " (168, 'infant:twelve years old twelve months old', False),\n",
       " (170, 'better equipped and large:defeated/victorious', False),\n",
       " (178, 'interview:persistent cooperative', False),\n",
       " (186, 'be full of:minority/majority', False),\n",
       " (188, 'like over:more/fewer', False),\n",
       " (190, 'place on all:not enough/too many', True),\n",
       " (192, 'stick:leave have', True),\n",
       " (196, 'follow:admire/influence', True),\n",
       " (198, 'fit through:wide/narrow', False),\n",
       " (200, 'trade:dowdy/great', False),\n",
       " (202, 'hire/hire oneself to:take care of', True),\n",
       " (204, 'promise/order', False),\n",
       " (208, 'mother:education place', True),\n",
       " (210, 'knock:get an answer/answer', True),\n",
       " (212, 'pay:receive/deliver', False),\n",
       " (218, '?', False),\n",
       " (220, 'say check:move take', False),\n",
       " (222, '?', False),\n",
       " (224, 'give a life:drive alone walk', False),\n",
       " (226, 'pass the plate:full/hungry', False),\n",
       " (228, 'pass:turn over turn next', False),\n",
       " (232, 'stretch pat', True),\n",
       " (234, 'accept share', False),\n",
       " (236, 'speak:break silence break concentration', False),\n",
       " (240, 'carry:leg ache leg dangle', True),\n",
       " (242, 'carry:in arms in bassinet', False),\n",
       " (244, 'hold:against chest against will', True),\n",
       " (250, 'stop', False),\n",
       " (252, 'even though/because/not', False),\n",
       " (255, 'give:not hungry/hungry', False),\n",
       " (259, 'ask for a favor:refuse/be refused`', False),\n",
       " (261, 'cede:less popular/more popular', False),\n",
       " (263, 'not pass although:see open/open', True),\n",
       " (271, 'suspect regret', True)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_dict(d, keys=['index', 'sentence', 'correct_answer', 'relational_word', 'is_associative', 'score']):\n",
    "    return {k: d[k] for k in d if k in keys}\n",
    "\n",
    "# ([[filter_dict(e) for e in eg] for eg in groups.values() if eg[0]['relational_word'] != 'none' and all([e['score'] for e in eg])])# / len([eg for eg in groups.values() if eg[0]['relational_word'] != 'none'])\n",
    "[(index, eg[0]['relational_word'], all([e['score'] for e in eg])) for index, eg in groups.items() if eg[0]['relational_word'] != 'none']\n",
    "# len([filter_dict(e) for e in examples if 'score' in e and not e['score'] and e['adjacent_ref']])\n",
    "# for e in examples:\n",
    "#     if e['index'] % 2 == 0:\n",
    "#         print(e['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['because' in e['sentence'] for e in examples]) + \\\n",
    "sum(['so ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['but ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['though' in e['sentence'] for e in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('WSC_switched_label.json', 'w') as f:\n",
    "#     json.dump(examples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_attn_topk = 3\n",
    "\n",
    "def has_chinese_label(labels):\n",
    "    labels = [label.split('->')[0].strip() for label in labels]\n",
    "    r = sum([len(label) > 1 for label in labels if label not in ['BOS', 'EOS']]) * 1. / (len(labels) - 1)\n",
    "    return 0 < r < 0.5  # r == 0 means empty query labels used in self attention\n",
    "\n",
    "def _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col, color='b'):\n",
    "    assert len(query_labels) == attn.size(0)\n",
    "    assert len(key_labels) == attn.size(1)\n",
    "\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax2 = ax1.twinx()\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    pos = range(nlabels)\n",
    "    \n",
    "    if 'self' in attn_name and col < ncols - 1:\n",
    "        query_labels = ['' for _ in query_labels]\n",
    "\n",
    "    for ax, labels in [(ax1, key_labels), (ax2, query_labels)]:\n",
    "        ax.set_yticks(pos)\n",
    "        if has_chinese_label(labels):\n",
    "            ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "        else:\n",
    "            ax.set_yticklabels(labels)\n",
    "        ax.set_ylim([nlabels - 1, 0])\n",
    "        ax.tick_params(width=0, labelsize='xx-large')\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "#     mask, attn = filter_attn(attn)\n",
    "    for qi in range(attn.size(0)):\n",
    "#         if not mask[qi]:\n",
    "#             continue\n",
    "#         for ki in range(attn.size(1)):\n",
    "        for ki in attn[qi].topk(vis_attn_topk)[1]:\n",
    "            a = attn[qi, ki]\n",
    "            ax1.plot((-1, 1), (ki, qi), color, alpha=a)\n",
    "#     print(attn.mean(dim=0).topk(5)[0])\n",
    "#     ax1.barh(pos, attn.mean(dim=0).data.cpu().numpy())\n",
    "\n",
    "def plot_layer_attn(result_tuple, attn_name='dec_self_attns', layer=0, heads=None):\n",
    "    hypo, nheads, labels_dict = result_tuple\n",
    "    key_labels, query_labels = labels_dict[attn_name]\n",
    "    if heads is None:\n",
    "        heads = range(nheads)\n",
    "    else:\n",
    "        nheads = len(heads)\n",
    "    \n",
    "    stride = 2 if attn_name == 'dec_enc_attns' else 1\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    rcParams['figure.figsize'] = 20, int(round(nlabels * stride * nheads / 8 * 1.0))\n",
    "    \n",
    "    rows = nheads // ncols * stride\n",
    "    fig, axes = plt.subplots(rows, ncols)\n",
    "    \n",
    "    # for head in range(nheads):\n",
    "    for head_i, head in enumerate(heads):\n",
    "        row, col = head_i * stride // ncols, head_i * stride % ncols\n",
    "        ax1 = axes[row, col]\n",
    "        attn = hypo[attn_name][layer][head]\n",
    "        _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col)\n",
    "        if attn_name == 'dec_enc_attns':\n",
    "            col = col + 1\n",
    "            axes[row, col].axis('off')  # next subfig acts as blank place holder\n",
    "    # plt.suptitle('%s with %d heads, Layer %d' % (attn_name, nheads, layer), fontsize=20)\n",
    "    plt.show()  \n",
    "            \n",
    "ncols = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertSelfAttention' object has no attribute 'attention_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-13903ae0e550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enc_self_attns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-13903ae0e550>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enc_self_attns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertSelfAttention' object has no attribute 'attention_probs'"
     ]
    }
   ],
   "source": [
    "attn_name = 'enc_self_attns'\n",
    "hypo = {attn_name: [model.bert.encoder.layer[i].attention.self.attention_probs[0] for i in range(config.num_hidden_layers)]}\n",
    "key_labels = query_labels = tokens\n",
    "labels_dict = {attn_name: (key_labels, query_labels)}\n",
    "result_tuple = (hypo, config.num_attention_heads, labels_dict)\n",
    "plot_layer_attn(result_tuple, attn_name=attn_name, layer=10, heads=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
