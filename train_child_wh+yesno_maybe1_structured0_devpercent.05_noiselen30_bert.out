06/09/2019 19:11:27 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:11:27 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:11:28 - INFO - run_child_finetuning -   num_sent = 92160
Traceback (most recent call last):
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/home/xd/projects/pytorch-pretrained-BERT/train_child.py", line 86, in <module>
    child_dataset = CHILDDataset(tokenizer, sentences, dev_percent=args.dev_percent)
  File "/home/xd/projects/pytorch-pretrained-BERT/run_child_finetuning.py", line 68, in __init__
    t1, t2, is_next_label = self.split_sent(line)
  File "/home/xd/projects/pytorch-pretrained-BERT/run_child_finetuning.py", line 121, in split_sent
    assert self.one_sent
AssertionError
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.
Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.
Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Namespace(dev_percent=0.05, do_eval=True, do_lower_case=True, do_train=True, eval_batch_size=128, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=6.0, seed=42, train_batch_size=32, warmup_proportion=0.1)
num_sent = 92160 -> 92160
06/09/2019 19:12:36 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:12:36 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:12:36 - INFO - run_child_finetuning -   num_sent = 92160
Traceback (most recent call last):
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/home/xd/projects/pytorch-pretrained-BERT/train_child.py", line 86, in <module>
    child_dataset = CHILDDataset(tokenizer, sentences, dev_percent=args.dev_percent)
  File "/home/xd/projects/pytorch-pretrained-BERT/run_child_finetuning.py", line 71, in __init__
    tokens_b = self.tokenizer.tokenize(t2)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/tokenization.py", line 94, in tokenize
    for token in self.basic_tokenizer.tokenize(text):
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/tokenization.py", line 174, in tokenize
    text = self._clean_text(text)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/tokenization.py", line 264, in _clean_text
    for char in text:
TypeError: 'NoneType' object is not iterable
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.
Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.
Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Namespace(dev_percent=0.05, do_eval=True, do_lower_case=True, do_train=True, eval_batch_size=128, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=6.0, seed=42, train_batch_size=32, warmup_proportion=0.1)
num_sent = 92160 -> 92159
06/09/2019 19:21:08 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:21:08 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:21:09 - INFO - run_child_finetuning -   num_sent = 92160
06/09/2019 19:22:02 - INFO - run_child_finetuning -   num_train_steps = 16416
06/09/2019 19:22:03 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 19:22:03 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 19:22:03 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 19:22:07 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 19:22:09 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 19:22:09 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 19:25:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:25:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:25:46 - INFO - run_child_finetuning -   num_sent = 46080
06/09/2019 19:26:12 - INFO - run_child_finetuning -   num_train_steps = 8208
06/09/2019 19:26:13 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 19:26:13 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 19:26:13 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 19:26:16 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 19:26:18 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 19:26:18 - INFO - run_child_finetuning -   Evaluating on train set...
