{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import tokenization, BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, BertConfig\n",
    "from examples.extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/21/2019 18:04:54 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/xd/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/21/2019 18:04:54 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/\n",
      "03/21/2019 18:04:54 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "args = Args()\n",
    "args.no_cuda = True\n",
    "\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "BERT_DIR = '/nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/'\n",
    "config_file = os.path.join(BERT_DIR, CONFIG_NAME)\n",
    "config = BertConfig.from_json_file(config_file)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')#do_lower_case：在标记化时将文本转换为小写。默认= True\n",
    "#tokenizer.tokenize = nltk.word_tokenize\n",
    "model = BertForPreTraining.from_pretrained(BERT_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertForPreTraining：\n",
    "Outputs:\n",
    "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
    "            sentence classification loss.\n",
    "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
    "            Outputs a tuple comprising\n",
    "            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n",
    "            - the next sentence classification logits of shape [batch_size, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from_pretrained：\n",
    "Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
    "Download and cache the pre-trained model file if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_text_to_examples(text): \n",
    "    '''功能：\n",
    "            把输入的文本变成一个实例，一个实例中包含text_a,text_b(text_b用于是否为上下句的任务，该任务不使用此功能)\n",
    "       输入：\n",
    "            text：一个列表结构，列表中包含原始文本字符串，由于仅完成mlm任务，所以text列表中仅包含一个字符串，就是待检查的字符串\n",
    "       输出：\n",
    "            example：实例，其中包含：\n",
    "                unique_id：此任务仅用到0\n",
    "                text_a：text列表内的字符串\n",
    "                text_b：此任务下该变量为None\n",
    "    '''\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    if True:\n",
    "        for line in text:\n",
    "            line = line.strip()\n",
    "            text_a = None\n",
    "            text_b = None\n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line) #想要匹配这样的字符串'You are my sunshine. ||| I love you.'\n",
    "            \n",
    "            if m is None:\n",
    "                text_a = line\n",
    "            else:\n",
    "                text_a = m.group(1) #匹配的第一句,比如You are my sunshine,my only sunshine.\n",
    "                text_b = m.group(2) #匹配的第二句，比如I love you.\n",
    "            \n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            unique_id += 1\n",
    "    return examples\n",
    "#print(convert_text_to_examples(['I love you. The cat is so cute.'])[0].text_a)\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, append_special_tokens=True, replace_mask=True, print_info=False):\n",
    "    '''功能：\n",
    "            把实例变成一个特征列表\n",
    "       输入：\n",
    "            examples：实例，convert_text_to_examples()函数的输出\n",
    "            tokenizer：BERT的tokenizer，用于将文本进行各种处理，它可以把一个text转变成tokens，把tokens变成每个token在词典中的编号以及逆运算\n",
    "            append_special_tokens：是否允许在生成的tokens中加入特殊符号，也就是[CLS]、[MASK]和[SEP]，默认为True\n",
    "            replace_mask：不明\n",
    "            print_info：不明\n",
    "       输出：\n",
    "            features：每一个feature包含：\n",
    "                unique_id：编号，目前实现的功能features里面仅有一个feature\n",
    "                tokens=tokens,tokens：是形如['i','love','you','.']的一个列表\n",
    "                input_ids=input_ids：字符串中的每个单词在词典中的index序列\n",
    "                input_mask=input_mask：一堆1\n",
    "                input_type_ids=input_type_ids))：对text_a,text_b的区分，用于上下句任务，对于本任务，该参数为一个列表，其中包含token长度个的0\n",
    "    '''\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a) #tokenize的作用是把\"i love you.\"变成['i','love','you','.']\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        tokens = []\n",
    "        input_type_ids = [] #segment embedding\n",
    "        if append_special_tokens: #输入参数中默认为true\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            if replace_mask and token == '_':  # XD\n",
    "                token = \"[MASK]\"\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                if replace_mask and token == '_':  # XD\n",
    "                    token = \"[MASK]\"\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(1)\n",
    "            if append_special_tokens:\n",
    "                tokens.append(\"[SEP]\")\n",
    "                input_type_ids.append(1)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) #把原来句子中的词语编成在字典中的编号\n",
    "        input_mask = [1] * len(input_ids) \n",
    "        \n",
    "        if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "#             logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\n",
    "#                 \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=example.unique_id,#编号，目前实现的功能features里面仅有一个feature\n",
    "                tokens=tokens,#形如['i','love','you','.']的一个列表\n",
    "                input_ids=input_ids,#字符串中的每个单词在词典中的index序列\n",
    "                input_mask=input_mask, #一堆1\n",
    "                input_type_ids=input_type_ids)) #第0类和第1类，对text_a,text_b的区分，本代码中全都是零\n",
    "    return features            \n",
    "\n",
    "def copy_and_mask_feature(feature, step, masked_tokens=None): \n",
    "    '''\n",
    "        功能：\n",
    "            输入feature生成训练的批次数以及mask好的训练素材\n",
    "        输入：\n",
    "            feature：convert_examples_to_features函数的输出\n",
    "            step：两个[mask]位置的步长\n",
    "            masked_tokens：默认为None，在程序中没有使用\n",
    "    '''\n",
    "    import copy\n",
    "    tokens = feature.tokens\n",
    "    len_token = len(tokens)\n",
    "    if len_token<step:\n",
    "        batches = range(0,len(tokens))\n",
    "    else:\n",
    "        batches = range(0,step)\n",
    "    \n",
    "    assert len_token > 0\n",
    "    masked_feature_copies = []\n",
    "    for i in batches: #用[mask]依次掩盖每一个位置\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        masked_pos = i\n",
    "        while masked_pos < len_token:\n",
    "            feature_copy.input_ids[masked_pos] = tokenizer.vocab[\"[MASK]\"]\n",
    "            masked_pos = masked_pos + step\n",
    "        masked_feature_copies.append(feature_copy)\n",
    "    return masked_feature_copies, batches\n",
    "\n",
    "#masked_feature_copies, batches = copy_and_mask_feature(features[0],3)\n",
    "#print(masked_feature_copies[0].input_ids) #结果[101, 1045, 2293, 103, 102]\n",
    "#print(batches) #结果是一个range(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(input_ids_sen)\\nprint(in_sentence)\\nprint(input_type_ids_sen)\\nprint(sentences)\\nprint(entire_ids)\\nprint(entire_type_ids)\\n#input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids = None'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzed_cache = {}\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG\n",
    "#print (lemma('gave'))\n",
    "#print (lexeme('production'))\n",
    "#print (conjugate(verb='give',tense=PRESENT,number=SG))\n",
    "def process_text(text): \n",
    "    '''\n",
    "        功能：\n",
    "            处理输入文本，将文本按句子分成若干token，得出原来text中index位置的单词在x句子的y位置，还得出各个句子类别码\n",
    "        输入：\n",
    "            text：文本字符串，注意区别\n",
    "        输出：\n",
    "            input_ids_sen：二维列表，第一维列表的元素是每个句子的input_ids列表\n",
    "            input_type_ids_sen：二维列表，第一维列表的元素是每个句子的input_type_ids列表\n",
    "            in_sentence：通过这个二维数组可以很方便的通过在完整text中的下标找到这个下标所在的句子和在句子中的下标\n",
    "            sentences：字符串列表，列表中每一个元素是一个句子字符串\n",
    "            entire_ids：整个text的input_ids\n",
    "            entire_type_ids：整个text的input_type_ids\n",
    "    '''\n",
    "    token =[]\n",
    "    entire_type_ids = []\n",
    "    token0 = tokenizer.tokenize(text)\n",
    "    token.append('[CLS]')\n",
    "    entire_type_ids.append(0)\n",
    "    for i in token0:\n",
    "        token.append(i)\n",
    "        entire_type_ids.append(0)\n",
    "    token.append('[SEP]')\n",
    "    entire_type_ids.append(0)\n",
    "    \n",
    "    entire_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "    in_sentence = [[0,0]] \n",
    "    sentence_n = 0\n",
    "    index = 1\n",
    "    for i in range(1,len(token)-1):\n",
    "        in_sentence.append([sentence_n,index])  #每个token中的词在所在句中的位置表示出来，以及该位置在哪一句中\n",
    "        index = index + 1                           #比如，位置i这个词在第sentence句的index位置上\n",
    "        if token[i] == '.':\n",
    "            sentence_n = sentence_n + 1\n",
    "            index = 1\n",
    "    sentences = text.split(\".\")\n",
    "    \n",
    "    sen_token = []\n",
    "    input_ids_sen = []\n",
    "    input_type_ids_sen = []\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentence = sentence + '.'\n",
    "        sentences[i] = sentences[i] + '.'\n",
    "        token = []\n",
    "        input_type_ids = []\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token.append('[CLS]')\n",
    "        input_type_ids.append(0) \n",
    "        for i in tokens:\n",
    "            token.append(i)\n",
    "            input_type_ids.append(0)        \n",
    "        token.append('[SEP]')        \n",
    "        input_type_ids.append(0)\n",
    "        input_ids_sen.append(tokenizer.convert_tokens_to_ids(token))\n",
    "        input_type_ids_sen.append(input_type_ids)\n",
    "    return input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids\n",
    "#text = [\"Last week I went to the theatre. I had an very good a seat.The play were very interesting. But I didn't enjoy it. A young man and a young woman were sitting behind me. They were talking loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angrily. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "#text = [\"Last week I went to the theatre. I had very good seat. The play was very interesting. But I didn't enjoy it. A young man and a young woman were sitting behind me. They were talking loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angrily. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "#text = [\"The question is more easy than that one.\"]\n",
    "text = [\"Last week I went to the theater. There are many person . Luckily I had very good seat. The plays was very interesting. However, I didn't enjoy it. A young man and a young woman were sitting behind me. They were talk loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angry. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "\n",
    "input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids = process_text(text[0])\n",
    "'''print(input_ids_sen)\n",
    "print(in_sentence)\n",
    "print(input_type_ids_sen)\n",
    "print(sentences)\n",
    "print(entire_ids)\n",
    "print(entire_type_ids)\n",
    "#input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids = None'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(index):\n",
    "    '''\n",
    "        输入：\n",
    "            index：在完整text中的位置\n",
    "        输出\n",
    "            word:该位置上的单词\n",
    "    '''\n",
    "    word_id = entire_ids[index]\n",
    "    word = tokenizer.ids_to_tokens[word_id]\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE\n",
    "\n",
    "def give_suggestion(input_ids_,input_type_ids_,id_in_sen,alternative_word,threshold):\n",
    "    '''\n",
    "        功能：\n",
    "            给出指定文本指定位置的推荐用词\n",
    "        输入：\n",
    "            input_ids_：要分析的文本的input_ids\n",
    "            input_type_ids_：要分析的文本的的input_type_ids\n",
    "            id_in_sen：要分析的文本中[MASK]的位置下标，也就是需要给出建议用词的位置\n",
    "            alternative_word：推荐的备选词范围\n",
    "            threshold：阈值\n",
    "        输出：\n",
    "            suggestion：推荐\n",
    "            need：推荐的是否是备选词中的词\n",
    "            suggestion_prob：推荐词填在id_in_sen位置的概率\n",
    "            top_of_alternative:备选词中最值得推荐的词\n",
    "    '''\n",
    "    input_ids = copy.deepcopy(input_ids_)\n",
    "    input_type_ids = copy.deepcopy(input_type_ids_)\n",
    "    word0 = input_ids[id_in_sen]\n",
    "    word0 = tokenizer.ids_to_tokens[word0]\n",
    "    list_word_id = []\n",
    "    \n",
    "    input_ids[id_in_sen] = tokenizer.vocab[\"[MASK]\"]\n",
    "    T_input_ids = torch.tensor([input_ids], dtype=torch.long) #把input_ids增加了一个维度\n",
    "    T_input_type_ids = torch.tensor([input_type_ids], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    T_input_ids = T_input_ids.to(device) #拿去GPU\n",
    "    T_input_type_ids = T_input_type_ids.to(device)\n",
    "\n",
    "    mlm_logits, _ = model(T_input_ids, T_input_type_ids)\n",
    "    mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "    reduced_mlm_probs = mlm_probs[0][id_in_sen]\n",
    "\n",
    "    top_ind = reduced_mlm_probs.argmax().item()\n",
    "    top_prob = reduced_mlm_probs.max().item() \n",
    "    \n",
    "    list_word = []\n",
    "    \n",
    "    top_of_alternative = None\n",
    "    if len(alternative_word)>0:\n",
    "        list_word_prob = {}\n",
    "        for word in alternative_word:\n",
    "            try:\n",
    "                list_word_id.append(tokenizer.vocab[word])\n",
    "                list_word.append(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        #print(list_word_id)    \n",
    "        #print(list_word)\n",
    "        for word,word_id in zip(list_word,list_word_id):\n",
    "            list_word_prob.update({word:float(reduced_mlm_probs[word_id].data)})\n",
    "        prob_ord = sorted(list_word_prob.items(),key = lambda x:x[1],reverse = True)\n",
    "        #print(prob_ord)\n",
    "        #print(tokenizer.ids_to_tokens[top_ind],top_prob)\n",
    "        #print(prob_ord[0][0],prob_ord[0][1])\n",
    "        top_prob_word = prob_ord[0][1]\n",
    "        top_of_alternative = prob_ord[0][0]\n",
    "        gap = math.log(top_prob) - math.log(top_prob_word)\n",
    "        if gap < threshold:\n",
    "            suggestion = prob_ord[0][0]\n",
    "            suggestion_prob = prob_ord[0][1]\n",
    "            need = 1\n",
    "        else:\n",
    "            suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "            suggestion_prob = top_prob\n",
    "            need = 0\n",
    "        #print(\"gap = \" + str(gap))\n",
    "        #print(prob_ord)\n",
    "    else:\n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        suggestion_prob = top_prob\n",
    "        need = 0\n",
    "        \n",
    "    return suggestion,need,suggestion_prob,top_of_alternative \n",
    "\n",
    "#返回变量5\n",
    "#suggestion -> 最值得推荐的词\n",
    "#need -> 是否需要可选词中的一个\n",
    "#suggestion_prob ->最值得推荐的词的概率\n",
    "#top_of_alternative -> 可选词中最值得推荐的\n",
    "#suggestion,need,suggestion_prob,top_of_alternative = give_suggestion(input_ids_,input_type_ids_,id_in_sen,alternative_word,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from pattern.en import comparative, superlative\n",
    "from pattern.en import suggest\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "totally time cost 0.2184145450592041 s\n"
     ]
    }
   ],
   "source": [
    "stemmers=[]\n",
    "stemmers.append(LancasterStemmer()) \n",
    "stemmers.append(SnowballStemmer(\"english\"))\n",
    "stemmers.append(PorterStemmer())\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "#分情况讨论，如果新词比旧的词长，或者是短\n",
    "def word_convert(word,new_word,Stemmer):\n",
    "    '''\n",
    "        功能：\n",
    "            根据提供的word和可能的变形new_word,得到正确的变形，例如给出basic，basicly得到basically\n",
    "        输入：\n",
    "            word：需要变形的词\n",
    "            new_word:猜想的变形\n",
    "        输出：\n",
    "            suggest_word:推荐的正确变形\n",
    "    '''\n",
    "    suggest_word = None\n",
    "    word_stem = Stemmer().stem(word)\n",
    "    suggest_ = new_word\n",
    "    \n",
    "    suggest_list = suggest(suggest_)\n",
    "\n",
    "    if len(word)<len(new_word):\n",
    "        flag = 0\n",
    "    else:\n",
    "        flag = 1\n",
    "    word_stem = word_stem[:-1]\n",
    "    suggestion_word_stem = Stemmer().stem(suggest_)\n",
    "    \n",
    "    for word_ in suggest_list:\n",
    "        if word == word_[0]:\n",
    "            continue\n",
    "        if (word_[0] == new_word and word_[1] > 0.95):# or word_[1] > 0.95 :\n",
    "            suggest_word = word_[0]\n",
    "            break           \n",
    "        if word_[1] < 0.001:\n",
    "            break\n",
    "        stem_list = []\n",
    "        for stemmer in stemmers:\n",
    "            suggest_stem = stemmer.stem(word_[0])\n",
    "            if flag == 1 and suggest_stem[:-1] in word_stem and word_stem[:3] in suggest_stem[:3]: #一般是去后缀\n",
    "                suggest_word = word_[0]\n",
    "                break\n",
    "            elif flag == 0 and word_stem in suggest_stem and word_[0][-1:] in suggest_[-1:]: #一般是加后缀，后缀一定要一样\n",
    "                suggest_word = word_[0]\n",
    "                break\n",
    "                \n",
    "        if suggest_word != None:\n",
    "            break\n",
    "    return suggest_word \n",
    "\n",
    "import time\n",
    "time_start=time.time()\n",
    "for i in range(1):\n",
    "    print(word_convert(\"dark\",\"darkment\",PorterStemmer))\n",
    "time_end=time.time()\n",
    "print('totally time cost',time_end-time_start,'s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beaus\n",
      "totally time cost 0.0006299018859863281 s\n"
     ]
    }
   ],
   "source": [
    "stemmers=[]\n",
    "stemmers.append(LancasterStemmer()) \n",
    "stemmers.append(SnowballStemmer(\"english\"))\n",
    "stemmers.append(PorterStemmer())\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "def word_convert(word,new_word,Stemmer):\n",
    "    '''\n",
    "        说明;\n",
    "            与上面的区别是使用的拼写改错算法不同，上面那个平均速度慢，但更符合我的要求，这个平均速度更快\n",
    "        功能：\n",
    "            根据提供的word和可能的变形new_word,得到正确的变形，例如给出basic，basicly得到basically\n",
    "        输入：\n",
    "            word：需要变形的词\n",
    "            new_word:猜想的变形\n",
    "            Stemmer:词根提取器\n",
    "        输出：\n",
    "            suggest_word:推荐的正确变形\n",
    "    '''\n",
    "    if d.check(new_word)==True: #如果发现new_word拼写正确，则直接返回\n",
    "        return new_word\n",
    "    else:\n",
    "        suggest_word = None\n",
    "        word_stem = Stemmer().stem(word)\n",
    "        suggest_ = new_word\n",
    "        suggest_list = d.suggest(suggest_) #可能的正确单词列表\n",
    "\n",
    "        if len(word)<len(new_word): #一般都是加后缀\n",
    "            flag = 0\n",
    "        else: #一般都是去后缀\n",
    "            flag = 1\n",
    "        word_stem = word_stem[:-1] #这样效果更好一点，防止某些去e加后缀或者y变i的变形被忽略\n",
    "        suggestion_word_stem = Stemmer().stem(suggest_)\n",
    "        for word_ in suggest_list:\n",
    "            if word == word_: #如果变形和原型一样，就跳过这个词\n",
    "                continue\n",
    "            if (word_ == new_word): #如果推荐的和new_word一样，直接把该词作为结果\n",
    "                suggest_word = word_\n",
    "                break\n",
    "            if ' ' in word_ or '-' in word_: #enchant.Dict模型特有的问题，一个拼写错误的词可能会给你返回一个带连字符词的或者是两个词\n",
    "                continue\n",
    "            stem_list = []\n",
    "            for stemmer in stemmers:\n",
    "                suggest_stem = stemmer.stem(word_)\n",
    "                if flag == 1 and suggest_stem in word_stem and word_stem[:3] in suggest_stem[:3]: #一般是去后缀\n",
    "                    suggest_word = word_\n",
    "                    break\n",
    "                elif flag == 0 and word_stem in suggest_stem and word_[-1:] in suggest_[-1:]: #一般是加后缀，后缀一定要一样\n",
    "                    suggest_word = word_\n",
    "                    break\n",
    "\n",
    "            if suggest_word != None:\n",
    "                break\n",
    "        return suggest_word \n",
    "import time\n",
    "time_start=time.time()\n",
    "for i in range(1):\n",
    "    print(word_convert(\"beautiful\",\"beauti\",PorterStemmer))\n",
    "time_end=time.time()\n",
    "print('totally time cost',time_end-time_start,'s')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "下面是词性转换系列函数\n",
    "    功能：\n",
    "        词性转变系列函数\n",
    "    输入：\n",
    "        word：原形词\n",
    "    输出：\n",
    "        suggest_word：推荐的变形\n",
    "        suggest_list：推荐的变形列表\n",
    "    说明：\n",
    "        词性变化的能力有限，对于有些特殊变形，比如die->death，success->succeed无能为力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully\n",
      "basic\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def adj_to_adv(word):\n",
    "    suggest_word = None\n",
    "    if(word == \"good\"):\n",
    "        return \"well\"\n",
    "    else:\n",
    "        suggest_ = word + 'ly'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        return suggest_word\n",
    "#如果形容词副词同形，那么他会返回none，但是不影响计算，因为形容词副词同形啊\n",
    "print(adj_to_adv(\"successful\"))\n",
    "\n",
    "def adv_to_adj(word):\n",
    "    suggest_word = None\n",
    "    if(word == \"well\"):\n",
    "        return \"good\"    \n",
    "    elif word[-2:] == 'ly':\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "    return suggest_word\n",
    "print(adv_to_adj(\"basically\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interested', 'interest']\n"
     ]
    }
   ],
   "source": [
    "def adj_to_anything(word):#形容词变成其他词性\n",
    "    suggest_word = None\n",
    "    suggest_list = []\n",
    "    if word[-1:] == 'y': #举例 healthy->health\n",
    "        suggest_ = word[:-1]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ful':#举例 successful->success\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ive': #举例 active -> act\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'ed': #举例 interested->interest->interesting\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)     \n",
    "        suggest_ = suggest_ + 'ing'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)      \n",
    "            \n",
    "    elif word[-3:] == 'ing':#举例 interesting->interest->interested\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "        suggest_ = suggest_ + 'ed'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)  \n",
    "            \n",
    "    elif word[-4:] == 'less': #举例 careless -> care\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'ly':  #举例： friendly -> friend , lovely -> love\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    " \n",
    "    elif word[-1:] == 't': #举例 different -> different\n",
    "        suggest_ = word[:-1]\n",
    "        suggest_ = suggest_ + 'ce'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ous': #举例 dangerous -> danger\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'al': #举例 original -> origin\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-4:] == 'able':\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'en': #举例 woolen -> wool\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'ic': \n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)        \n",
    "    elif word[-3:] == 'ish':\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word == None:\n",
    "            suggest_ = word[:-3]\n",
    "            suggest_ = suggest_ + 'and'\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer) \n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ese':\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_ = suggest_ + 'a'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)  \n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ian':\n",
    "        suggest_ = word[:-1]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word == None:\n",
    "            suggest_ = word[:-3]\n",
    "            suggest_ = suggest_ + 'y'\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    if suggest_word == None:\n",
    "        HouZhui_list = ['ment','ness','tion','ture','sion','ty','y','tive','sive']\n",
    "        for HouZhui in HouZhui_list:\n",
    "            suggest_ = word + HouZhui\n",
    "            new_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if new_word != None:\n",
    "                suggest_word = new_word\n",
    "                suggest_list.append(suggest_word)\n",
    "    suggest_list = list(set(suggest_list))      \n",
    "    return suggest_list\n",
    "\n",
    "print(adj_to_anything('interesting'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import time\\ntime_start=time.time()\\nprint(N_to_anything(\"success\"))\\ntime_end=time.time()\\nprint(\\'time cost\\',time_end-time_start,\\'s\\')'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def N_to_anything(word):#名词变成其他词性\n",
    "    suggest_list = []\n",
    "    list_HouZhui = ['y','ful','tive','sive','ed','ing','less','ly','ous','al','able','en','tic','ish','ance','er','or']\n",
    "    list_QianZhui = ['a']\n",
    "    if word[-4:] in ['ment','ness','tion','ture','sion','tive','sive']:\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    else:\n",
    "        for HouZhui in list_HouZhui:\n",
    "            suggest_ = word + HouZhui\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)\n",
    "        for QianZhui in list_QianZhui:\n",
    "            suggest_ = QianZhui + word\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)\n",
    "        if word[-2:] == 'ce':\n",
    "            suggest_ = word[:-2]\n",
    "            suggest_ = syggest_ + 't'\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)        \n",
    "        elif word[-4:] == 'land':\n",
    "            suggest_ = word[:-4]\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word == None:\n",
    "                suggest_ = suggest_ + 'lish'\n",
    "                suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)  \n",
    "        #print(suggest_list)\n",
    "    suggest_list = list(set(suggest_list))\n",
    "    return suggest_list\n",
    "'''import time\n",
    "time_start=time.time()\n",
    "print(N_to_anything(\"success\"))\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['succeeder', 'succeeds', 'succeeded']\n",
      "time cost 0.654491662979126 s\n"
     ]
    }
   ],
   "source": [
    "def V_to_anything(word):#动词变成其他词性\n",
    "    suggest_word = None\n",
    "    suggest_list = []\n",
    "\n",
    "    HouZhui_list = ['ful','tive','sive','ed','less','ly','ous','al','able','en','tic','ish','ance','tion','sion','ment','er','or','ee']\n",
    "    for HouZhui in HouZhui_list:\n",
    "        suggest_ = word + HouZhui\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "\n",
    "    suggest_list = list(set(suggest_list))\n",
    "    return suggest_list\n",
    "\n",
    "time_start=time.time()\n",
    "print(V_to_anything('succeed'))\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    功能：\\n        生成形容词，副词关联词表\\n    输入：\\n        word：形容词/副词\\n    输出：\\n        list_word：为没有添加词的其他形式，包括三音节以下词的比较级最高级\\n        list_word2：为三音节及以上的词的比较级最高级，如果输入形容词比较级最高级没有more/most，该列表为空\\n    说明：\\n        由于三音节形容词/副词的比较级，最高级为more/most+原形容词/副词，所以特别把形容词/副词和其他词性变形区分出来\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['difficult', 'difficulty', 'difficultly'], ['more difficult', 'most difficult'])\n",
      "(['early', 'ear', 'earliest', 'earlier'], [])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    功能：\n",
    "        生成形容词，副词关联词表\n",
    "    输入：\n",
    "        word：形容词/副词\n",
    "    输出：\n",
    "        list_word：为没有添加词的其他形式，包括三音节以下词的比较级最高级\n",
    "        list_word2：为三音节及以上的词的比较级最高级，如果输入形容词比较级最高级没有more/most，该列表为空\n",
    "    说明：\n",
    "        由于三音节形容词/副词的比较级，最高级为more/most+原形容词/副词，所以特别把形容词/副词和其他词性变形区分出来\n",
    "'''\n",
    "\n",
    "def build_like_word_adj(word): #创建类似形容词列表\n",
    "    list_word = []\n",
    "    list_word2 = [] #把比较级最高级带more的放在这里\n",
    "    lemmas = lemmatizer(word, u'adj')\n",
    "    #print(lemmas)\n",
    "    for i in lemmas:\n",
    "        list_word.append(i)\n",
    "        word_er = comparative(i)\n",
    "        if \"more\" in word_er:  #把比较级带more，most的词放在另一个列表list_word2\n",
    "            list_word2.append(word_er)\n",
    "        else:\n",
    "            list_word.append(word_er)\n",
    "        word_est = superlative(i)\n",
    "        if \"most\" in word_est:\n",
    "            list_word2.append(word_est)\n",
    "        else:\n",
    "            list_word.append(word_est)\n",
    "        word_adv = adj_to_adv(i)\n",
    "        if word_adv != None:\n",
    "            list_word.append(word_adv)\n",
    "    list_N = adj_to_anything(word)\n",
    "    for N in list_N:\n",
    "        list_word.append(N)\n",
    "    \n",
    "    list_word = list(set(list_word))\n",
    "    return list_word,list_word2\n",
    "\n",
    "def build_like_word_adv(word): #创建类似形容词列表\n",
    "    list_word = []\n",
    "    list_word2 = []\n",
    "    list_special = ['however','seldom','often','never','otherwise']\n",
    "    if word in list_special:\n",
    "        list_word = [word]\n",
    "        list_word2 = []\n",
    "    else:\n",
    "        lemmas = lemmatizer(word, u'adj')\n",
    "        #print(lemmas)\n",
    "        for i in lemmas:\n",
    "            list_word.append(i)\n",
    "            word_er = comparative(i)\n",
    "            if \"more\" in word_er:\n",
    "                list_word2.append(word_er)\n",
    "            else:\n",
    "                list_word.append(word_er)\n",
    "            word_est = superlative(i)\n",
    "            if \"most\" in word_est:\n",
    "                list_word2.append(word_est)\n",
    "            else:\n",
    "                list_word.append(word_est)\n",
    "            word_adv = adv_to_adj(i)\n",
    "            if word_adv != None:\n",
    "                list_word.append(word_adv)\n",
    "    list_word = list(set(list_word))\n",
    "    return list_word,list_word2\n",
    "\n",
    "\n",
    "\n",
    "print(build_like_word_adj(\"difficult\"))\n",
    "print(build_like_word_adv(\"early\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    功能：\\n        根据检查的位置整理出放入BERT模型的input_ids,input_type_ids以及检查位置在input_ids中的下标位置\\n        pre_training_input_in_sentence得到检查位置所在句子的信息\\n        pre_training_input_entire得到检查位置所在句子的信息\\n    输入：\\n        index：在完整text中的位置\\n    输出：\\n        input_ids：\\n        input_type_ids：\\n        id_in_sen：检查位置在句子中的下标\\n        index：检查位置在完整text中的下标，其实就是输入的下标\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "[101, 2197, 2733, 1045, 2253, 2000, 1996, 4258, 1012, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    功能：\n",
    "        根据检查的位置整理出放入BERT模型的input_ids,input_type_ids以及检查位置在input_ids中的下标位置\n",
    "        pre_training_input_in_sentence得到检查位置所在句子的信息\n",
    "        pre_training_input_entire得到检查位置所在句子的信息\n",
    "    输入：\n",
    "        index：在完整text中的位置\n",
    "    输出：\n",
    "        input_ids：\n",
    "        input_type_ids：\n",
    "        id_in_sen：检查位置在句子中的下标\n",
    "        index：检查位置在完整text中的下标，其实就是输入的下标\n",
    "'''\n",
    "def pre_training_input_in_sentence(index): \n",
    "    sentence_id = in_sentence[index][0]\n",
    "    id_in_sen = in_sentence[index][1]\n",
    "    word = input_ids_sen[sentence_id][id_in_sen]\n",
    "    word = tokenizer.ids_to_tokens[word]\n",
    "    input_ids = copy.deepcopy(input_ids_sen[sentence_id])\n",
    "    input_type_ids = copy.deepcopy(input_type_ids_sen[sentence_id])\n",
    "\n",
    "    return word,input_ids,input_type_ids,id_in_sen\n",
    "\n",
    "def pre_training_input_entire(index): \n",
    "    word = entire_ids[index]\n",
    "    word = tokenizer.ids_to_tokens[word]\n",
    "    input_ids = copy.deepcopy(entire_ids)\n",
    "    input_type_ids = copy.deepcopy(entire_type_ids)\n",
    "\n",
    "    return word,input_ids,input_type_ids,index\n",
    "\n",
    "word,input_ids,input_type_ids,index = pre_training_input_in_sentence(6)\n",
    "print(word)\n",
    "print(input_ids)\n",
    "print(input_type_ids)\n",
    "print(index)\n",
    "#[101, 1045, 2572, 3153, 2006, 1996, 2754, 1012, 102]\n",
    "#[101, 1045, 2572, 3153, 2006, 1996, 2754, 1012, 1045, 2018, 1037, 2200, 2204, 2835, 1012, 1996, 2377, 2001, 2200, 5875, 1012, 102]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "分析各种词性系列函数\n",
    "    功能：对第一遍检查得出的有问题的位置的单词，根据不同的词性进行不同步骤的分析\n",
    "    输入：\n",
    "        index：在原文中的错误位置\n",
    "    输出：\n",
    "        给出的修改建议，修改建议不局限于错误位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'week'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE\n",
    "\n",
    "def analyse_V(index):\n",
    "#这是一个处理动词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "    need_to_will = need_be = 0\n",
    "    list_be = lexeme('be')\n",
    "    list_be = lexeme('be')[:8]\n",
    "    #**************************************判断是不是动词其他形式************************\n",
    "    wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "    if wordV in list_be:\n",
    "        list_word = list_be\n",
    "    else:\n",
    "        list_word = lexeme(wordV)\n",
    "        list_others = V_to_anything(conjugate(verb=wordV,tense=PRESENT,person = 1))\n",
    "        for other in list_others:\n",
    "            list_word.append(other)\n",
    "    #print(\"list_word = \",list_word)\n",
    "    #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    suggestion0,need,_,_= give_suggestion(input_ids,input_type_ids,index,list_word,5)\n",
    "    if need == 1 and suggestion0 != wordV:\n",
    "        return suggestion0\n",
    "    \n",
    "    else:#**************************************判断是不是缺介词***************************\n",
    "        wordV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "        input_ids.insert(id_in_sen + 1,tokenizer.vocab['at'])#就随便插入一个东西，占位子\n",
    "        input_type_ids.append(0)\n",
    "        list_IN = [\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\",\"of\"]\n",
    "        suggestion4,need_IN,_,_ = give_suggestion(input_ids,input_type_ids,id_in_sen + 1,list_IN,1)\n",
    "        if need_IN == 1:\n",
    "            input_ids[id_in_sen + 1] = tokenizer.vocab[suggestion4]\n",
    "            list_word = lexeme(wordV)\n",
    "            suggestion44,need,_,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,3)\n",
    "            if need == 1:\n",
    "                suggestion = suggestion44 + ' ' +suggestion4\n",
    "                return suggestion\n",
    "        #**************************************判断是不是不定式或者将来时***************************    \n",
    "        #print(\"是否用不定式或将来时\")\n",
    "        wordV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "        input_ids.insert(id_in_sen,tokenizer.vocab['to'])#就随便插入一个东西，占位子\n",
    "        input_type_ids.append(0)\n",
    "        input_ids[id_in_sen + 1] = tokenizer.vocab[conjugate(verb=wordV,tense=PRESENT,person = 1)]\n",
    "        #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        suggestion_to_will,need_to_will,prob0,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,[\"to\",\"will\"],1)\n",
    "        if need_to_will == 1:\n",
    "            list_word = [conjugate(verb=wordV,tense=PRESENT,person = 1),conjugate(verb=wordV,tense=PRESENT,aspect=PROGRESSIVE)]\n",
    "            suggestion,need0,_,prob00= give_suggestion(input_ids,input_type_ids,id_in_sen + 1,list_word,5) \n",
    "            \n",
    "        #**********************************判断是不是被动语态或者进行时*******************   \n",
    "\n",
    "        #********************是不是被动语态****************   \n",
    "        #print(\"是不是被动语态\")\n",
    "        wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "        input_ids.insert(index,tokenizer.vocab['be'])#就随便插入一个东西，占位子\n",
    "        try:\n",
    "            input_ids[index + 1]=tokenizer.vocab[conjugate(verb=wordV,tense=PAST,aspect=PROGRESSIVE)]\n",
    "            input_type_ids.append(0)\n",
    "            #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "            suggestion1,need_be1,prob1,_ = give_suggestion(input_ids,input_type_ids,index,list_be,1)\n",
    "        except KeyError:\n",
    "            need_be1 = 0\n",
    "        #********************是不是现在分词****************   \n",
    "        #print(\"是不是进行时\")\n",
    "        try:\n",
    "            input_ids[index + 1]=tokenizer.vocab[conjugate(verb=wordV,tense=PRESENT,aspect=PROGRESSIVE)]\n",
    "            suggestion2,need_be2,prob2,_ = give_suggestion(input_ids,input_type_ids,index,list_be,1)\n",
    "            #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        except KeyError:\n",
    "            need_be2 = 0\n",
    "            \n",
    "        #if need_be1 == 1 or need_be2 == 1:\n",
    "            #print(\"需要be\")\n",
    "        #***************************选择是不定式还是被动语态还是进行时****************************\n",
    "        prob_max = 0\n",
    "        if need_to_will == 1:\n",
    "            prob_max = max(prob_max,prob0)\n",
    "        if need_be1 == 1:\n",
    "            prob_max = max(prob_max,prob1)\n",
    "        if need_be2 == 1:\n",
    "            prob_max = max(prob_max,prob2)\n",
    "\n",
    "        if need_to_will == 1 and prob_max == prob0:\n",
    "            need_be = 0\n",
    "        if need_be1 == 1 and prob_max == prob1:\n",
    "            need_to_will = 0\n",
    "            need_be = 1\n",
    "            be_ = suggestion1\n",
    "        if need_be2 == 1 and prob_max == prob2:\n",
    "            need_to_will = 0\n",
    "            need_be = 1\n",
    "            be_ = suggestion2\n",
    "        #*************************************************处理各种语法******************************************************************\n",
    "        if need_to_will == 1:\n",
    "            wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "            input_ids.insert(index,tokenizer.vocab[suggestion_to_will])\n",
    "            input_type_ids.append(0)\n",
    "            list_word = [conjugate(verb=wordV,tense=PRESENT,person = 1),conjugate(verb=wordV,tense=PRESENT,aspect=PROGRESSIVE)]\n",
    "            suggestion,_,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,5)\n",
    "            return 'to ' + suggestion\n",
    "\n",
    "        elif need_be == 1:\n",
    "            #********************************被动语态或者进行时*****************\n",
    "            wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "            input_ids.insert(index,tokenizer.vocab[be_])\n",
    "            input_type_ids.append(0)\n",
    "            list_word = lexeme(wordV)\n",
    "            suggestion,_,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,5)\n",
    "            suggestion = be_ + ' '+ suggestion\n",
    "        else:\n",
    "            #*****************************************判断该位置是不是动词的其他时态**************************************************************\n",
    "            suggestion = suggestion0\n",
    "\n",
    "        return suggestion\n",
    "    \n",
    "analyse_V(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was unwilling\n"
     ]
    }
   ],
   "source": [
    "def analyse_adj(index):\n",
    "    #这是一个处理形容词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "    wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_entire(index)  \n",
    "    list_word,list_word2 = build_like_word_adj(wordADJ)\n",
    "    #print(list_word)\n",
    "    suggestion0,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,5)\n",
    "    \n",
    "    if need_adj == 1 and suggestion0 != wordADJ:#判断是不是形容词其他变形\n",
    "        return suggestion0\n",
    "    elif get_word(index - 1) in ['more','most'] and len(list_word2) == 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级不需要加more/most，但是前面有more/most\n",
    "        wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        del input_ids[id_in_sen - 1]\n",
    "        del input_type_ids[0]\n",
    "        suggestion3,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen - 1,list_word,6)\n",
    "        return '去掉前面 ' + get_word(index - 1)+ ' 原位置改成 ' + suggestion3\n",
    "    elif get_word(index + 1) in ['##er','##est','##r','##st'] and len(list_word2) != 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级需要more/most，但是错写成形容词+er/est\n",
    "        wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids[id_in_sen + 1] = tokenizer.vocab[wordADJ]\n",
    "        suggestion4,need_bijiao,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,['more','most'],2)\n",
    "        if need_bijiao == 1:\n",
    "            input_ids[id_in_sen] = tokenizer.vocab[suggestion4]\n",
    "            suggestion5,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen+1,list_word,6)\n",
    "            return '去掉后面 '+ get_word(index + 1) + ' 原位置改成 '+ suggestion4 + ' ' + suggestion5  \n",
    "    else:#检查形容词前面是否需要加冠词或者是需要more，most的比较级，最高级抑或是be动词\n",
    "        #print(\"缺冠词或者没用比较级\")\n",
    "        wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "        input_type_ids.append(0)\n",
    "        #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        list_DT = ['the','a','an','this','that','these','those','some','any','all','more','most','am','is','are','was','were'] \n",
    "        suggestion,need_DT,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_DT,1)\n",
    "        if need_DT == 1:\n",
    "            wordADJ,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "            input_ids.insert(index,tokenizer.vocab[suggestion])\n",
    "            input_type_ids.append(0)\n",
    "            #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "            suggestion2,_,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,6)     \n",
    "            return suggestion + ' ' + suggestion2\n",
    "        else:\n",
    "            return suggestion0\n",
    "print(analyse_adj(78))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n"
     ]
    }
   ],
   "source": [
    "def analyse_adv(index):\n",
    "    #这是一个处理形容词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "    need_DT = 0\n",
    "    need_douhao = 0\n",
    "    wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_entire(index)\n",
    "    list_word,list_word2 = build_like_word_adv(wordADV)\n",
    "    suggestion0,need_adv,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,3.5)\n",
    "    if need_adv == 1 and suggestion0 != wordADV:\n",
    "        return suggestion0\n",
    "    elif get_word(index - 1) in ['more','most'] and len(list_word2) == 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级不需要加more/most，但是前面有more/most\n",
    "        wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        del input_ids[id_in_sen - 1]\n",
    "        del input_type_ids[0]\n",
    "        suggestion3,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen - 1,list_word,5)\n",
    "        return '去掉前面 ' + get_word(index - 1)+ ' 原位置改成 ' + suggestion3\n",
    "    elif get_word(index + 1) in ['##er','##est','##r','##st'] and len(list_word2) != 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级需要more/most，但是错写成形容词+er/est\n",
    "        wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids[id_in_sen + 1] = tokenizer.vocab[wordADV]\n",
    "        suggestion4,need_bijiao,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,['more','most'],2)\n",
    "        if need_bijiao == 1:\n",
    "            input_ids[id_in_sen] = tokenizer.vocab[suggestion4]\n",
    "            suggestion5,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen+1,list_word,5)\n",
    "            return '去掉后面 '+ get_word(index + 1) + ' 原位置改成 '+ suggestion4 + ' ' + suggestion5  \n",
    "    else:#检查形容词前面是否需要加冠词或者是需要more，most的比较级，最高级，be动词\n",
    "        wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "        input_ids.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "        input_type_ids.append(0)\n",
    "        #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        list_DT = ['the','a','an','this','that','these','those','some','any','all','more','most','am','is','are','was','were'] \n",
    "        suggestion,need_DT,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_DT,1)\n",
    "        if need_DT == 1:\n",
    "            #print(\"需要冠词\")\n",
    "            wordADV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "            input_ids.insert(index,tokenizer.vocab[suggestion])\n",
    "            input_type_ids.append(0)\n",
    "            #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "            suggestion2,_,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,3)     \n",
    "            return suggestion + ' ' + suggestion2\n",
    "        else:\n",
    "            #副词后面可能缺少逗号，比如 Luckily,I won the game.\n",
    "            wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "            input_ids.insert(id_in_sen + 1,tokenizer.vocab[\",\"])\n",
    "            input_type_ids.append(0)\n",
    "            suggestion3,need_douhao,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,2)\n",
    "            if need_douhao == 1:\n",
    "                return suggestion3 + ' ,'\n",
    "            else:\n",
    "                return suggestion0\n",
    "print(analyse_adv(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanted\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import article,referenced,pluralize, singularize\n",
    "import nltk\n",
    "def analyse_N(index):\n",
    "    #这是一个处理名词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "#******************************************初始数据处理**************************************************************************\n",
    "    need_DT = 0 #表示是否需要在前面加冠词 \n",
    "    wordN,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "    word_tag = nltk.pos_tag([wordN])\n",
    "    if word_tag[0][1] == \"NN\":\n",
    "        N_ = wordN\n",
    "        N_s= pluralize(wordN)\n",
    "    else:\n",
    "        N_ = singularize(wordN)\n",
    "        N_s= wordN\n",
    "    list_word = [N_,N_s]\n",
    "    list_others = N_to_anything(N_)\n",
    "    for other in list_others:\n",
    "        list_word.append(other)\n",
    "    #print(list_word)\n",
    "#*****************************************判断是否需要冠词或者代词************************************************************************   \n",
    "    \n",
    "    input_ids.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "    input_type_ids.append(0)\n",
    "    #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    list_DT = ['the','a','an','this','that','these','those','some','any','all']\n",
    "    suggestion,need_DT,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_DT,1)\n",
    "    if need_DT == 0:#不需要冠词\n",
    "        #print(\"不需要冠词\")\n",
    "        wordN,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "        suggestion,need_DT,_,top_of_list_word = give_suggestion(input_ids,input_type_ids,index,list_word,7)\n",
    "        return suggestion\n",
    "    elif need_DT == 1:\n",
    "        wordN,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "        input_ids.insert(index,tokenizer.vocab[suggestion])\n",
    "        input_type_ids.append(0)\n",
    "        suggestion2,_,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,7)\n",
    "        return suggestion + ' ' + suggestion2\n",
    "\n",
    "print(analyse_N(78))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    这是一个相关代词的词典，容易混淆的词放在一个列表中\\n\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': ['he', 'his', 'him', 'himself', 'who', 'whom', 'whose'], 'his': ['he', 'his', 'him', 'himself', 'who', 'whom', 'whose'], 'him': ['he', 'his', 'him', 'himself', 'who', 'whom', 'whose'], 'himself': ['he', 'his', 'him', 'himself', 'who', 'whom', 'whose'], 'who': ['that', 'which', 'who', 'whom', 'whose', 'as'], 'whom': ['that', 'which', 'who', 'whom', 'whose', 'as'], 'whose': ['that', 'which', 'who', 'whom', 'whose', 'as'], 'she': ['she', 'her', 'herself', 'hers', 'who', 'whom', 'whose'], 'her': ['she', 'her', 'herself', 'hers', 'who', 'whom', 'whose'], 'herself': ['she', 'her', 'herself', 'hers', 'who', 'whom', 'whose'], 'hers': ['she', 'her', 'herself', 'hers', 'who', 'whom', 'whose'], 'it': ['it', 'its', 'itself', 'who', 'whom', 'whose'], 'its': ['it', 'its', 'itself', 'who', 'whom', 'whose'], 'itself': ['it', 'its', 'itself', 'who', 'whom', 'whose'], 'i': ['i', 'me', 'my', 'myself', 'mine'], 'me': ['i', 'me', 'my', 'myself', 'mine'], 'my': ['i', 'me', 'my', 'myself', 'mine'], 'myself': ['i', 'me', 'my', 'myself', 'mine'], 'mine': ['i', 'me', 'my', 'myself', 'mine'], 'you': ['you', 'your', 'yourself', 'yourselves'], 'your': ['you', 'your', 'yourself', 'yourselves'], 'yourself': ['you', 'your', 'yourself', 'yourselves'], 'yourselves': ['you', 'your', 'yourself', 'yourselves'], 'we': ['we', 'us', 'our', 'ours', 'ourselves'], 'us': ['we', 'us', 'our', 'ours', 'ourselves'], 'our': ['we', 'us', 'our', 'ours', 'ourselves'], 'ours': ['we', 'us', 'our', 'ours', 'ourselves'], 'ourselves': ['we', 'us', 'our', 'ours', 'ourselves'], 'they': ['they', 'them', 'their', 'theirs'], 'them': ['they', 'them', 'their', 'theirs'], 'their': ['they', 'them', 'their', 'theirs'], 'theirs': ['they', 'them', 'their', 'theirs'], 'this': ['this', 'these'], 'these': ['this', 'these'], 'that': ['that', 'which', 'who', 'whom', 'whose', 'as'], 'those': ['that', 'those'], 'which': ['that', 'which', 'who', 'whom', 'whose', 'as'], 'what': ['who', 'whom', 'whose', 'which', 'what', 'whoever', 'whichever', 'whatever'], 'whoever': ['who', 'whom', 'whose', 'which', 'what', 'whoever', 'whichever', 'whatever'], 'whichever': ['who', 'whom', 'whose', 'which', 'what', 'whoever', 'whichever', 'whatever'], 'whatever': ['who', 'whom', 'whose', 'which', 'what', 'whoever', 'whichever', 'whatever'], 'as': ['that', 'which', 'who', 'whom', 'whose', 'as'], 'some': ['some', 'any'], 'any': ['some', 'any'], 'few': ['few', 'little'], 'little': ['few', 'little'], 'many': ['many', 'much'], 'much': ['many', 'much'], 'another': ['another', 'other'], 'other': ['another', 'other']}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    这是一个相关代词的词典，容易混淆的词放在一个列表中\n",
    "\n",
    "'''\n",
    "like_he = ['he','his','him','himself','who', 'whom', 'whose']\n",
    "like_she = ['she','her','herself','hers','who', 'whom', 'whose']\n",
    "like_it = ['it','its','itself','who', 'whom', 'whose']\n",
    "like_i = ['i','me','my','myself','mine']\n",
    "like_you = ['you','your','yourself','yourselves']\n",
    "like_we = ['we','us','our','ours','ourselves']\n",
    "like_they = ['they','them','their','theirs']\n",
    "\n",
    "like_this = ['this', 'these'] \n",
    "like_that = ['that','those'] \n",
    "pronoun_Question = ['who', 'whom', 'whose', 'which', 'what', 'whoever', 'whichever', 'whatever'] #疑问代词\n",
    "pronoun_relation =  ['that', 'which', 'who', 'whom', 'whose', 'as'] #关系代词\n",
    "like_some = ['some','any']\n",
    "like_few = ['few','little']\n",
    "like_many = ['many','much']\n",
    "like_other = ['another','other']\n",
    "\n",
    "pronoun = [like_he,like_she,like_it,like_i,like_you,like_we,like_they,like_this,like_that,pronoun_Question,pronoun_relation,like_some,like_few,like_many,like_other]\n",
    "pronoun_dictionary = {}\n",
    "\n",
    "for list_word in pronoun:\n",
    "    for word in list_word:\n",
    "        pronoun_dictionary.update({word:list_word})\n",
    "print(pronoun_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'night'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a4fd0c1c6763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msuggestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgive_suggestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuggestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyse_pronoun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-a4fd0c1c6763>\u001b[0m in \u001b[0;36manalyse_pronoun\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#这是一个处理代词语法问题的函数，输入为问题词在text的token中的下标index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwordPROP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_training_input_entire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlist_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpronoun_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordPROP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msuggestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgive_suggestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuggestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'night'"
     ]
    }
   ],
   "source": [
    "def analyse_pronoun(index):\n",
    "    #这是一个处理代词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "    wordPROP,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "    list_word = pronoun_dictionary[wordPROP]\n",
    "    suggestion,_,_,_= give_suggestion(input_ids,input_type_ids,index,list_word,3)\n",
    "    return suggestion\n",
    "print(analyse_pronoun(14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释一下，有很多副词确实也不是ly形式结尾的,比如用在三音节形容词前面的比较级more，most，还有频度副词often，seldom，never这种。因为这些词比较不容易用错，先暂时不考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    }
   ],
   "source": [
    "def analyse_DT(index):\n",
    "    #检查冠词，检查是不是用别的冠词，或者是去掉会不会更好\n",
    "    wordDT,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)  \n",
    "    if wordDT in ['all',\"every\",'per']:\n",
    "        return wordDT\n",
    "    else:\n",
    "        if wordDT in ['some','any']:\n",
    "            list_word = ['some','any']\n",
    "        elif wordDT in ['this','that','these','those']:\n",
    "            list_word = ['this','that','these','those']\n",
    "        elif wordDT in ['the','a','an']:\n",
    "            list_word = ['the','a','an']\n",
    "        elif wordDT in ['another','other']:\n",
    "            list_word = ['another','other']\n",
    "        else:\n",
    "            list_word = []\n",
    "        suggestion0,need_DT,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,1)\n",
    "        if wordDT in ['some','any','this','that','these','those','another','other','the','a','an']:\n",
    "            if need_DT == 1:\n",
    "                return suggestion0\n",
    "            else:\n",
    "                return \"去掉 \" + get_word(index)\n",
    "        else:\n",
    "            return wordDT\n",
    "    \n",
    "print(analyse_DT(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but\n"
     ]
    }
   ],
   "source": [
    "def analyse_IN(index):\n",
    "    #检查介词是否需要去掉\n",
    "    wordIN,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "    list_word = [\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\",\"of\",'to']\n",
    "    suggestion0,need_IN,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,3)\n",
    "    if need_IN == 1:\n",
    "        return suggestion0\n",
    "    else:\n",
    "        if wordIN in list_word:\n",
    "            return \"去掉 \" + get_word(index)\n",
    "        else:\n",
    "            return suggestion0\n",
    "    \n",
    "print(analyse_IN(76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    这是一个输出BERT模型训练结果的函数，方便查看调试\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG\n",
    "'''\n",
    "    这是一个输出BERT模型训练结果的函数，方便查看调试\n",
    "'''\n",
    "def show_lm_probs(tokens, input_ids, probs, topk=5, firstk=20): #输出结果的函数，要最高概率topk个输出\n",
    "    def print_pair(token, prob, end_str='', hit_mark=' '):\n",
    "        if i < firstk:\n",
    "            # token = token.replace('</w>', '').replace('\\n', '/n')\n",
    "            print('{}{: >3} | {: <12}'.format(hit_mark, int(round(prob*100)), token), end=end_str)\n",
    "    \n",
    "    ret = None\n",
    "    for i in range(len(tokens)):\n",
    "        ind_ = input_ids[i].item() if input_ids is not None else tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item() #这个probs是该字符串第i个位置上填上词典上各个词的概率，prob_是词典上原来天的这个词的概率\n",
    "        print_pair(tokens[i], prob_, end_str='\\t')\n",
    "        values, indices = probs[i].topk(topk)\n",
    "        #print(values, indices)\n",
    "        #print(\"****************************************************************************************************\")\n",
    "        top_pairs = []\n",
    "        for j in range(topk):\n",
    "            ind, prob = indices[j].item(), values[j].item()\n",
    "            hit_mark = '*' if ind == ind_ else ' '\n",
    "            token = tokenizer.ids_to_tokens[ind]\n",
    "            print_pair(token, prob, hit_mark=hit_mark, end_str='' if j < topk - 1 else '\\n')\n",
    "            top_pairs.append((token, prob))\n",
    "        if tokens[i] == \"[MASK]\":\n",
    "            ret = top_pairs\n",
    "    return ret #返回的这是个啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    功能：\\n        judge_and_suggestion系列函数，这个系列函数是在analyse之前做的一个预先判断处理，判断的是该位置原来词的相关词中有没有可以代替它的词\\n        当相关词中有词的可能性和原词的可能性的差距大于阈值，则认为原词是错的\\n    输入：\\n        prob：该位置可能性列表\\n        original：该位置原先的词\\n        list_word：该位置相关词表\\n        threhold：门槛，也就是阈值\\n    输出：\\n        judge：判断原来的词是否正确，0表示需要换词，1表示不需要换词或者说相关词里面没一个合适的\\n        suggestion：相关词中最好的推荐\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from pattern import en\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE\n",
    "'''\n",
    "    功能：\n",
    "        judge_and_suggestion系列函数，这个系列函数是在analyse之前做的一个预先判断处理，判断的是该位置原来词的相关词中有没有可以代替它的词\n",
    "        当相关词中有词的可能性和原词的可能性的差距大于阈值，则认为原词是错的\n",
    "    输入：\n",
    "        prob：该位置可能性列表\n",
    "        original：该位置原先的词\n",
    "        list_word：该位置相关词表\n",
    "        threhold：门槛，也就是阈值\n",
    "    输出：\n",
    "        judge：判断原来的词是否正确，0表示需要换词，1表示不需要换词或者说相关词里面没一个合适的\n",
    "        suggestion：相关词中最好的推荐\n",
    "'''\n",
    "def judge_and_suggestion(prob,original,list_word,threhold):\n",
    "    top_prob = 0\n",
    "    original_prob = prob[tokenizer.vocab[original]]\n",
    "    best = None\n",
    "    suggestion = None\n",
    "    for word in list_word:\n",
    "        try:\n",
    "            word_id = tokenizer.vocab[word]\n",
    "            prob_word = prob[word_id]\n",
    "            if prob_word > top_prob:\n",
    "                top_prob = prob_word\n",
    "                best_word = word\n",
    "        except KeyError:\n",
    "            pass\n",
    "    #print(best_word,top_prob)\n",
    "    #print(original,original_prob)\n",
    "    gap = math.log(top_prob) - math.log(original_prob)\n",
    "    #print(gap)\n",
    "    if gap > threhold:\n",
    "        suggestion = best_word\n",
    "        return 0,suggestion\n",
    "    else:\n",
    "        return 1,suggestion\n",
    "def judge_CC_and_suggestion(prob,original_CC):\n",
    "    list_CC = [\"but\",\"yet\",\"still\",\"however\",\"although\",\"for\",\"so\",\"thus\",\"and\",\"or\",\"too\",\"again\",\"another\",\"either\",\"or\",\"neither\",\"nor\",\"when\",\"while\",\"as\",\"whenever\",\"since\",\"until\",\"till\"]\n",
    "    judge,suggestion = judge_and_suggestion(prob,original_CC,list_CC,2)\n",
    "    return judge,suggestion\n",
    "def judge_V_and_suggestion(prob,original_V):\n",
    "    list_V = lexeme(original_V)\n",
    "    judge,suggestion = judge_and_suggestion(prob,original_V,list_V,2)\n",
    "    #print(\"检查点\",judge,suggestion)\n",
    "    return judge,suggestion\n",
    "    \n",
    "def judge_IN_and_suggestion(prob,original_IN):\n",
    "    list_IN = [\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\",\"of\",'to']\n",
    "    judge,suggestion = judge_and_suggestion(prob,original_IN,list_IN,1)\n",
    "    return judge,suggestion\n",
    "def judge_DT_and_suggestion(prob,original_DT):\n",
    "    if original_DT in ['some','any']:\n",
    "        list_word = ['some','any']\n",
    "    elif original_DT in ['this','that','these','those']:\n",
    "        list_word = ['this','that','these','those']\n",
    "    elif original_DT in ['the','a','an']:\n",
    "        list_word = ['the','a','an']\n",
    "    elif original_DT in ['another','other']:\n",
    "        list_word = ['another','other']\n",
    "    judge,suggestion = judge_and_suggestion(prob,original_DT,list_DT,1)\n",
    "    return judge,suggestion\n",
    "\n",
    "def judge_MD_and_suggestion(prob,original_MD):\n",
    "    if original_MD in ['can','could']:\n",
    "        list_MD = ['can','could']\n",
    "    elif original_MD in ['may','might']:\n",
    "        list_MD = ['may','might']\n",
    "    elif original_MD in ['shall','should']:\n",
    "        list_MD = ['shall','should']   \n",
    "    elif original_MD in ['will','would']:\n",
    "        list_MD = ['will','would']  \n",
    "    elif original_MD in ['dare','dared']:\n",
    "        list_MD = ['dare','dared']  \n",
    "    else:\n",
    "        list_MD = []\n",
    "    judge,suggestion = judge_and_suggestion(prob,original_MD,list_MD,1)\n",
    "    if original_MD not in ['can','could','may','might','shall','should','will','would'] :\n",
    "        return judge,suggestion\n",
    "    else:\n",
    "        return 1,None\n",
    "    \n",
    "def judge_N_and_suggestion(prob,original_N):\n",
    "    word_tag = nltk.pos_tag([original_N])\n",
    "    if word_tag[0][1] == \"NN\":\n",
    "        N_ = original_N\n",
    "        N_s= pluralize(original_N)\n",
    "    else:\n",
    "        N_ = singularize(original_N)\n",
    "        N_s= original_N\n",
    "    list_N = [N_,N_s]\n",
    "    list_others = N_to_anything(N_)\n",
    "    for other in list_others:\n",
    "        list_N.append(other)\n",
    "    judge,suggestion = judge_and_suggestion(prob,original_N,list_N,0.5)\n",
    "    return judge,suggestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "suggestions = {} #\n",
    "def show_abnormals(tokens, probs, show_suggestions=False):\n",
    "    global suggestions\n",
    "    suggestions = {} \n",
    "    def gap2color(gap):\n",
    "        if gap <= 5:\n",
    "            return 'yellow_1'\n",
    "        elif gap <= 10:\n",
    "            return 'orange_1'\n",
    "        else:\n",
    "            return 'red_1'\n",
    "        \n",
    "    def print_token(token, suggestion, gap):\n",
    "        if gap == 0:\n",
    "            print(stylize(token + ' ', colored.fg('white') + colored.bg('black')), end='')\n",
    "        else:\n",
    "            print(stylize(token, colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "            if show_suggestions and gap > 5:\n",
    "                print(stylize('/' + suggestion + ' ', colored.fg('green' if gap > 10 else 'cyan') + colored.bg('black')), end='')\n",
    "            else:\n",
    "                print(stylize(' ', colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "                # print('/' + suggestion, end=' ')\n",
    "            # print('%.2f' % gap, end=' ')\n",
    "        \n",
    "    avg_gap = 0.\n",
    "    tokens_tag = nltk.pos_tag(tokens)\n",
    "    #print(tokens_tag)\n",
    "    for i in range(1, len(tokens) - 1):  # skip first [CLS] and last [SEP]\n",
    "        ind_ = tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item()\n",
    "        top_prob = probs[i].max().item()\n",
    "        top_ind = probs[i].argmax().item()\n",
    "        top_word = tokenizer.ids_to_tokens[top_ind]\n",
    "        gap = math.log(top_prob) - math.log(prob_) #计算两个词之间的差距\n",
    "        print()\n",
    "        print(\"*******************************************************************************************************************\")\n",
    "        print(i)\n",
    "        print(gap)\n",
    "        avg_gap += gap\n",
    "        #suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        suggestion = None\n",
    "        #tag = tokens_tag[i][1]\n",
    "        #doc = nlp(tokens[i])\n",
    "        #tag = doc[0].tag_\n",
    "        tag = tokens_tag[i][1]\n",
    "        #print(tokens_tag[i])\n",
    "        print(tag)\n",
    "        if 'VB' in tag:\n",
    "            if gap>3 and top_word in [\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\",\"of\",'to']:\n",
    "                suggestion = analyse_V(i)   #如果推荐的是介词，说明这个位置可能需要补充什么 \n",
    "            elif gap > 7.5:\n",
    "                suggestion = analyse_V(i)\n",
    "            elif gap < 7.5 and gap > 3:\n",
    "                judge,suggestion = judge_V_and_suggestion(probs[i],tokens[i])\n",
    "                if judge == 0 :\n",
    "                    gap = 6\n",
    "                else:\n",
    "                    gap = 3\n",
    "        elif \"DT\" == tag and gap > 3:\n",
    "            suggestion = analyse_DT(i)\n",
    "        elif \"JJ\" in tag :\n",
    "            if gap > 6:\n",
    "                suggestion = analyse_adj(i)\n",
    "            else:\n",
    "                gap = 3\n",
    "        elif \"RB\" in tag and gap > 5:\n",
    "            suggestion = analyse_adv(i)\n",
    "            \n",
    "        elif \"PRP\" in tag and gap >5:\n",
    "            suggestion = analyse_pronoun(i)\n",
    "        elif \"NN\" in tag:\n",
    "            if gap > 4 and tokens[i][:2]==\"##\" and suggestions.__contains__(i-1)==False:\n",
    "                #如果gap>4并且该位置是后缀，并且前一个位置被建议修改，说明该位置需要去掉\n",
    "                suggestion = '去掉' + ' ' + tokens[i]\n",
    "            elif gap > 7.5:\n",
    "                suggestion = analyse_N(i)\n",
    "            elif gap < 7.5 and gap > 2:\n",
    "                judge,suggestion = judge_N_and_suggestion(probs[i],tokens[i])\n",
    "                if judge == 0 :\n",
    "                    gap = 6\n",
    "                else:\n",
    "                    gap = 3\n",
    "        elif \"CC\" in tag and gap > 2 :\n",
    "            judge,suggestion = judge_CC_and_suggestion(probs[i],tokens[i])\n",
    "            if judge == 1 :\n",
    "                gap = 3\n",
    "\n",
    "        elif (\"IN\" == tag or 'TO' == tag) and gap > 2:\n",
    "            suggestion = analyse_IN(i)\n",
    "            \n",
    "        elif 'MD' in tag and gap > 5:\n",
    "            print(\"检查点1*****************************************************\")\n",
    "            judge,suggestion = judge_MD_and_suggestion(probs[i],tokens[i])\n",
    "            if judge == 1:\n",
    "                gap = 3\n",
    "                \n",
    "        elif \"CD\" in tag:\n",
    "            gap = 0  \n",
    "            \n",
    "        elif \"WDT\" == tag and gap > 2: #who，which，that那些\n",
    "            suggestion = top_word\n",
    "            \n",
    "        elif gap > 5:\n",
    "            suggestion = top_word\n",
    "            \n",
    "        if suggestion != tokens[i] and suggestion != None:\n",
    "            suggestions.update({i:suggestion})\n",
    "            gap = max(gap,6)\n",
    "        else:\n",
    "            gap = min(gap,3)\n",
    "        print_token(tokens[i], suggestion, gap)\n",
    "        \n",
    "    avg_gap /= (len(tokens) - 2)\n",
    "    print()\n",
    "    print('平均gap:'+ str(avg_gap))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[101,\n",
       "   2043,\n",
       "   1045,\n",
       "   2001,\n",
       "   2210,\n",
       "   1010,\n",
       "   5958,\n",
       "   1005,\n",
       "   1055,\n",
       "   2305,\n",
       "   2001,\n",
       "   2256,\n",
       "   2155,\n",
       "   2208,\n",
       "   2305,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2044,\n",
       "   15264,\n",
       "   1010,\n",
       "   2057,\n",
       "   2052,\n",
       "   2377,\n",
       "   4003,\n",
       "   2399,\n",
       "   1997,\n",
       "   2035,\n",
       "   4066,\n",
       "   1999,\n",
       "   1996,\n",
       "   3564,\n",
       "   2282,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2004,\n",
       "   1996,\n",
       "   4845,\n",
       "   1010,\n",
       "   1045,\n",
       "   3866,\n",
       "   2000,\n",
       "   3422,\n",
       "   13941,\n",
       "   1989,\n",
       "   2021,\n",
       "   2053,\n",
       "   3043,\n",
       "   2129,\n",
       "   2116,\n",
       "   2335,\n",
       "   1045,\n",
       "   2356,\n",
       "   2000,\n",
       "   3666,\n",
       "   2068,\n",
       "   1989,\n",
       "   2026,\n",
       "   3008,\n",
       "   2052,\n",
       "   2025,\n",
       "   2000,\n",
       "   2292,\n",
       "   2033,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2027,\n",
       "   2052,\n",
       "   2360,\n",
       "   2000,\n",
       "   2149,\n",
       "   2008,\n",
       "   2652,\n",
       "   4003,\n",
       "   2399,\n",
       "   2052,\n",
       "   2393,\n",
       "   2026,\n",
       "   4167,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2145,\n",
       "   1045,\n",
       "   15175,\n",
       "   2000,\n",
       "   2377,\n",
       "   1996,\n",
       "   2399,\n",
       "   2005,\n",
       "   2068,\n",
       "   2823,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   1045,\n",
       "   2134,\n",
       "   1005,\n",
       "   1056,\n",
       "   5382,\n",
       "   2129,\n",
       "   2157,\n",
       "   2026,\n",
       "   3008,\n",
       "   2024,\n",
       "   2127,\n",
       "   1045,\n",
       "   3133,\n",
       "   2152,\n",
       "   2082,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   1996,\n",
       "   2399,\n",
       "   2026,\n",
       "   3008,\n",
       "   4036,\n",
       "   2033,\n",
       "   2073,\n",
       "   1045,\n",
       "   2001,\n",
       "   1037,\n",
       "   2775,\n",
       "   2357,\n",
       "   2041,\n",
       "   2000,\n",
       "   2022,\n",
       "   2200,\n",
       "   6179,\n",
       "   2101,\n",
       "   1999,\n",
       "   2026,\n",
       "   2166,\n",
       "   1012,\n",
       "   102],\n",
       "  [101, 1012, 102]],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [0, 4],\n",
       "  [0, 5],\n",
       "  [0, 6],\n",
       "  [0, 7],\n",
       "  [0, 8],\n",
       "  [0, 9],\n",
       "  [0, 10],\n",
       "  [0, 11],\n",
       "  [0, 12],\n",
       "  [0, 13],\n",
       "  [0, 14],\n",
       "  [0, 15],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [1, 4],\n",
       "  [1, 5],\n",
       "  [1, 6],\n",
       "  [1, 7],\n",
       "  [1, 8],\n",
       "  [1, 9],\n",
       "  [1, 10],\n",
       "  [1, 11],\n",
       "  [1, 12],\n",
       "  [1, 13],\n",
       "  [1, 14],\n",
       "  [1, 15],\n",
       "  [1, 16],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [2, 4],\n",
       "  [2, 5],\n",
       "  [2, 6],\n",
       "  [2, 7],\n",
       "  [2, 8],\n",
       "  [2, 9],\n",
       "  [2, 10],\n",
       "  [2, 11],\n",
       "  [2, 12],\n",
       "  [2, 13],\n",
       "  [2, 14],\n",
       "  [2, 15],\n",
       "  [2, 16],\n",
       "  [2, 17],\n",
       "  [2, 18],\n",
       "  [2, 19],\n",
       "  [2, 20],\n",
       "  [2, 21],\n",
       "  [2, 22],\n",
       "  [2, 23],\n",
       "  [2, 24],\n",
       "  [2, 25],\n",
       "  [2, 26],\n",
       "  [2, 27],\n",
       "  [2, 28],\n",
       "  [2, 29],\n",
       "  [2, 30],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [3, 4],\n",
       "  [3, 5],\n",
       "  [3, 6],\n",
       "  [3, 7],\n",
       "  [3, 8],\n",
       "  [3, 9],\n",
       "  [3, 10],\n",
       "  [3, 11],\n",
       "  [3, 12],\n",
       "  [3, 13],\n",
       "  [3, 14],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3],\n",
       "  [4, 4],\n",
       "  [4, 5],\n",
       "  [4, 6],\n",
       "  [4, 7],\n",
       "  [4, 8],\n",
       "  [4, 9],\n",
       "  [4, 10],\n",
       "  [4, 11],\n",
       "  [5, 1],\n",
       "  [5, 2],\n",
       "  [5, 3],\n",
       "  [5, 4],\n",
       "  [5, 5],\n",
       "  [5, 6],\n",
       "  [5, 7],\n",
       "  [5, 8],\n",
       "  [5, 9],\n",
       "  [5, 10],\n",
       "  [5, 11],\n",
       "  [5, 12],\n",
       "  [5, 13],\n",
       "  [5, 14],\n",
       "  [5, 15],\n",
       "  [5, 16],\n",
       "  [6, 1],\n",
       "  [6, 2],\n",
       "  [6, 3],\n",
       "  [6, 4],\n",
       "  [6, 5],\n",
       "  [6, 6],\n",
       "  [6, 7],\n",
       "  [6, 8],\n",
       "  [6, 9],\n",
       "  [6, 10],\n",
       "  [6, 11],\n",
       "  [6, 12],\n",
       "  [6, 13],\n",
       "  [6, 14],\n",
       "  [6, 15],\n",
       "  [6, 16],\n",
       "  [6, 17],\n",
       "  [6, 18],\n",
       "  [6, 19],\n",
       "  [6, 20],\n",
       "  [6, 21],\n",
       "  [6, 22]],\n",
       " [\"When I was little, Friday's night was our family game night.\",\n",
       "  ' After supper, we would play card games of all sort in the sitting room.',\n",
       "  ' As the kid, I loved to watch cartoons，but no matter how many times I asked to watching them， my parents would not to let me.',\n",
       "  ' They would say to us that playing card games would help my brain.',\n",
       "  ' Still I unwilling to play the games for them sometimes.',\n",
       "  \" I didn't realize how right my parents are until I entered high school.\",\n",
       "  ' The games my parents taught me where I was a child turned out to be very useful later in my life.',\n",
       "  '.'],\n",
       " [101,\n",
       "  2043,\n",
       "  1045,\n",
       "  2001,\n",
       "  2210,\n",
       "  1010,\n",
       "  5958,\n",
       "  1005,\n",
       "  1055,\n",
       "  2305,\n",
       "  2001,\n",
       "  2256,\n",
       "  2155,\n",
       "  2208,\n",
       "  2305,\n",
       "  1012,\n",
       "  2044,\n",
       "  15264,\n",
       "  1010,\n",
       "  2057,\n",
       "  2052,\n",
       "  2377,\n",
       "  4003,\n",
       "  2399,\n",
       "  1997,\n",
       "  2035,\n",
       "  4066,\n",
       "  1999,\n",
       "  1996,\n",
       "  3564,\n",
       "  2282,\n",
       "  1012,\n",
       "  2004,\n",
       "  1996,\n",
       "  4845,\n",
       "  1010,\n",
       "  1045,\n",
       "  3866,\n",
       "  2000,\n",
       "  3422,\n",
       "  13941,\n",
       "  1989,\n",
       "  2021,\n",
       "  2053,\n",
       "  3043,\n",
       "  2129,\n",
       "  2116,\n",
       "  2335,\n",
       "  1045,\n",
       "  2356,\n",
       "  2000,\n",
       "  3666,\n",
       "  2068,\n",
       "  1989,\n",
       "  2026,\n",
       "  3008,\n",
       "  2052,\n",
       "  2025,\n",
       "  2000,\n",
       "  2292,\n",
       "  2033,\n",
       "  1012,\n",
       "  2027,\n",
       "  2052,\n",
       "  2360,\n",
       "  2000,\n",
       "  2149,\n",
       "  2008,\n",
       "  2652,\n",
       "  4003,\n",
       "  2399,\n",
       "  2052,\n",
       "  2393,\n",
       "  2026,\n",
       "  4167,\n",
       "  1012,\n",
       "  2145,\n",
       "  1045,\n",
       "  15175,\n",
       "  2000,\n",
       "  2377,\n",
       "  1996,\n",
       "  2399,\n",
       "  2005,\n",
       "  2068,\n",
       "  2823,\n",
       "  1012,\n",
       "  1045,\n",
       "  2134,\n",
       "  1005,\n",
       "  1056,\n",
       "  5382,\n",
       "  2129,\n",
       "  2157,\n",
       "  2026,\n",
       "  3008,\n",
       "  2024,\n",
       "  2127,\n",
       "  1045,\n",
       "  3133,\n",
       "  2152,\n",
       "  2082,\n",
       "  1012,\n",
       "  1996,\n",
       "  2399,\n",
       "  2026,\n",
       "  3008,\n",
       "  4036,\n",
       "  2033,\n",
       "  2073,\n",
       "  1045,\n",
       "  2001,\n",
       "  1037,\n",
       "  2775,\n",
       "  2357,\n",
       "  2041,\n",
       "  2000,\n",
       "  2022,\n",
       "  2200,\n",
       "  6179,\n",
       "  2101,\n",
       "  1999,\n",
       "  2026,\n",
       "  2166,\n",
       "  1012,\n",
       "  102],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids\n",
    "\n",
    "def analyze_text(text, masked_tokens=None, show_suggestions=True, show_firstk_probs=20):\n",
    "    step = 15\n",
    "    #print(text[0])\n",
    "    global input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids\n",
    "    input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids = process_text(text[0])\n",
    "\n",
    "    examples = convert_text_to_examples(text)\n",
    "    features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "    given_mask = \"[MASK]\" in features[0].tokens\n",
    "    if not given_mask or masked_tokens is not None:\n",
    "        assert len(features) == 1\n",
    "        features, batches = copy_and_mask_feature(features[0],step, masked_tokens=masked_tokens)\n",
    "        #print(len(features))\n",
    "\n",
    "    input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) #把input_ids增加了一个维度，变成[n_features,sequence_len]\n",
    "    #这里的n_features实际上是句子有多少批训练\n",
    "\n",
    "    input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    input_ids = input_ids.to(device) #拿去GPU\n",
    "    input_type_ids = input_type_ids.to(device)\n",
    "\n",
    "    mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "    mlm_probs = F.softmax(mlm_logits, dim=-1) #最后一维，也就是vocab 换算成概率和为百分之百\n",
    "    #print(mlm_probs.size())#这里实验的是torch.Size([5, 5, 30522])\n",
    "    tokens = features[0].tokens #为了输出，[mask]在input_ids里面表示出来，features的token都一样\n",
    "    #print(tokens)\n",
    "    if not given_mask or masked_tokens is not None:\n",
    "        bsz, seq_len, vocab_size = mlm_probs.size() #三个维度分别是batch_size, sequence_length, vocab_size\n",
    "        assert bsz == len(batches)\n",
    "        # reduced_mlm_probs = torch.Tensor(1, seq_len, vocab_size)\n",
    "        # for i in range(seq_len):\n",
    "        #    reduced_mlm_probs[0, i] = mlm_probs[i, i]\n",
    "        reduced_mlm_probs = torch.Tensor(1, len(tokens), vocab_size)\n",
    "        for i in batches:\n",
    "            pos = i\n",
    "            while pos < len(tokens):\n",
    "                reduced_mlm_probs[0, pos] = mlm_probs[i, pos]\n",
    "                pos = pos + step\n",
    "        mlm_probs = reduced_mlm_probs #压缩一下大小，节约不必要浪费的空间（只需要第i个batch里面[mask]位置的词汇表概率即可）\n",
    "        #tokens = [tokens[i] for i in masked_positions]\n",
    "    top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=show_firstk_probs) #传入的probs是二维的\n",
    "    #print(top_pairs) #******************************\n",
    "    if not given_mask:\n",
    "        show_abnormals(tokens, mlm_probs[0], show_suggestions=show_suggestions)\n",
    "    #return top_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/21/2019 18:19:51 - INFO - examples.extract_features -   tokens: [CLS] when i was little , friday ' s night was our family game night . after supper , we would play card games of all sort in the sitting room . as the kid , i loved to watch cartoons ， but no matter how many times i asked to watching them ， my parents would not to let me . they would say to us that playing card games would help my brain . still i unwilling to play the games for them sometimes . i didn ' t realize how right my parents are until i entered high school . the games my parents taught me where i was a child turned out to be very useful later in my life . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 | [CLS]       \t   2 | .              1 | the            1 | )              1 | \"              1 | ,           \n",
      "  97 | when        \t* 97 | when           2 | since          1 | until          0 | while          0 | before      \n",
      "  99 | i           \t* 99 | i              0 | she            0 | he             0 | we             0 | me          \n",
      " 100 | was         \t*100 | was            0 | were           0 | got            0 | turned         0 | is          \n",
      "  11 | little      \t  19 | younger     * 11 | little         8 | eight          7 | young          7 | twelve      \n",
      "  51 | ,           \t* 51 | ,             24 | .              4 | and            1 | of             1 | the         \n",
      "   0 | friday      \t  33 | valentine     16 | mother         7 | children       7 | father         5 | grandma     \n",
      " 100 | '           \t*100 | '              0 | `              0 | ′              0 | \"              0 | *           \n",
      " 100 | s           \t*100 | s              0 | til            0 | n              0 | d              0 | round       \n",
      "  39 | night       \t* 39 | night         16 | dinner         6 | eve            5 | day            5 | supper      \n",
      "  90 | was         \t* 90 | was            8 | became         1 | is             0 | were           0 | ,           \n",
      "   4 | our         \t  79 | a             13 | the         *  4 | our            1 | my             0 | their       \n",
      "   1 | family      \t  59 | favorite      18 | first          3 | favourite      2 | only           1 | last        \n",
      "   3 | game        \t  12 | dinner         7 | christmas      6 | fun            5 | day         *  3 | game        \n",
      "  81 | night       \t* 81 | night         13 | day            1 | dinner         1 | date           0 | nights      \n",
      "  97 | .           \t* 97 | .              2 | and            0 | ;              0 | ,              0 | !           \n",
      "  80 | after       \t* 80 | after          6 | during         4 | at             4 | over           3 | before      \n",
      "   1 | supper      \t  68 | school        10 | dinner         9 | that           2 | midnight       1 | breakfast   \n",
      " 100 | ,           \t*100 | ,              0 | ##time         0 | together       0 | time           0 | dinner      \n",
      "  98 | we          \t* 98 | we             1 | i              0 | they           0 | everyone       0 | people      \n",
      "  64 | would       \t* 64 | would         21 | could          2 | will           2 | can            1 | did         \n",
      "  96 | play        \t* 96 | play           2 | have           1 | watch          0 | enjoy          0 | played      \n",
      "  97 | card        \t* 97 | card           1 | board          1 | cards          0 | video          0 | computer    \n",
      " 100 | games       \t*100 | games          0 | game           0 | ##games        0 | matches        0 | sports      \n",
      "  99 | of          \t* 99 | of             0 | in             0 | with           0 | and            0 | ,           \n",
      "   0 | all         \t  85 | some           6 | any            3 | a              3 | every          1 | this        \n",
      "   2 | sort        \t  41 | kinds         34 | types          5 | sorts          4 | sizes          3 | kind        \n",
      "  98 | in          \t* 98 | in             0 | around         0 | inside         0 | at             0 | from        \n",
      "  73 | the         \t* 73 | the           23 | our            1 | my             1 | a              1 | their       \n",
      "   0 | sitting     \t  56 | family        17 | dining        14 | living         1 | back           1 | same        \n",
      "  99 | room        \t* 99 | room           0 | area           0 | rooms          0 | hall           0 | areas       \n",
      "  99 | .           \t* 99 | .              1 | and            0 | ;              0 | ,              0 | ...         \n",
      "  48 | as          \t* 48 | as            29 | like           8 | being          3 | unlike         3 | for         \n",
      "   0 | the         \t 100 | a              0 | another        0 | an          *  0 | the            0 | one         \n",
      "   8 | kid         \t  43 | child         22 | youngest    *  8 | kid            4 | baby           3 | oldest      \n",
      "  63 | ,           \t* 63 | ,              5 | i              2 | .              1 | myself         1 | and         \n",
      "  99 | i           \t* 99 | i              0 | we             0 | he             0 | she            0 | me          \n",
      "  15 | loved       \t  36 | wanted        21 | used        * 15 | loved         11 | liked          4 | tried       \n",
      " 100 | to          \t*100 | to             0 | and            0 | playing        0 | watching       0 | going       \n",
      "  99 | watch       \t* 99 | watch          1 | see            0 | play           0 | watching       0 | watched     \n",
      "   0 | cartoons    \t  52 | them          41 | games          1 | movies         1 | cards          1 | it          \n",
      "   0 | ，           \t  81 | ,             19 | .              0 | ;              0 | -              0 | ...         \n",
      "  44 | but         \t  47 | and         * 44 | but            6 | so             1 | yet            1 | because     \n",
      " 100 | no          \t*100 | no             0 | little         0 | the            0 | zero           0 | not         \n",
      " 100 | matter      \t*100 | matter         0 | to             0 | telling        0 | idea           0 | ,           \n",
      " 100 | how         \t*100 | how            0 | what           0 | however        0 | the            0 | where       \n",
      " 100 | many        \t*100 | many           0 | often          0 | few            0 | several        0 | numerous    \n",
      "  85 | times       \t* 85 | times          3 | questions      1 | minutes        1 | hours          1 | people      \n",
      "  82 | i           \t* 82 | i             10 | we             1 | was            1 | being          1 | he          \n",
      "   0 | asked       \t  37 | took          19 | went          13 | admitted       6 | got            4 | confessed   \n",
      "   5 | to          \t  23 | for           13 | about       *  5 | to             4 | myself         3 | me          \n",
      "   0 | watching    \t  64 | play          30 | watch          3 | see            1 | join           0 | read        \n",
      "  57 | them        \t* 57 | them          23 | cartoons       5 | it             2 | movies         2 | games       \n",
      "   0 | ，           \t 100 | ,              0 | .              0 | ...            0 | again          0 | even        \n",
      "  99 | my          \t* 99 | my             0 | the            0 | her            0 | his            0 | our         \n",
      "  98 | parents     \t* 98 | parents        0 | family         0 | father         0 | mother         0 | grandparents\n",
      "   0 | would       \t  47 | decided       18 | chose          8 | tried          4 | seemed         4 | knew        \n",
      "   0 | not         \t  70 | refuse        10 | have           5 | agree          2 | want           1 | promise     \n",
      "   0 | to          \t  45 | always        17 | have          11 | really         9 | even           3 | ever        \n",
      "  28 | let         \t* 28 | let           17 | believe       11 | tell          11 | bother         5 | stop        \n",
      "  91 | me          \t* 91 | me             6 | go             1 | up             1 | on             0 | it          \n",
      "  97 | .           \t* 97 | .              1 | and            1 | ;              0 | because        0 | ,           \n",
      "  94 | they        \t* 94 | they           1 | he             1 | she            1 | dad            1 | i           \n",
      "  97 | would       \t* 97 | would          1 | did            1 | always         1 | could          0 | might       \n",
      "  27 | say         \t* 27 | say           21 | prove         16 | explain        4 | swear          3 | lie         \n",
      "  65 | to          \t* 65 | to             6 | about          2 | for            2 | that           2 | in          \n",
      "   0 | us          \t  99 | me             0 | themselves     0 | myself      *  0 | us             0 | him         \n",
      "  94 | that        \t* 94 | that           3 | how            1 | if             1 | ,              0 | maybe       \n",
      "  89 | playing     \t* 89 | playing        4 | the            4 | watching       0 | doing          0 | their       \n",
      "  46 | card        \t* 46 | card          40 | the            4 | these          3 | those          1 | cards       \n",
      "  99 | games       \t* 99 | games          0 | game           0 | together       0 | tricks         0 | again       \n",
      "  68 | would       \t* 68 | would         14 | could          4 | might          4 | will           4 | did         \n",
      "   5 | help        \t  23 | change         6 | use         *  5 | help           3 | drain          3 | control     \n",
      "  61 | my          \t* 61 | my            22 | the            9 | our            3 | your           3 | their       \n",
      "   1 | brain       \t  15 | life           5 | family         4 | dad            3 | future         3 | parents     \n",
      "  57 | .           \t* 57 | .             16 | and           14 | ,              9 | but            1 | ;           \n",
      "   0 | still       \t  35 | am            26 | was            8 | but            7 | is             5 | and         \n",
      "   6 | i           \t  60 | ,           *  6 | i              3 | .              3 | too            3 | ...         \n",
      "   0 | unwilling   \t   8 | want           8 | used           8 | have           8 | wanted         7 | had         \n",
      "  48 | to          \t  50 | ##ly        * 48 | to             0 | always         0 | t              0 | ##tly       \n",
      "  28 | play        \t* 28 | play           6 | do             5 | make           1 | stop           1 | keep        \n",
      "   5 | the         \t  82 | card        *  5 | the            1 | these          1 | cards          1 | those       \n",
      "  59 | games       \t* 59 | games         37 | game           1 | cards          0 | piano          0 | kids        \n",
      "   1 | for         \t  92 | with        *  1 | for            1 | in             1 | against        1 | without     \n",
      "  22 | them        \t  51 | myself      * 22 | them           9 | fun            1 | hours          1 | real        \n",
      "   2 | sometimes   \t  16 | anyway        12 | anymore       11 | too           10 | all            4 | though      \n",
      "  96 | .           \t* 96 | .              1 | because        1 | and            1 | ;              0 | ,           \n",
      "  99 | i           \t* 99 | i              0 | we             0 | they           0 | you            0 | people      \n",
      "  99 | didn        \t* 99 | didn           0 | wouldn         0 | don            0 | couldn         0 | did         \n",
      " 100 | '           \t*100 | '              0 | `              0 | \"              0 | ,              0 | ′           \n",
      " 100 | t           \t*100 | t              0 | m              0 | s              0 | d              0 | no          \n",
      "  45 | realize     \t  46 | know        * 45 | realize        3 | understand     3 | realise        2 | see         \n",
      " 100 | how         \t*100 | how            0 | what           0 | the            0 | it             0 | however     \n",
      "   0 | right       \t   6 | strict         5 | powerful       4 | wonderful      4 | smart          4 | helpful     \n",
      "  97 | my          \t* 97 | my             1 | our            0 | the            0 | your           0 | their       \n",
      "  29 | parents     \t* 29 | parents        6 | thoughts       4 | words          2 | people         2 | kids        \n",
      "   1 | are         \t  97 | were        *  1 | are            0 | thought        0 | felt           0 | was         \n",
      "  87 | until       \t* 87 | until          9 | when           2 | before         1 | till           0 | once        \n",
      " 100 | i           \t*100 | i              0 | we             0 | they           0 | he             0 | me          \n",
      "  13 | entered     \t  54 | graduated   * 13 | entered        9 | finished       7 | started        6 | left        \n",
      "  51 | high        \t* 51 | high          20 | elementary    15 | middle         8 | grade          1 | primary     \n",
      " 100 | school      \t*100 | school         0 | schools        0 | society        0 | college        0 | class       \n",
      "  81 | .           \t* 81 | .             14 | and            2 | but            1 | ,              1 | ;           \n",
      "  92 | the         \t* 92 | the            5 | card           1 | those          0 | playing        0 | these       \n",
      "  43 | games       \t* 43 | games         30 | game           6 | lessons        4 | rules          1 | math        \n",
      " 100 | my          \t*100 | my             0 | our            0 | his            0 | me             0 | that        \n",
      "  53 | parents     \t* 53 | parents       15 | father        13 | mother         5 | dad            4 | grandparents\n",
      "  56 | taught      \t* 56 | taught        20 | showed        12 | played         8 | gave           1 | told        \n",
      " 100 | me          \t*100 | me             0 | us             0 | i              0 | him            0 | my          \n",
      "   0 | where       \t  96 | when           2 | since          2 | while          0 | as             0 | until       \n",
      "  99 | i           \t* 99 | i              0 | me             0 | he             0 | she            0 | my          \n",
      "  99 | was         \t* 99 | was            0 | were           0 | as             0 | became         0 | had         \n",
      " 100 | a           \t*100 | a              0 | the            0 | and            0 | one            0 | still       \n",
      "  22 | child       \t  51 | kid         * 22 | child          7 | boy            4 | teenager       4 | freshman    \n",
      "  97 | turned      \t* 97 | turned         1 | turn           1 | came           0 | grew           0 | turning     \n",
      " 100 | out         \t*100 | out            0 | into           0 | on             0 | up             0 | proving     \n",
      " 100 | to          \t*100 | to             0 | into           0 | and            0 | not            0 | would       \n",
      "  94 | be          \t* 94 | be             3 | become         3 | prove          0 | get            0 | seem        \n",
      "  69 | very        \t* 69 | very           6 | extremely      5 | quite          3 | more           3 | really      \n",
      "   7 | useful      \t  19 | important     14 | different      9 | helpful     *  7 | useful         6 | influential \n",
      "   6 | later       \t  46 | things        17 | early       *  6 | later          3 | lessons        3 | times       \n",
      " 100 | in          \t*100 | in             0 | on             0 | during         0 | into           0 | than        \n",
      " 100 | my          \t*100 | my             0 | our            0 | his            0 | their          0 | the         \n",
      "  99 | life        \t* 99 | life           1 | career         0 | childhood      0 | education      0 | lives       \n",
      " 100 | .           \t*100 | .              0 | ;              0 | !              0 | ?              0 | ...         \n",
      "   0 | [SEP]       \t  25 | \"              3 | for            3 | now            3 | and            2 | so          \n",
      "\n",
      "*******************************************************************************************************************\n",
      "1\n",
      "0.0\n",
      "WRB\n",
      "\u001b[38;5;15m\u001b[48;5;0mwhen \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "2\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "3\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "4\n",
      "0.4996413875309904\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mlittle\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "5\n",
      "0.0\n",
      ",\n",
      "\u001b[38;5;15m\u001b[48;5;0m, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "6\n",
      "5.037531860577574\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mfriday\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "7\n",
      "0.0\n",
      "POS\n",
      "\u001b[38;5;15m\u001b[48;5;0m' \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "8\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0ms \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "9\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mnight \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "10\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "11\n",
      "2.9288282257051295\n",
      "PRP$\n",
      "\u001b[38;5;226m\u001b[48;5;0mour\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "12\n",
      "3.944041330267972\n",
      "NN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m\u001b[48;5;0mfamily\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "13\n",
      "1.2859363936756965\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0mgame\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "14\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mnight \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "15\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "16\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mafter \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "17\n",
      "3.864973993379616\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0msupper\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "18\n",
      "0.0\n",
      ",\n",
      "\u001b[38;5;15m\u001b[48;5;0m, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "19\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mwe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "20\n",
      "0.0\n",
      "MD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwould \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "21\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mplay \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "22\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mcard\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "23\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "24\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mof \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "25\n",
      "6.181150402503261\n",
      "DT\n",
      "\u001b[38;5;226m\u001b[48;5;0mall\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "26\n",
      "3.2442493513478983\n",
      "NN\n",
      "\u001b[38;5;214m\u001b[48;5;0msort\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/sorts \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "27\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0min \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "28\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0mthe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "29\n",
      "5.1759204333264215\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0msitting\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "30\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mroom \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "31\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "32\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "33\n",
      "11.548374205660924\n",
      "DT\n",
      "\u001b[38;5;196m\u001b[48;5;0mthe\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/a \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "34\n",
      "1.7249087614151182\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0mkid\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "35\n",
      "0.0\n",
      ",\n",
      "\u001b[38;5;15m\u001b[48;5;0m, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "36\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "37\n",
      "0.9132128054919955\n",
      "VBD\n",
      "\u001b[38;5;226m\u001b[48;5;0mloved\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "38\n",
      "0.0\n",
      "TO\n",
      "\u001b[38;5;15m\u001b[48;5;0mto \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "39\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mwatch \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "40\n",
      "6.495473375871382\n",
      "NNS\n",
      "\u001b[38;5;226m\u001b[48;5;0mcartoons\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "41\n",
      "15.452660592340097\n",
      "VB\n",
      "\u001b[38;5;196m\u001b[48;5;0m，\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "42\n",
      "0.053008093757376584\n",
      "CC\n",
      "\u001b[38;5;226m\u001b[48;5;0mbut\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "43\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0mno \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "44\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mmatter \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "45\n",
      "0.0\n",
      "WRB\n",
      "\u001b[38;5;15m\u001b[48;5;0mhow \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "46\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mmany\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "47\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mtimes \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "48\n",
      "0.0\n",
      "VBP\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "49\n",
      "10.646676048338493\n",
      "VBN\n",
      "\u001b[38;5;196m\u001b[48;5;0masked\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/was used \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "50\n",
      "1.5529499099577042\n",
      "TO\n",
      "\u001b[38;5;226m\u001b[48;5;0mto\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "51\n",
      "8.95932484381705\n",
      "VBG\n",
      "\u001b[38;5;214m\u001b[48;5;0mwatching\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/watch \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "52\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mthem \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "53\n",
      "18.383069999315744\n",
      "VB\n",
      "\u001b[38;5;196m\u001b[48;5;0m，\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "54\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "55\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mparents \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "56\n",
      "5.4762173007041035\n",
      "MD\n",
      "检查点1*****************************************************\n",
      "\u001b[38;5;226m\u001b[48;5;0mwould\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "57\n",
      "5.3354081649787535\n",
      "RB\n",
      "\u001b[38;5;214m\u001b[48;5;0mnot\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/refuse \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "58\n",
      "5.981459151215268\n",
      "TO\n",
      "\u001b[38;5;214m\u001b[48;5;0mto\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/去掉 to \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "59\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mlet \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "60\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mme \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "61\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "62\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mthey \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "63\n",
      "0.0\n",
      "MD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwould \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "64\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0msay \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "65\n",
      "0.0\n",
      "TO\n",
      "\u001b[38;5;15m\u001b[48;5;0mto \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "66\n",
      "6.581833896065917\n",
      "PRP\n",
      "\u001b[38;5;214m\u001b[48;5;0mus\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/me \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "67\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mthat \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "68\n",
      "0.0\n",
      "VBG\n",
      "\u001b[38;5;15m\u001b[48;5;0mplaying \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "69\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mcard \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "70\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "71\n",
      "0.0\n",
      "MD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwould \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "72\n",
      "1.4588194328350998\n",
      "VB\n",
      "\u001b[38;5;226m\u001b[48;5;0mhelp\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "73\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "74\n",
      "3.173226871228209\n",
      "NN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m\u001b[48;5;0mbrain\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "75\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "76\n",
      "5.90809919263306\n",
      "RB\n",
      "\u001b[38;5;214m\u001b[48;5;0mstill\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/still , \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "77\n",
      "2.2313680234481628\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mi\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "78\n",
      "7.241924210620825\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0munwilling\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "79\n",
      "0.033503519227476186\n",
      "TO\n",
      "\u001b[38;5;226m\u001b[48;5;0mto\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "80\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mplay \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "81\n",
      "2.7297515213736863\n",
      "DT\n",
      "\u001b[38;5;226m\u001b[48;5;0mthe\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "82\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "83\n",
      "4.231741889869705\n",
      "IN\n",
      "\u001b[38;5;214m\u001b[48;5;0mfor\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/with \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "84\n",
      "0.8675317652760016\n",
      "PRP\n",
      "\u001b[38;5;226m\u001b[48;5;0mthem\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "85\n",
      "2.1558967133083646\n",
      "RB\n",
      "\u001b[38;5;226m\u001b[48;5;0msometimes\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "86\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "87\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mi\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "88\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mdidn \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "89\n",
      "0.0\n",
      "POS\n",
      "\u001b[38;5;15m\u001b[48;5;0m' \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "90\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mt \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "91\n",
      "0.011093191090367771\n",
      "VB\n",
      "\u001b[38;5;226m\u001b[48;5;0mrealize\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "92\n",
      "0.0\n",
      "WRB\n",
      "\u001b[38;5;15m\u001b[48;5;0mhow \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "93\n",
      "3.6692828920487384\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mright\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "94\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "95\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mparents \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "96\n",
      "4.758635578869137\n",
      "VBP\n",
      "\u001b[38;5;214m\u001b[48;5;0mare\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/were \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "97\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0muntil \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "98\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "99\n",
      "1.4610567542265707\n",
      "VBD\n",
      "\u001b[38;5;226m\u001b[48;5;0mentered\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "100\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mhigh\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "101\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mschool \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "102\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "103\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0mthe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "104\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "105\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "106\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mparents \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "107\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mtaught \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "108\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mme \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "109\n",
      "9.636217093727145\n",
      "WRB\n",
      "\u001b[38;5;214m\u001b[48;5;0mwhere\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/when \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "110\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "111\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "112\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0ma \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "113\n",
      "0.8537064036270944\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0mchild\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "114\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mturned \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "115\n",
      "0.0\n",
      "RP\n",
      "\u001b[38;5;15m\u001b[48;5;0mout \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "116\n",
      "0.0\n",
      "TO\n",
      "\u001b[38;5;15m\u001b[48;5;0mto \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "117\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mbe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "118\n",
      "0.0\n",
      "RB\n",
      "\u001b[38;5;15m\u001b[48;5;0mvery \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "119\n",
      "0.9869002719874604\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0museful\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "120\n",
      "1.9653529850905183\n",
      "RB\n",
      "\u001b[38;5;226m\u001b[48;5;0mlater\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "121\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0min \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "122\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "123\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mlife \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "124\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "平均gap:1.4890399906268712\n",
      "time cost 8.737523794174194 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# text = [\"Who was Jim Henson? Jim Henson _ a puppeteer.\"]\n",
    "# text = [\"Last week I went to the theater. There are many person . Luckily , I had very good seat. The plays was very interesting. However, I didn't enjoy it. A young man and a young woman were sitting behind me. They were talk loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angry. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "\n",
    "#text = [\"Last week I went to the theater. I had very good seat. The plays was very interesting. However, I didn't enjoy it. A young man and a young woman were sitting behind me. They were talk loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angry. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "# text = [\"After the outbreak of the disease, the Ministry of Agriculture and rural areas immediately sent a supervision team to the local. Local Emergency Response Mechanism has been activated in accordance with the requirements, to take blockade, culling, harmless treatment, disinfection and other treatment measures to all disease and culling of pigs for harmless treatment. At the same time, all live pigs and their products are prohibited from transferring out of the blockade area, and live pigs are not allowed to be transported into the blockade area. At present, all the above measures have been implemented.\"]\n",
    "# text = [\"Early critics of Emily Dickinson's poetry mistook for simplemindedness the surface of artlessness that in fact she constructed with such innocence.\"]\n",
    "#text = [\"The journey was long and tired. We left London at five o'clock in the evening and spend eight hours in the train. We had been travelled for 3 hours after someone appeared selling food and drinks. It was darkness all the time we were crossing Wales, but we could see nothing through the windows. When we finally arrived Holyhead nearly , everyone was slept. As soon as the train stopped, everybody come to life, grabbing their suitcases and rushing onto the platform.\"]\n",
    "text = [\"When I was little, Friday's night was our family game night. After supper, we would play card games of all sort in the sitting room. As the kid, I loved to watch cartoons，but no matter how many times I asked to watching them， my parents would not to let me. They would say to us that playing card games would help my brain. Still I unwilling to play the games for them sometimes. I didn't realize how right my parents are until I entered high school. The games my parents taught me where I was a child turned out to be very useful later in my life.\"]\n",
    "#text = [\"Mr. and Mrs.Zhang all work in our school. They live far from the school, and it takes them about a hour and a half to go to work every day. In their spare time, they are interesting in planting vegetables in their garden, that is on the rooftop of their house. They often get up earlier and water the vegetables together. They have also bought in some gardening tools.beside, they often get some useful informations from the internet. When summer came, they will invite their students pick the vegetables！\"]\n",
    "#text = ['The question is more easy than that.']\n",
    "#text = [\"Last week I go to the zoo. I had a very good seat. The play was very interesting.\"]\n",
    "#text =[\"Last week I went to the theater. I had very good seat. The play was very interesting.But I didn't enjoy it. A young man and a young woman were sitting behind me.They were talking loudly. I got very angry.\"]#因为外面有中括号，所以是二维的\n",
    "time_start=time.time()\n",
    "analyze_text(text, show_firstk_probs=200)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************display_suggestions********************************************************\n",
      "| suggestion                                         : position in text\n",
      "---------------------------------------------------------------------------------------\n",
      "| 去掉前面 more 原位置改成 easier                             : 5\n",
      "*************************************************************************************************************\n",
      "['去掉前面', 'more', '原位置改成', 'easier']\n",
      " the question is easier than that .\n"
     ]
    }
   ],
   "source": [
    "#print(suggestions)\n",
    "def display_suggestion():\n",
    "    print(\"**********************************display_suggestions********************************************************\")\n",
    "    print(\"| {:50} : {}\".format(\"suggestion\",\"position in text\"))\n",
    "    print(\"---------------------------------------------------------------------------------------\")\n",
    "    for key in suggestions:\n",
    "        print(\"| {:<50} : {}\".format(suggestions[key] ,key))\n",
    "    print(\"*************************************************************************************************************\")\n",
    "display_suggestion()\n",
    "\n",
    "def modify_text(index):\n",
    "    #entire_ids,entire_type_ids\n",
    "    entire_ids_copy = copy.deepcopy(entire_ids)\n",
    "    new_text = \"\"\n",
    "    suggestion = suggestions[index]\n",
    "    if suggestion[0:2] == '##':\n",
    "        suggestion = tokenizer.ids_to_tokens[entire_ids_copy[index - 1]] + suggestion[2:]\n",
    "        del entire_ids_copy[index]\n",
    "        index = index - 1\n",
    "    #print(suggestion)\n",
    "    suggestion_tokens = suggestion.split(\" \")\n",
    "    print(suggestion_tokens)\n",
    "    if '去掉前面' == suggestion_tokens[0]:\n",
    "        del entire_ids_copy[index - 1]\n",
    "        del suggestion_tokens[0]\n",
    "        del suggestion_tokens[0]\n",
    "        index = index - 1\n",
    "    elif '去掉后面' == suggestion_tokens[0]:\n",
    "        del entire_ids_copy[index + 1]\n",
    "        del suggestion_tokens[0]\n",
    "        del suggestion_tokens[0]\n",
    "    elif '去掉' == suggestion_tokens[0]:\n",
    "        del entire_ids_copy[index]\n",
    "        del suggestion_tokens[0]\n",
    "        del suggestion_tokens[0]\n",
    "    if '原位置改成' in suggestion_tokens:\n",
    "        del suggestion_tokens[0]\n",
    "        \n",
    "    len_suggest = len(suggestion_tokens)\n",
    "    if len_suggest == 1:\n",
    "        entire_ids_copy[index] = tokenizer.vocab[suggestion_tokens[0]]\n",
    "    elif len_suggest == 2:\n",
    "        entire_ids_copy.insert(index,tokenizer.vocab[suggestion_tokens[0]])\n",
    "        entire_ids_copy[index + 1] = tokenizer.vocab[suggestion_tokens[1]]\n",
    "        \n",
    "    for i in range(1,len(entire_ids_copy)-1):\n",
    "        word = tokenizer.ids_to_tokens[entire_ids_copy[i]]\n",
    "        if word[0:2] == \"##\":\n",
    "            new_text = new_text + word[2:]\n",
    "        else:\n",
    "            new_text = new_text + ' ' + tokenizer.ids_to_tokens[entire_ids_copy[i]]\n",
    "    return new_text\n",
    "\n",
    "print(modify_text(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/20/2019 15:48:16 - INFO - examples.extract_features -   tokens: [CLS] when i was little , friday ' s night was our family game night . after supper , we would play card games of all sort in the sitting room . as the kid , i loved to watch cartoons ， but no matter how many times i asked to watching them ， my parents would not to let me . they would say to us that playing card games would help my brain . still i unwilling to play the games for them sometimes . i didn ' t realize how right my parents are until i entered high school . the games my parents taught me where i was a child turned out to be very useful later in my life . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'when', 'i', 'was', 'little', ',', 'friday', \"'\", 's', 'night', 'was', 'our', 'family', 'game', 'night', '.', 'after', 'supper', ',', 'we', 'would', 'play', 'card', 'games', 'of', 'all', 'sort', 'in', 'the', 'sitting', 'room', '.', 'as', 'the', 'kid', ',', 'i', 'loved', 'to', 'watch', 'cartoons', '，', 'but', 'no', 'matter', 'how', 'many', 'times', 'i', 'asked', 'to', 'watching', 'them', '，', 'my', 'parents', 'would', 'not', 'to', 'let', 'me', '.', 'they', 'would', 'say', 'to', 'us', 'that', 'playing', 'card', 'games', 'would', 'help', 'my', 'brain', '.', 'still', 'i', 'unwilling', 'to', 'play', 'the', 'games', 'for', 'them', 'sometimes', '.', 'i', 'didn', \"'\", 't', 'realize', 'how', 'right', 'my', 'parents', 'are', 'until', 'i', 'entered', 'high', 'school', '.', 'the', 'games', 'my', 'parents', 'taught', 'me', 'where', 'i', 'was', 'a', 'child', 'turned', 'out', 'to', 'be', 'very', 'useful', 'later', 'in', 'my', 'life', '.', '[SEP]']\n",
      "********************************************************************\n",
      "   0 | [CLS]       \t   2 | .              1 | the            1 | )              1 | \"              1 | ,           \n",
      "  97 | when        \t* 97 | when           2 | since          1 | until          0 | while          0 | before      \n",
      "  99 | i           \t* 99 | i              0 | she            0 | he             0 | we             0 | me          \n",
      " 100 | was         \t*100 | was            0 | were           0 | got            0 | turned         0 | is          \n",
      "  11 | little      \t  19 | younger     * 11 | little         8 | eight          7 | young          7 | twelve      \n",
      "  51 | ,           \t* 51 | ,             24 | .              4 | and            1 | of             1 | the         \n",
      "   0 | friday      \t  33 | valentine     16 | mother         7 | children       7 | father         5 | grandma     \n",
      " 100 | '           \t*100 | '              0 | `              0 | ′              0 | \"              0 | *           \n",
      " 100 | s           \t*100 | s              0 | til            0 | n              0 | d              0 | round       \n",
      "  39 | night       \t* 39 | night         16 | dinner         6 | eve            5 | day            5 | supper      \n",
      "  90 | was         \t* 90 | was            8 | became         1 | is             0 | were           0 | ,           \n",
      "   4 | our         \t  79 | a             13 | the         *  4 | our            1 | my             0 | their       \n",
      "   1 | family      \t  59 | favorite      18 | first          3 | favourite      2 | only           1 | last        \n",
      "   3 | game        \t  12 | dinner         7 | christmas      6 | fun            5 | day         *  3 | game        \n",
      "  81 | night       \t* 81 | night         13 | day            1 | dinner         1 | date           0 | nights      \n",
      "  97 | .           \t* 97 | .              2 | and            0 | ;              0 | ,              0 | !           \n",
      "  80 | after       \t* 80 | after          6 | during         4 | at             4 | over           3 | before      \n",
      "   1 | supper      \t  68 | school        10 | dinner         9 | that           2 | midnight       1 | breakfast   \n",
      " 100 | ,           \t*100 | ,              0 | ##time         0 | together       0 | time           0 | dinner      \n",
      "  98 | we          \t* 98 | we             1 | i              0 | they           0 | everyone       0 | people      \n",
      "  64 | would       \t* 64 | would         21 | could          2 | will           2 | can            1 | did         \n",
      "  96 | play        \t* 96 | play           2 | have           1 | watch          0 | enjoy          0 | played      \n",
      "  97 | card        \t* 97 | card           1 | board          1 | cards          0 | video          0 | computer    \n",
      " 100 | games       \t*100 | games          0 | game           0 | ##games        0 | matches        0 | sports      \n",
      "  99 | of          \t* 99 | of             0 | in             0 | with           0 | and            0 | ,           \n",
      "   0 | all         \t  85 | some           6 | any            3 | a              3 | every          1 | this        \n",
      "   2 | sort        \t  41 | kinds         34 | types          5 | sorts          4 | sizes          3 | kind        \n",
      "  98 | in          \t* 98 | in             0 | around         0 | inside         0 | at             0 | from        \n",
      "  73 | the         \t* 73 | the           23 | our            1 | my             1 | a              1 | their       \n",
      "   0 | sitting     \t  56 | family        17 | dining        14 | living         1 | back           1 | same        \n",
      "  99 | room        \t* 99 | room           0 | area           0 | rooms          0 | hall           0 | areas       \n",
      "  99 | .           \t* 99 | .              1 | and            0 | ;              0 | ,              0 | ...         \n",
      "  48 | as          \t* 48 | as            29 | like           8 | being          3 | unlike         3 | for         \n",
      "   0 | the         \t 100 | a              0 | another        0 | an          *  0 | the            0 | one         \n",
      "   8 | kid         \t  43 | child         22 | youngest    *  8 | kid            4 | baby           3 | oldest      \n",
      "  63 | ,           \t* 63 | ,              5 | i              2 | .              1 | myself         1 | and         \n",
      "  99 | i           \t* 99 | i              0 | we             0 | he             0 | she            0 | me          \n",
      "  15 | loved       \t  36 | wanted        21 | used        * 15 | loved         11 | liked          4 | tried       \n",
      " 100 | to          \t*100 | to             0 | and            0 | playing        0 | watching       0 | going       \n",
      "  99 | watch       \t* 99 | watch          1 | see            0 | play           0 | watching       0 | watched     \n",
      "   0 | cartoons    \t  52 | them          41 | games          1 | movies         1 | cards          1 | it          \n",
      "   0 | ，           \t  81 | ,             19 | .              0 | ;              0 | -              0 | ...         \n",
      "  44 | but         \t  47 | and         * 44 | but            6 | so             1 | yet            1 | because     \n",
      " 100 | no          \t*100 | no             0 | little         0 | the            0 | zero           0 | not         \n",
      " 100 | matter      \t*100 | matter         0 | to             0 | telling        0 | idea           0 | ,           \n",
      " 100 | how         \t*100 | how            0 | what           0 | however        0 | the            0 | where       \n",
      " 100 | many        \t*100 | many           0 | often          0 | few            0 | several        0 | numerous    \n",
      "  85 | times       \t* 85 | times          3 | questions      1 | minutes        1 | hours          1 | people      \n",
      "  82 | i           \t* 82 | i             10 | we             1 | was            1 | being          1 | he          \n",
      "   0 | asked       \t  37 | took          19 | went          13 | admitted       6 | got            4 | confessed   \n",
      "   5 | to          \t  23 | for           13 | about       *  5 | to             4 | myself         3 | me          \n",
      "   0 | watching    \t  64 | play          30 | watch          3 | see            1 | join           0 | read        \n",
      "  57 | them        \t* 57 | them          23 | cartoons       5 | it             2 | movies         2 | games       \n",
      "   0 | ，           \t 100 | ,              0 | .              0 | ...            0 | again          0 | even        \n",
      "  99 | my          \t* 99 | my             0 | the            0 | her            0 | his            0 | our         \n",
      "  98 | parents     \t* 98 | parents        0 | family         0 | father         0 | mother         0 | grandparents\n",
      "   0 | would       \t  47 | decided       18 | chose          8 | tried          4 | seemed         4 | knew        \n",
      "   0 | not         \t  70 | refuse        10 | have           5 | agree          2 | want           1 | promise     \n",
      "   0 | to          \t  45 | always        17 | have          11 | really         9 | even           3 | ever        \n",
      "  28 | let         \t* 28 | let           17 | believe       11 | tell          11 | bother         5 | stop        \n",
      "  91 | me          \t* 91 | me             6 | go             1 | up             1 | on             0 | it          \n",
      "  97 | .           \t* 97 | .              1 | and            1 | ;              0 | because        0 | ,           \n",
      "  94 | they        \t* 94 | they           1 | he             1 | she            1 | dad            1 | i           \n",
      "  97 | would       \t* 97 | would          1 | did            1 | always         1 | could          0 | might       \n",
      "  27 | say         \t* 27 | say           21 | prove         16 | explain        4 | swear          3 | lie         \n",
      "  65 | to          \t* 65 | to             6 | about          2 | for            2 | that           2 | in          \n",
      "   0 | us          \t  99 | me             0 | themselves     0 | myself      *  0 | us             0 | him         \n",
      "  94 | that        \t* 94 | that           3 | how            1 | if             1 | ,              0 | maybe       \n",
      "  89 | playing     \t* 89 | playing        4 | the            4 | watching       0 | doing          0 | their       \n",
      "  46 | card        \t* 46 | card          40 | the            4 | these          3 | those          1 | cards       \n",
      "  99 | games       \t* 99 | games          0 | game           0 | together       0 | tricks         0 | again       \n",
      "  68 | would       \t* 68 | would         14 | could          4 | might          4 | will           4 | did         \n",
      "   5 | help        \t  23 | change         6 | use         *  5 | help           3 | drain          3 | control     \n",
      "  61 | my          \t* 61 | my            22 | the            9 | our            3 | your           3 | their       \n",
      "   1 | brain       \t  15 | life           5 | family         4 | dad            3 | future         3 | parents     \n",
      "  57 | .           \t* 57 | .             16 | and           14 | ,              9 | but            1 | ;           \n",
      "   0 | still       \t  35 | am            26 | was            8 | but            7 | is             5 | and         \n",
      "   6 | i           \t  60 | ,           *  6 | i              3 | .              3 | too            3 | ...         \n",
      "   0 | unwilling   \t   8 | want           8 | used           8 | have           8 | wanted         7 | had         \n",
      "  48 | to          \t  50 | ##ly        * 48 | to             0 | always         0 | t              0 | ##tly       \n",
      "  28 | play        \t* 28 | play           6 | do             5 | make           1 | stop           1 | keep        \n",
      "   5 | the         \t  82 | card        *  5 | the            1 | these          1 | cards          1 | those       \n",
      "  59 | games       \t* 59 | games         37 | game           1 | cards          0 | piano          0 | kids        \n",
      "   1 | for         \t  92 | with        *  1 | for            1 | in             1 | against        1 | without     \n",
      "  22 | them        \t  51 | myself      * 22 | them           9 | fun            1 | hours          1 | real        \n",
      "   2 | sometimes   \t  16 | anyway        12 | anymore       11 | too           10 | all            4 | though      \n",
      "  96 | .           \t* 96 | .              1 | because        1 | and            1 | ;              0 | ,           \n",
      "  99 | i           \t* 99 | i              0 | we             0 | they           0 | you            0 | people      \n",
      "  99 | didn        \t* 99 | didn           0 | wouldn         0 | don            0 | couldn         0 | did         \n",
      " 100 | '           \t*100 | '              0 | `              0 | \"              0 | ,              0 | ′           \n",
      " 100 | t           \t*100 | t              0 | m              0 | s              0 | d              0 | no          \n",
      "  45 | realize     \t  46 | know        * 45 | realize        3 | understand     3 | realise        2 | see         \n",
      " 100 | how         \t*100 | how            0 | what           0 | the            0 | it             0 | however     \n",
      "   0 | right       \t   6 | strict         5 | powerful       4 | wonderful      4 | smart          4 | helpful     \n",
      "  97 | my          \t* 97 | my             1 | our            0 | the            0 | your           0 | their       \n",
      "  29 | parents     \t* 29 | parents        6 | thoughts       4 | words          2 | people         2 | kids        \n",
      "   1 | are         \t  97 | were        *  1 | are            0 | thought        0 | felt           0 | was         \n",
      "  87 | until       \t* 87 | until          9 | when           2 | before         1 | till           0 | once        \n",
      " 100 | i           \t*100 | i              0 | we             0 | they           0 | he             0 | me          \n",
      "  13 | entered     \t  54 | graduated   * 13 | entered        9 | finished       7 | started        6 | left        \n",
      "  51 | high        \t* 51 | high          20 | elementary    15 | middle         8 | grade          1 | primary     \n",
      " 100 | school      \t*100 | school         0 | schools        0 | society        0 | college        0 | class       \n",
      "  81 | .           \t* 81 | .             14 | and            2 | but            1 | ,              1 | ;           \n",
      "  92 | the         \t* 92 | the            5 | card           1 | those          0 | playing        0 | these       \n",
      "  43 | games       \t* 43 | games         30 | game           6 | lessons        4 | rules          1 | math        \n",
      " 100 | my          \t*100 | my             0 | our            0 | his            0 | me             0 | that        \n",
      "  53 | parents     \t* 53 | parents       15 | father        13 | mother         5 | dad            4 | grandparents\n",
      "  56 | taught      \t* 56 | taught        20 | showed        12 | played         8 | gave           1 | told        \n",
      " 100 | me          \t*100 | me             0 | us             0 | i              0 | him            0 | my          \n",
      "   0 | where       \t  96 | when           2 | since          2 | while          0 | as             0 | until       \n",
      "  99 | i           \t* 99 | i              0 | me             0 | he             0 | she            0 | my          \n",
      "  99 | was         \t* 99 | was            0 | were           0 | as             0 | became         0 | had         \n",
      " 100 | a           \t*100 | a              0 | the            0 | and            0 | one            0 | still       \n",
      "  22 | child       \t  51 | kid         * 22 | child          7 | boy            4 | teenager       4 | freshman    \n",
      "  97 | turned      \t* 97 | turned         1 | turn           1 | came           0 | grew           0 | turning     \n",
      " 100 | out         \t*100 | out            0 | into           0 | on             0 | up             0 | proving     \n",
      " 100 | to          \t*100 | to             0 | into           0 | and            0 | not            0 | would       \n",
      "  94 | be          \t* 94 | be             3 | become         3 | prove          0 | get            0 | seem        \n",
      "  69 | very        \t* 69 | very           6 | extremely      5 | quite          3 | more           3 | really      \n",
      "   7 | useful      \t  19 | important     14 | different      9 | helpful     *  7 | useful         6 | influential \n",
      "   6 | later       \t  46 | things        17 | early       *  6 | later          3 | lessons        3 | times       \n",
      " 100 | in          \t*100 | in             0 | on             0 | during         0 | into           0 | than        \n",
      " 100 | my          \t*100 | my             0 | our            0 | his            0 | their          0 | the         \n",
      "  99 | life        \t* 99 | life           1 | career         0 | childhood      0 | education      0 | lives       \n",
      " 100 | .           \t*100 | .              0 | ;              0 | !              0 | ?              0 | ...         \n",
      "   0 | [SEP]       \t  25 | \"              3 | for            3 | now            3 | and            2 | so          \n",
      "\n",
      "*******************************************************************************************************************\n",
      "1\n",
      "0.0\n",
      "WRB\n",
      "\u001b[38;5;15m\u001b[48;5;0mwhen \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "2\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "3\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "4\n",
      "0.4996413875309904\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mlittle\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "5\n",
      "0.0\n",
      ",\n",
      "\u001b[38;5;15m\u001b[48;5;0m, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "6\n",
      "5.037531860577574\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mfriday\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "7\n",
      "0.0\n",
      "POS\n",
      "\u001b[38;5;15m\u001b[48;5;0m' \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "8\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0ms \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "9\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mnight \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "10\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "11\n",
      "2.9288282257051295\n",
      "PRP$\n",
      "\u001b[38;5;226m\u001b[48;5;0mour\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "12\n",
      "3.944041330267972\n",
      "NN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m\u001b[48;5;0mfamily\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "13\n",
      "1.2859363936756965\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0mgame\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "14\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mnight \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "15\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "16\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mafter \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "17\n",
      "3.864973993379616\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0msupper\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "18\n",
      "0.0\n",
      ",\n",
      "\u001b[38;5;15m\u001b[48;5;0m, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "19\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mwe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "20\n",
      "0.0\n",
      "MD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwould \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "21\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mplay \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "22\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mcard\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "23\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "24\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mof \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "25\n",
      "6.181150402503261\n",
      "DT\n",
      "\u001b[38;5;226m\u001b[48;5;0mall\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "26\n",
      "3.2442493513478983\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0msort\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "27\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0min \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "28\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0mthe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "29\n",
      "5.1759204333264215\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0msitting\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "30\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mroom \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "31\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "32\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "33\n",
      "11.548374205660924\n",
      "DT\n",
      "\u001b[38;5;196m\u001b[48;5;0mthe\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/a \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "34\n",
      "1.7249087614151182\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0mkid\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "35\n",
      "0.0\n",
      ",\n",
      "\u001b[38;5;15m\u001b[48;5;0m, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "36\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "37\n",
      "0.9132128054919955\n",
      "VBD\n",
      "\u001b[38;5;226m\u001b[48;5;0mloved\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "38\n",
      "0.0\n",
      "TO\n",
      "\u001b[38;5;15m\u001b[48;5;0mto \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "39\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mwatch \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "40\n",
      "6.495473375871382\n",
      "NNS\n",
      "\u001b[38;5;226m\u001b[48;5;0mcartoons\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "41\n",
      "15.452660592340097\n",
      "VB\n",
      "\u001b[38;5;196m\u001b[48;5;0m，\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "42\n",
      "0.053008093757376584\n",
      "CC\n",
      "\u001b[38;5;226m\u001b[48;5;0mbut\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "43\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0mno \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "44\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mmatter \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "45\n",
      "0.0\n",
      "WRB\n",
      "\u001b[38;5;15m\u001b[48;5;0mhow \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "46\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mmany\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "47\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mtimes \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "48\n",
      "0.0\n",
      "VBP\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "49\n",
      "10.646676048338493\n",
      "VBN\n",
      "\u001b[38;5;196m\u001b[48;5;0masked\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/was used \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "50\n",
      "1.5529499099577042\n",
      "TO\n",
      "\u001b[38;5;226m\u001b[48;5;0mto\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "51\n",
      "8.95932484381705\n",
      "VBG\n",
      "\u001b[38;5;214m\u001b[48;5;0mwatching\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/watch \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "52\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mthem \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "53\n",
      "18.383069999315744\n",
      "VB\n",
      "\u001b[38;5;196m\u001b[48;5;0m，\u001b[0m\u001b[38;5;2m\u001b[48;5;0m/, \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "54\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "55\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mparents \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "56\n",
      "5.4762173007041035\n",
      "MD\n",
      "检查点1*****************************************************\n",
      "\u001b[38;5;226m\u001b[48;5;0mwould\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "57\n",
      "5.3354081649787535\n",
      "RB\n",
      "\u001b[38;5;214m\u001b[48;5;0mnot\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/refuse \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "58\n",
      "5.981459151215268\n",
      "TO\n",
      "\u001b[38;5;214m\u001b[48;5;0mto\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/去掉 to \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "59\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mlet \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "60\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mme \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "61\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "62\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mthey \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "63\n",
      "0.0\n",
      "MD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwould \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "64\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0msay \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "65\n",
      "0.0\n",
      "TO\n",
      "\u001b[38;5;15m\u001b[48;5;0mto \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "66\n",
      "6.581833896065917\n",
      "PRP\n",
      "\u001b[38;5;214m\u001b[48;5;0mus\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/me \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "67\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mthat \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "68\n",
      "0.0\n",
      "VBG\n",
      "\u001b[38;5;15m\u001b[48;5;0mplaying \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "69\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mcard \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "70\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "71\n",
      "0.0\n",
      "MD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwould \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "72\n",
      "1.4588194328350998\n",
      "VB\n",
      "\u001b[38;5;226m\u001b[48;5;0mhelp\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "73\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "74\n",
      "3.173226871228209\n",
      "NN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m\u001b[48;5;0mbrain\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "75\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "76\n",
      "5.90809919263306\n",
      "RB\n",
      "\u001b[38;5;214m\u001b[48;5;0mstill\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/still , \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "77\n",
      "2.2313680234481628\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mi\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "78\n",
      "7.241924210620825\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0munwilling\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "79\n",
      "0.033503519227476186\n",
      "TO\n",
      "\u001b[38;5;226m\u001b[48;5;0mto\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "80\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mplay \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "81\n",
      "2.7297515213736863\n",
      "DT\n",
      "\u001b[38;5;226m\u001b[48;5;0mthe\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "82\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "83\n",
      "4.231741889869705\n",
      "IN\n",
      "\u001b[38;5;214m\u001b[48;5;0mfor\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/with \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "84\n",
      "0.8675317652760016\n",
      "PRP\n",
      "\u001b[38;5;226m\u001b[48;5;0mthem\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "85\n",
      "2.1558967133083646\n",
      "RB\n",
      "\u001b[38;5;226m\u001b[48;5;0msometimes\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "86\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "87\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mi\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "88\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mdidn \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "89\n",
      "0.0\n",
      "POS\n",
      "\u001b[38;5;15m\u001b[48;5;0m' \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "90\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mt \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "91\n",
      "0.011093191090367771\n",
      "VB\n",
      "\u001b[38;5;226m\u001b[48;5;0mrealize\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "92\n",
      "0.0\n",
      "WRB\n",
      "\u001b[38;5;15m\u001b[48;5;0mhow \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "93\n",
      "3.6692828920487384\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mright\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "94\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "95\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mparents \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "96\n",
      "4.758635578869137\n",
      "VBP\n",
      "\u001b[38;5;214m\u001b[48;5;0mare\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/were \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "97\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0muntil \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "98\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "99\n",
      "1.4610567542265707\n",
      "VBD\n",
      "\u001b[38;5;226m\u001b[48;5;0mentered\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "100\n",
      "0.0\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mhigh\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "101\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mschool \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "102\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "103\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0mthe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "104\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mgames \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "105\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "106\n",
      "0.0\n",
      "NNS\n",
      "\u001b[38;5;15m\u001b[48;5;0mparents \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "107\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0mtaught \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "108\n",
      "0.0\n",
      "PRP\n",
      "\u001b[38;5;15m\u001b[48;5;0mme \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "109\n",
      "9.636217093727145\n",
      "WRB\n",
      "\u001b[38;5;214m\u001b[48;5;0mwhere\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/when \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "110\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "111\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwas \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "112\n",
      "0.0\n",
      "DT\n",
      "\u001b[38;5;15m\u001b[48;5;0ma \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "113\n",
      "0.8537064036270944\n",
      "NN\n",
      "\u001b[38;5;226m\u001b[48;5;0mchild\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "114\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mturned \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "115\n",
      "0.0\n",
      "RP\n",
      "\u001b[38;5;15m\u001b[48;5;0mout \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "116\n",
      "0.0\n",
      "TO\n",
      "\u001b[38;5;15m\u001b[48;5;0mto \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "117\n",
      "0.0\n",
      "VB\n",
      "\u001b[38;5;15m\u001b[48;5;0mbe \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "118\n",
      "0.0\n",
      "RB\n",
      "\u001b[38;5;15m\u001b[48;5;0mvery \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "119\n",
      "0.9869002719874604\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0museful\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "120\n",
      "1.9653529850905183\n",
      "RB\n",
      "\u001b[38;5;226m\u001b[48;5;0mlater\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "121\n",
      "0.0\n",
      "IN\n",
      "\u001b[38;5;15m\u001b[48;5;0min \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "122\n",
      "0.0\n",
      "PRP$\n",
      "\u001b[38;5;15m\u001b[48;5;0mmy \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "123\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mlife \u001b[0m\n",
      "*******************************************************************************************************************\n",
      "124\n",
      "0.0\n",
      ".\n",
      "\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "平均gap:1.4890399906268712\n",
      "**********************************display_suggestions********************************************************\n",
      "| suggestion                                         : position in text\n",
      "---------------------------------------------------------------------------------------\n",
      "| a                                                  : 33\n",
      "| ,                                                  : 41\n",
      "| was used                                           : 49\n",
      "| watch                                              : 51\n",
      "| ,                                                  : 53\n",
      "| refuse                                             : 57\n",
      "| 去掉 to                                              : 58\n",
      "| me                                                 : 66\n",
      "| still ,                                            : 76\n",
      "| with                                               : 83\n",
      "| were                                               : 96\n",
      "| when                                               : 109\n",
      "*************************************************************************************************************\n",
      "建议的数量是 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-fd8043b977d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_firstk_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0manalyse_and_modify_and_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-283-fd8043b977d8>\u001b[0m in \u001b[0;36manalyse_and_modify_and_review\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please input the position you want to modify：\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodify_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "#text = [\"Last week I went to the theater. There are many person . Luckily I had very good seat. The plays was very interesting. However, I didn't enjoy it. A young man and a young woman were sitting behind me. They were talk loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angry. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "#text = [\"After the outbreak of the disease, the Ministry of Agriculture and rural areas immediately sent a supervision team to the local. Local Emergency Response Mechanism has been activated in accordance with the requirements, to take blockade, culling, harmless treatment, disinfection and other treatment measures to all disease and culling of pigs for harmless treatment. At the same time, all live pigs and their products are prohibited from transferring out of the blockade area, and live pigs are not allowed to be transported into the blockade area. At present, all the above measures have been implemented.\"]\n",
    "#text = [\"me love yours.\"]\n",
    "#text = [\"Mr. and Mrs.Zhang all work in our school. They live far from the school, and it takes them about a hour and a half to go to work every day. In their spare time, they are interesting in planting vegetables in their garden, that is on the rooftop of their house. They often get up earlier and water the vegetables together. They have also bought in some gardening tools.beside, they often get some useful informations from the internet. When summer came, they will invite their students pick the vegetables！\"]\n",
    "text = [\"When I was little, Friday's night was our family game night. After supper, we would play card games of all sort in the sitting room. As the kid, I loved to watch cartoons，but no matter how many times I asked to watching them， my parents would not to let me. They would say to us that playing card games would help my brain. Still I unwilling to play the games for them sometimes. I didn't realize how right my parents are until I entered high school. The games my parents taught me where I was a child turned out to be very useful later in my life.\"]\n",
    "def analyse_and_modify_and_review():\n",
    "    global text\n",
    "    analyze_text(text, show_firstk_probs=200)\n",
    "    while len(suggestions)>0:\n",
    "        display_suggestion()\n",
    "        print('建议的数量是',len(suggestions))\n",
    "        if len(suggestions) == 0:\n",
    "            break\n",
    "        else:\n",
    "            index = input(\"Please input the position you want to modify：\")\n",
    "            index = int(index)\n",
    "            text[0] = modify_text(index)\n",
    "            analyze_text(text, show_firstk_probs=200)\n",
    "            \n",
    "analyse_and_modify_and_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The trophy doesn't fit into the brown suitcase because the _ is too large.\"]\n",
    "# text = [\"Mary beat John in the match because _ was very strong.\"]\n",
    "features = convert_examples_to_features(convert_text_to_examples(text), tokenizer, print_info=False)\n",
    "input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long).to(device)\n",
    "input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long).to(device)\n",
    "mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "tokens = features[0].tokens\n",
    "top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    # same / different\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.\",\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.\",\n",
    "    \"Tom has yellow hair. Mary has black hair. John has black hair. Mary and _ have the same hair color.\",\n",
    "    # because / although\n",
    "    \"John is taller/shorter than Mary because/although _ is older/younger.\",\n",
    "    \"The red ball is heavier/lighter than the blue ball because/although the _ ball is bigger/smaller.\",\n",
    "    \"Charles did a lot better/worse than his good friend Nancy on the test because/although _ had/hadn't studied so hard.\",\n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thought that he would arrive earlier than Susan, but/and indeed _ was the first to arrive.\",\n",
    "    # reverse\n",
    "    \"John came then Mary came. They left in reverse order. _ left then _ left.\",\n",
    "    \"John came after Mary. They left in reverse order. _ left after _ .\",\n",
    "    \"John came first, then came Mary. They left in reverse order: _ left first, then left _ .\",\n",
    "    # compare\n",
    "    \"Though John is tall, Tom is taller than John. So John is _ than Tom.\",\n",
    "    \"Tom is taller than John. So _ is shorter than _.\",\n",
    "    # WSC-style: before /after\n",
    "    \"Mary came before/after John. _ was late/early .\",\n",
    "    # yes / no\n",
    "    \"Was Tom taller than Susan? Yes, _ was taller.\",\n",
    "    # right / wrong, epistemic modality\n",
    "    \"John said the rain was about to stop. Mary said the rain would continue. Later the rain stopped. _ was wrong.\",\n",
    "    \n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thanked Mary because  _ had given help to _ . \",\n",
    "    \"John felt vindicated/crushed when his longtime rival Mary revealed that _ was the winner of the competition.\",\n",
    "    \"John couldn't see the stage with Mary in front of him because _ is so short/tall.\",\n",
    "    \"Although they ran at about the same speed, John beat Sally because _ had such a bad start.\",\n",
    "    \"The fish ate the worm. The _ was hungry/tasty.\",\n",
    "    \n",
    "    \"John beat Mary. _ won the game/e winner.\",\n",
    "]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 1345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_switched_label.json') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_child_problem.json') as f:\n",
    "    cexamples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    for s in ce['sentences']:\n",
    "        for a in s['answer0'] + s['answer1']:\n",
    "            a = a.lower()\n",
    "            if a not in tokenizer.vocab:\n",
    "                ce\n",
    "                print(a, 'not in vocab!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    if len(ce['sentences']) > 0:\n",
    "        e = examples[ce['index']]\n",
    "        assert ce['index'] == e['index']\n",
    "        e['score'] = all([s['score'] for s in ce['sentences']])\n",
    "        assert len(set([s['adjacent_ref'] for s in ce['sentences']])) == 1, 'adjcent_refs are different!'\n",
    "        e['adjacent_ref'] = ce['sentences'][0]['adjacent_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for e in examples:\n",
    "    if 'score' in e:\n",
    "        index = e['index']\n",
    "        if index < 252:\n",
    "            if index % 2 == 1:\n",
    "                index -= 1\n",
    "        elif index in [252, 253, 254]:\n",
    "            index = 252\n",
    "        else:\n",
    "            if index % 2 == 0:\n",
    "                index -= 1\n",
    "        groups[index].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'fit into:large/small', False),\n",
       " (4, 'thank:receive/give', False),\n",
       " (6, 'call:successful available', True),\n",
       " (8, 'ask:repeat answer', False),\n",
       " (10, 'zoom by:fast/slow', False),\n",
       " (12, 'vindicated/crushed:be the winner', False),\n",
       " (14, 'lift:weak heavy', False),\n",
       " (16, 'crash through:[hard]/[soft]', False),\n",
       " (18, '[block]:short/tall', False),\n",
       " (20, 'down to:top/bottom', False),\n",
       " (22, 'beat:good/bad', False),\n",
       " (24, 'roll off:anchored level', False),\n",
       " (26, 'above/below', False),\n",
       " (28, 'better/worse:study hard', False),\n",
       " (30, 'after/before:far away', False),\n",
       " (32, 'be upset with:buy from not work/sell not work', True),\n",
       " (34, '?yell at comfort:upset', False),\n",
       " (36, 'above/below:moved first', False),\n",
       " (38, 'although/because', False),\n",
       " (40, 'bully:punish rescue', False),\n",
       " (42, 'pour:empty/full', False),\n",
       " (44, 'know:nosy indiscreet', False),\n",
       " (46, 'explain:convince/understand', True),\n",
       " (48, '?know tell:so/because', True),\n",
       " (50, 'beat:younger/older', False),\n",
       " (56, 'clog:cleaned removed', True),\n",
       " (58, '?immediately follow:short delayed', False),\n",
       " (60, '?between:see see around', True),\n",
       " (64, 'but/and', False),\n",
       " (66, 'clean:put in the trash put in the drawer', False),\n",
       " (68, 'because/but', False),\n",
       " (70, 'out of:handy lighter', False),\n",
       " (72, 'put:tall high', False),\n",
       " (74, 'show:good famous', True),\n",
       " (76, 'pay for:generous grateful', False),\n",
       " (78, 'but', False),\n",
       " (80, 'if', False),\n",
       " (82, 'if', False),\n",
       " (84, 'fool:get/lose', False),\n",
       " (88, 'wait:impatient cautious', False),\n",
       " (90, 'give birth:woman baby', True),\n",
       " (92, '?stop normal/stop abnormal:strange', False),\n",
       " (96, 'eat:hungry tasty', False),\n",
       " (98, 'put ... into filled with ... :get in/get out', False),\n",
       " (100, 'up:at the bottom/at the top', False),\n",
       " (102, 'crash through:removed repaired', False),\n",
       " (104, 'stab:taken to the police station taken to the hospital', False),\n",
       " (106, 'hear ... humming and whistling:annoyed/annoying', True),\n",
       " (108, 'see ... juggling watermelons:impressed/impressive', True),\n",
       " (114, 'tell lies: truthful skeptical', True),\n",
       " (130, 'but:disappointed', True),\n",
       " (132, 'visit:invite come out/invite come in', True),\n",
       " (134, 'take classes from:eager known to speak it fluently', False),\n",
       " (138, 'cover:out gone', True),\n",
       " (144, 'tuck:work sleep', True),\n",
       " (150, 'influence:later/earlier', False),\n",
       " (152, 'can not cut:thick small', False),\n",
       " (154, 'attack:kill guard', False),\n",
       " (156, 'attack:bold nervous', False),\n",
       " (160, 'change:hard:easy', False),\n",
       " (166, 'alive:is/was', False),\n",
       " (168, 'infant:twelve years old twelve months old', False),\n",
       " (170, 'better equipped and large:defeated/victorious', False),\n",
       " (178, 'interview:persistent cooperative', False),\n",
       " (186, 'be full of:minority/majority', False),\n",
       " (188, 'like over:more/fewer', False),\n",
       " (190, 'place on all:not enough/too many', True),\n",
       " (192, 'stick:leave have', True),\n",
       " (196, 'follow:admire/influence', True),\n",
       " (198, 'fit through:wide/narrow', False),\n",
       " (200, 'trade:dowdy/great', False),\n",
       " (202, 'hire/hire oneself to:take care of', True),\n",
       " (204, 'promise/order', False),\n",
       " (208, 'mother:education place', True),\n",
       " (210, 'knock:get an answer/answer', True),\n",
       " (212, 'pay:receive/deliver', False),\n",
       " (218, '?', False),\n",
       " (220, 'say check:move take', False),\n",
       " (222, '?', False),\n",
       " (224, 'give a life:drive alone walk', False),\n",
       " (226, 'pass the plate:full/hungry', False),\n",
       " (228, 'pass:turn over turn next', False),\n",
       " (232, 'stretch pat', True),\n",
       " (234, 'accept share', False),\n",
       " (236, 'speak:break silence break concentration', False),\n",
       " (240, 'carry:leg ache leg dangle', True),\n",
       " (242, 'carry:in arms in bassinet', False),\n",
       " (244, 'hold:against chest against will', True),\n",
       " (250, 'stop', False),\n",
       " (252, 'even though/because/not', False),\n",
       " (255, 'give:not hungry/hungry', False),\n",
       " (259, 'ask for a favor:refuse/be refused`', False),\n",
       " (261, 'cede:less popular/more popular', False),\n",
       " (263, 'not pass although:see open/open', True),\n",
       " (271, 'suspect regret', True)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_dict(d, keys=['index', 'sentence', 'correct_answer', 'relational_word', 'is_associative', 'score']):\n",
    "    return {k: d[k] for k in d if k in keys}\n",
    "\n",
    "# ([[filter_dict(e) for e in eg] for eg in groups.values() if eg[0]['relational_word'] != 'none' and all([e['score'] for e in eg])])# / len([eg for eg in groups.values() if eg[0]['relational_word'] != 'none'])\n",
    "[(index, eg[0]['relational_word'], all([e['score'] for e in eg])) for index, eg in groups.items() if eg[0]['relational_word'] != 'none']\n",
    "# len([filter_dict(e) for e in examples if 'score' in e and not e['score'] and e['adjacent_ref']])\n",
    "# for e in examples:\n",
    "#     if e['index'] % 2 == 0:\n",
    "#         print(e['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['because' in e['sentence'] for e in examples]) + \\\n",
    "sum(['so ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['but ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['though' in e['sentence'] for e in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('WSC_switched_label.json', 'w') as f:\n",
    "#     json.dump(examples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_attn_topk = 3\n",
    "\n",
    "def has_chinese_label(labels):\n",
    "    labels = [label.split('->')[0].strip() for label in labels]\n",
    "    r = sum([len(label) > 1 for label in labels if label not in ['BOS', 'EOS']]) * 1. / (len(labels) - 1)\n",
    "    return 0 < r < 0.5  # r == 0 means empty query labels used in self attention\n",
    "\n",
    "def _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col, color='b'):\n",
    "    assert len(query_labels) == attn.size(0)\n",
    "    assert len(key_labels) == attn.size(1)\n",
    "\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax2 = ax1.twinx()\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    pos = range(nlabels)\n",
    "    \n",
    "    if 'self' in attn_name and col < ncols - 1:\n",
    "        query_labels = ['' for _ in query_labels]\n",
    "\n",
    "    for ax, labels in [(ax1, key_labels), (ax2, query_labels)]:\n",
    "        ax.set_yticks(pos)\n",
    "        if has_chinese_label(labels):\n",
    "            ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "        else:\n",
    "            ax.set_yticklabels(labels)\n",
    "        ax.set_ylim([nlabels - 1, 0])\n",
    "        ax.tick_params(width=0, labelsize='xx-large')\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "#     mask, attn = filter_attn(attn)\n",
    "    for qi in range(attn.size(0)):\n",
    "#         if not mask[qi]:\n",
    "#             continue\n",
    "#         for ki in range(attn.size(1)):\n",
    "        for ki in attn[qi].topk(vis_attn_topk)[1]:\n",
    "            a = attn[qi, ki]\n",
    "            ax1.plot((-1, 1), (ki, qi), color, alpha=a)\n",
    "#     print(attn.mean(dim=0).topk(5)[0])\n",
    "#     ax1.barh(pos, attn.mean(dim=0).data.cpu().numpy())\n",
    "\n",
    "def plot_layer_attn(result_tuple, attn_name='dec_self_attns', layer=0, heads=None):\n",
    "    hypo, nheads, labels_dict = result_tuple\n",
    "    key_labels, query_labels = labels_dict[attn_name]\n",
    "    if heads is None:\n",
    "        heads = range(nheads)\n",
    "    else:\n",
    "        nheads = len(heads)\n",
    "    \n",
    "    stride = 2 if attn_name == 'dec_enc_attns' else 1\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    rcParams['figure.figsize'] = 20, int(round(nlabels * stride * nheads / 8 * 1.0))\n",
    "    \n",
    "    rows = nheads // ncols * stride\n",
    "    fig, axes = plt.subplots(rows, ncols)\n",
    "    \n",
    "    # for head in range(nheads):\n",
    "    for head_i, head in enumerate(heads):\n",
    "        row, col = head_i * stride // ncols, head_i * stride % ncols\n",
    "        ax1 = axes[row, col]\n",
    "        attn = hypo[attn_name][layer][head]\n",
    "        _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col)\n",
    "        if attn_name == 'dec_enc_attns':\n",
    "            col = col + 1\n",
    "            axes[row, col].axis('off')  # next subfig acts as blank place holder\n",
    "    # plt.suptitle('%s with %d heads, Layer %d' % (attn_name, nheads, layer), fontsize=20)\n",
    "    plt.show()  \n",
    "            \n",
    "ncols = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertSelfAttention' object has no attribute 'attention_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-13903ae0e550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enc_self_attns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-13903ae0e550>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enc_self_attns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertSelfAttention' object has no attribute 'attention_probs'"
     ]
    }
   ],
   "source": [
    "attn_name = 'enc_self_attns'\n",
    "hypo = {attn_name: [model.bert.encoder.layer[i].attention.self.attention_probs[0] for i in range(config.num_hidden_layers)]}\n",
    "key_labels = query_labels = tokens\n",
    "labels_dict = {attn_name: (key_labels, query_labels)}\n",
    "result_tuple = (hypo, config.num_attention_heads, labels_dict)\n",
    "plot_layer_attn(result_tuple, attn_name=attn_name, layer=10, heads=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
