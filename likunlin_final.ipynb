{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import tokenization, BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, BertConfig\n",
    "from examples.extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/27/2019 11:51:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased/vocab.txt\n",
      "05/27/2019 11:51:05 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased\n",
      "05/27/2019 11:51:05 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/27/2019 11:51:08 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "args = Args()\n",
    "args.no_cuda = True #不用GPU\n",
    "\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "BERT_DIR = '/nas/pretrain-bert/pretrain-pytorch/bert-base-uncased'\n",
    "config_file = os.path.join(BERT_DIR, CONFIG_NAME)\n",
    "config = BertConfig.from_json_file(config_file)\n",
    "\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(os.path.join(BERT_DIR, 'vocab.txt'))\n",
    "except:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#tokenizer.tokenize = nltk.word_tokenize\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(BERT_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()\n",
    "\n",
    "input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids = [],[],[],[],[],[]\n",
    "suggestions = {} #外部变量，需要传到前端\n",
    "original_tokens = [] #外部变量，需要传到前端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertForPreTraining：\n",
    "Outputs:\n",
    "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
    "            sentence classification loss.\n",
    "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
    "            Outputs a tuple comprising\n",
    "            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n",
    "            - the next sentence classification logits of shape [batch_size, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from_pretrained：\n",
    "Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
    "Download and cache the pre-trained model file if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_text_to_examples(text): \n",
    "    '''功能：\n",
    "            把输入的文本变成一个实例，一个实例中包含text_a,text_b(text_b用于是否为上下句的任务，该任务不使用此功能)\n",
    "       输入：\n",
    "            text：一个列表结构，列表中包含原始文本字符串，由于仅完成mlm任务，所以text列表中仅包含一个字符串，就是待检查的字符串\n",
    "       输出：\n",
    "            example：实例，其中包含：\n",
    "                unique_id：此任务仅用到0\n",
    "                text_a：text列表内的字符串\n",
    "                text_b：此任务下该变量为None\n",
    "    '''\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    if True:\n",
    "        for line in text:\n",
    "            line = line.strip()\n",
    "            text_a = None\n",
    "            text_b = None\n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line) #想要匹配这样的字符串'You are my sunshine. ||| I love you.'\n",
    "            \n",
    "            if m is None:\n",
    "                text_a = line\n",
    "            else:\n",
    "                text_a = m.group(1) #匹配的第一句,比如You are my sunshine,my only sunshine.\n",
    "                text_b = m.group(2) #匹配的第二句，比如I love you.\n",
    "            \n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            unique_id += 1\n",
    "    return examples\n",
    "#print(convert_text_to_examples(['I love you. The cat is so cute.'])[0].text_a)\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, append_special_tokens=True, replace_mask=True, print_info=False):\n",
    "    '''功能：\n",
    "            把实例变成一个特征列表\n",
    "       输入：\n",
    "            examples：实例，convert_text_to_examples()函数的输出\n",
    "            tokenizer：BERT的tokenizer，用于将文本进行各种处理，它可以把一个text转变成tokens，把tokens变成每个token在词典中的编号以及逆运算\n",
    "            append_special_tokens：是否允许在生成的tokens中加入特殊符号，也就是[CLS]、[MASK]和[SEP]，默认为True\n",
    "            replace_mask：不明\n",
    "            print_info：不明\n",
    "       输出：\n",
    "            features：每一个feature包含：\n",
    "                unique_id：编号，目前实现的功能features里面仅有一个feature\n",
    "                tokens=tokens,tokens：是形如['i','love','you','.']的一个列表\n",
    "                input_ids=input_ids：字符串中的每个单词在词典中的index序列\n",
    "                input_mask=input_mask：一堆1\n",
    "                input_type_ids=input_type_ids))：对text_a,text_b的区分，用于上下句任务，对于本任务，该参数为一个列表，其中包含token长度个的0\n",
    "    '''\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a) #tokenize的作用是把\"i love you.\"变成['i','love','you','.']\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        tokens = []\n",
    "        input_type_ids = [] #segment embedding\n",
    "        if append_special_tokens: #输入参数中默认为true\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            if replace_mask and token == '_':  # XD\n",
    "                token = \"[MASK]\"\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                if replace_mask and token == '_':  # XD\n",
    "                    token = \"[MASK]\"\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(1)\n",
    "            if append_special_tokens:\n",
    "                tokens.append(\"[SEP]\")\n",
    "                input_type_ids.append(1)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) #把原来句子中的词语编成在字典中的编号\n",
    "        input_mask = [1] * len(input_ids) \n",
    "        \n",
    "        if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "#             logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\n",
    "#                 \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=example.unique_id,#编号，目前实现的功能features里面仅有一个feature\n",
    "                tokens=tokens,#形如['i','love','you','.']的一个列表\n",
    "                input_ids=input_ids,#字符串中的每个单词在词典中的index序列\n",
    "                input_mask=input_mask, #一堆1\n",
    "                input_type_ids=input_type_ids)) #第0类和第1类，对text_a,text_b的区分，本代码中全都是零\n",
    "    return features            \n",
    "\n",
    "def copy_and_mask_feature(feature, step, masked_tokens=None): \n",
    "    '''\n",
    "        功能：\n",
    "            输入feature生成训练的批次数以及mask好的训练素材\n",
    "        输入：\n",
    "            feature：convert_examples_to_features函数的输出\n",
    "            step：两个[mask]位置的步长\n",
    "            masked_tokens：默认为None，在程序中没有使用\n",
    "    '''\n",
    "    import copy\n",
    "    tokens = feature.tokens\n",
    "    len_token = len(tokens)\n",
    "    if len_token<step:\n",
    "        batches = range(0,len(tokens))\n",
    "    else:\n",
    "        batches = range(0,step)\n",
    "    \n",
    "    assert len_token > 0\n",
    "    masked_feature_copies = []\n",
    "    for i in batches: #用[mask]依次掩盖每一个位置\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        masked_pos = i\n",
    "        while masked_pos < len_token:\n",
    "            feature_copy.input_ids[masked_pos] = tokenizer.vocab[\"[MASK]\"]\n",
    "            masked_pos = masked_pos + step\n",
    "        masked_feature_copies.append(feature_copy)\n",
    "    return masked_feature_copies, batches\n",
    "\n",
    "#masked_feature_copies, batches = copy_and_mask_feature(features[0],3)\n",
    "#print(masked_feature_copies[0].input_ids) #结果[101, 1045, 2293, 103, 102]\n",
    "#print(batches) #结果是一个range(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyzed_cache = {}\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG\n",
    "#print (lemma('gave'))\n",
    "#print (lexeme('production'))\n",
    "#print (conjugate(verb='give',tense=PRESENT,number=SG))\n",
    "def process_text(text): \n",
    "    '''\n",
    "        功能：\n",
    "            处理输入文本，将文本按句子分成若干token，得出原来text中index位置的单词在x句子的y位置，还得出各个句子类别码\n",
    "        输入：\n",
    "            text：文本字符串，注意区别\n",
    "        输出：\n",
    "            input_ids_sen：二维列表，第一维列表的元素是每个句子的input_ids列表\n",
    "            input_type_ids_sen：二维列表，第一维列表的元素是每个句子的input_type_ids列表\n",
    "            in_sentence：通过这个二维数组可以很方便的通过在完整text中的下标找到这个下标所在的句子和在句子中的下标\n",
    "            sentences：字符串列表，列表中每一个元素是一个句子字符串\n",
    "            entire_ids：整个text的input_ids\n",
    "            entire_type_ids：整个text的input_type_ids\n",
    "    '''\n",
    "    token =[]\n",
    "    entire_type_ids = []\n",
    "    token0 = tokenizer.tokenize(text)\n",
    "    token.append('[CLS]')\n",
    "    entire_type_ids.append(0)\n",
    "    for i in token0:\n",
    "        token.append(i)\n",
    "        entire_type_ids.append(0)\n",
    "    token.append('[SEP]')\n",
    "    entire_type_ids.append(0)\n",
    "    \n",
    "    entire_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "    in_sentence = [[0,0]] \n",
    "    sentence_n = 0\n",
    "    index = 1\n",
    "    for i in range(1,len(token)-1):\n",
    "        in_sentence.append([sentence_n,index])  #每个token中的词在所在句中的位置表示出来，以及该位置在哪一句中\n",
    "        index = index + 1                           #比如，位置i这个词在第sentence句的index位置上\n",
    "        if token[i] == '.':\n",
    "            sentence_n = sentence_n + 1\n",
    "            index = 1\n",
    "    sentences = text.split(\".\")\n",
    "    \n",
    "    sen_token = []\n",
    "    input_ids_sen = []\n",
    "    input_type_ids_sen = []\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sentence = sentence + '.'\n",
    "        sentences[i] = sentences[i] + '.'\n",
    "        token = []\n",
    "        input_type_ids = []\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token.append('[CLS]')\n",
    "        input_type_ids.append(0) \n",
    "        for i in tokens:\n",
    "            token.append(i)\n",
    "            input_type_ids.append(0)        \n",
    "        token.append('[SEP]')        \n",
    "        input_type_ids.append(0)\n",
    "        input_ids_sen.append(tokenizer.convert_tokens_to_ids(token))\n",
    "        input_type_ids_sen.append(input_type_ids)\n",
    "    return input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(index):\n",
    "    '''\n",
    "        输入：\n",
    "            index：在完整text中的位置\n",
    "        输出\n",
    "            word:该位置上的单词\n",
    "    '''\n",
    "    word_id = entire_ids[index]\n",
    "    word = tokenizer.ids_to_tokens[word_id]\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE\n",
    "\n",
    "def give_suggestion(input_ids_,input_type_ids_,id_in_sen,alternative_word,threshold):\n",
    "    '''\n",
    "        功能：\n",
    "            给出指定文本指定位置的推荐用词\n",
    "        输入：\n",
    "            input_ids_：要分析的文本的input_ids\n",
    "            input_type_ids_：要分析的文本的的input_type_ids\n",
    "            id_in_sen：要分析的文本中[MASK]的位置下标，也就是需要给出建议用词的位置\n",
    "            alternative_word：推荐的备选词范围\n",
    "            threshold：阈值\n",
    "        输出：\n",
    "            suggestion：推荐\n",
    "            need：推荐的是否是备选词中的词\n",
    "            suggestion_prob：推荐词填在id_in_sen位置的概率\n",
    "            top_of_alternative:备选词中最值得推荐的词\n",
    "    '''\n",
    "    input_ids = copy.deepcopy(input_ids_)\n",
    "    input_type_ids = copy.deepcopy(input_type_ids_)\n",
    "    word0 = input_ids[id_in_sen]\n",
    "    word0 = tokenizer.ids_to_tokens[word0]\n",
    "    list_word_id = []\n",
    "    \n",
    "    input_ids[id_in_sen] = tokenizer.vocab[\"[MASK]\"]\n",
    "    T_input_ids = torch.tensor([input_ids], dtype=torch.long) #把input_ids增加了一个维度\n",
    "    T_input_type_ids = torch.tensor([input_type_ids], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    T_input_ids = T_input_ids.to(device) #拿去GPU\n",
    "    T_input_type_ids = T_input_type_ids.to(device)\n",
    "\n",
    "    mlm_logits, _ = model(T_input_ids, T_input_type_ids)\n",
    "    mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "    reduced_mlm_probs = mlm_probs[0][id_in_sen]\n",
    "\n",
    "    top_ind = reduced_mlm_probs.argmax().item()\n",
    "    top_prob = reduced_mlm_probs.max().item() \n",
    "    \n",
    "    list_word = []\n",
    "    \n",
    "    top_of_alternative = None\n",
    "    if len(alternative_word)>0:\n",
    "        list_word_prob = {}\n",
    "        for word in alternative_word:\n",
    "            try:\n",
    "                list_word_id.append(tokenizer.vocab[word])\n",
    "                list_word.append(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        for word,word_id in zip(list_word,list_word_id):\n",
    "            list_word_prob.update({word:float(reduced_mlm_probs[word_id].data)})\n",
    "        prob_ord = sorted(list_word_prob.items(),key = lambda x:x[1],reverse = True)\n",
    "        \n",
    "        top_prob_word = prob_ord[0][1]\n",
    "        top_of_alternative = prob_ord[0][0]\n",
    "        gap = math.log(top_prob) - math.log(top_prob_word)\n",
    "        \n",
    "        if gap < threshold:\n",
    "            suggestion = prob_ord[0][0]\n",
    "            suggestion_prob = prob_ord[0][1]\n",
    "            need = 1\n",
    "        else:\n",
    "            suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "            suggestion_prob = top_prob\n",
    "            need = 0\n",
    "        #print(\"gap = \" + str(gap))\n",
    "        #print(prob_ord)\n",
    "    else:\n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        suggestion_prob = top_prob\n",
    "        need = 0\n",
    "        \n",
    "    return suggestion,need,suggestion_prob,top_of_alternative \n",
    "\n",
    "#返回变量5\n",
    "#suggestion -> 最值得推荐的词\n",
    "#need -> 是否需要可选词中的一个\n",
    "#suggestion_prob ->最值得推荐的词的概率\n",
    "#top_of_alternative -> 可选词中最值得推荐的\n",
    "#suggestion,need,suggestion_prob,top_of_alternative = give_suggestion(input_ids_,input_type_ids_,id_in_sen,alternative_word,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from pattern.en import comparative, superlative\n",
    "from pattern.en import suggest\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmers=[]\n",
    "stemmers.append(LancasterStemmer()) \n",
    "stemmers.append(SnowballStemmer(\"english\"))\n",
    "stemmers.append(PorterStemmer())\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "def word_convert(word,new_word,Stemmer):\n",
    "    '''\n",
    "        功能：\n",
    "            根据提供的word和可能的变形new_word,得到正确的变形，例如给出basic，basicly得到basically\n",
    "        输入：\n",
    "            word：需要变形的词\n",
    "            new_word:猜想的变形\n",
    "        输出：\n",
    "            suggest_word:推荐的正确变形\n",
    "    '''\n",
    "    suggest_word = None\n",
    "    word_stem = Stemmer().stem(word)\n",
    "    suggest_ = new_word\n",
    "    \n",
    "    suggest_list = suggest(suggest_)\n",
    "\n",
    "    if len(word)<len(new_word):\n",
    "        flag = 0\n",
    "    else:\n",
    "        flag = 1\n",
    "    word_stem = word_stem[:-1]\n",
    "    suggestion_word_stem = Stemmer().stem(suggest_)\n",
    "    \n",
    "    for word_ in suggest_list:\n",
    "        if word == word_[0]:\n",
    "            continue\n",
    "        if (word_[0] == new_word and word_[1] > 0.95):# or word_[1] > 0.95 :\n",
    "            suggest_word = word_[0]\n",
    "            break           \n",
    "        if word_[1] < 0.001:\n",
    "            break\n",
    "        stem_list = []\n",
    "        for stemmer in stemmers:\n",
    "            suggest_stem = stemmer.stem(word_[0])\n",
    "            if flag == 1 and suggest_stem[:-1] in word_stem and word_stem[:3] in suggest_stem[:3]: #一般是去后缀\n",
    "                suggest_word = word_[0]\n",
    "                break\n",
    "            elif flag == 0 and word_stem in suggest_stem and word_[0][-1:] in suggest_[-1:]: #一般是加后缀，后缀一定要一样\n",
    "                suggest_word = word_[0]\n",
    "                break\n",
    "                \n",
    "        if suggest_word != None:\n",
    "            break\n",
    "    return suggest_word \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmers=[]\n",
    "stemmers.append(LancasterStemmer()) \n",
    "stemmers.append(SnowballStemmer(\"english\"))\n",
    "stemmers.append(PorterStemmer())\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "def word_convert(word,new_word,Stemmer):\n",
    "    '''\n",
    "        说明;\n",
    "            与上面的区别是使用的拼写改错算法不同，上面那个平均速度慢，但更符合我的要求，这个平均速度更快\n",
    "        功能：\n",
    "            根据提供的word和可能的变形new_word,得到正确的变形，例如给出basic，basicly得到basically\n",
    "        输入：\n",
    "            word：需要变形的词\n",
    "            new_word:猜想的变形\n",
    "            Stemmer:词根提取器\n",
    "        输出：\n",
    "            suggest_word:推荐的正确变形\n",
    "    '''\n",
    "    if d.check(new_word)==True: #如果发现new_word拼写正确，则直接返回\n",
    "        return new_word\n",
    "    else:\n",
    "        suggest_word = None\n",
    "        word_stem = Stemmer().stem(word)\n",
    "        suggest_ = new_word\n",
    "        suggest_list = d.suggest(suggest_) #可能的正确单词列表\n",
    "\n",
    "        if len(word)<len(new_word): #一般都是加后缀\n",
    "            flag = 0\n",
    "        else: #一般都是去后缀\n",
    "            flag = 1\n",
    "        word_stem = word_stem[:-1] #这样效果更好一点，防止某些去e加后缀或者y变i的变形被忽略\n",
    "        suggestion_word_stem = Stemmer().stem(suggest_)\n",
    "        for word_ in suggest_list:\n",
    "            if word == word_: #如果变形和原型一样，就跳过这个词\n",
    "                continue\n",
    "            if (word_ == new_word): #如果推荐的和new_word一样，直接把该词作为结果\n",
    "                suggest_word = word_\n",
    "                break\n",
    "            if ' ' in word_ or '-' in word_: #enchant.Dict模型特有的问题，一个拼写错误的词可能会给你返回一个带连字符词的或者是两个词\n",
    "                continue\n",
    "            stem_list = []\n",
    "            for stemmer in stemmers:\n",
    "                suggest_stem = stemmer.stem(word_)\n",
    "                if flag == 1 and suggest_stem in word_stem and word_stem[:3] in suggest_stem[:3]: #一般是去后缀\n",
    "                    suggest_word = word_\n",
    "                    break\n",
    "                elif flag == 0 and word_stem in suggest_stem and word_[-1:] in suggest_[-1:]: #一般是加后缀，后缀一定要一样\n",
    "                    suggest_word = word_\n",
    "                    break\n",
    "\n",
    "            if suggest_word != None:\n",
    "                break\n",
    "        return suggest_word \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'下面是词性转换系列函数\\n    功能：\\n        词性转变系列函数\\n    输入：\\n        word：原形词\\n    输出：\\n        suggest_word：推荐的变形\\n        suggest_list：推荐的变形列表\\n    说明：\\n        词性变化的能力有限，对于有些特殊变形，比如die->death，success->succeed无能为力'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''下面是词性转换系列函数\n",
    "    功能：\n",
    "        词性转变系列函数\n",
    "    输入：\n",
    "        word：原形词\n",
    "    输出：\n",
    "        suggest_word：推荐的变形\n",
    "        suggest_list：推荐的变形列表\n",
    "    说明：\n",
    "        词性变化的能力有限，对于有些特殊变形，比如die->death，success->succeed无能为力'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adj_to_adv(word):\n",
    "    suggest_word = None\n",
    "    if(word == \"good\"):\n",
    "        return \"well\"\n",
    "    else:\n",
    "        suggest_ = word + 'ly'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        return suggest_word\n",
    "#如果形容词副词同形，那么他会返回none，但是不影响计算，因为形容词副词同形啊\n",
    "\n",
    "\n",
    "def adv_to_adj(word):\n",
    "    suggest_word = None\n",
    "    if(word == \"well\"):\n",
    "        return \"good\"    \n",
    "    elif word[-2:] == 'ly':\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "    return suggest_word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_anything(word):#形容词变成其他词性\n",
    "    suggest_word = None\n",
    "    suggest_list = []\n",
    "    if word[-1:] == 'y': #举例 healthy->health\n",
    "        suggest_ = word[:-1]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ful':#举例 successful->success\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ive': #举例 active -> act\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'ed': #举例 interested->interest->interesting\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)     \n",
    "        suggest_ = suggest_ + 'ing'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)      \n",
    "            \n",
    "    elif word[-3:] == 'ing':#举例 interesting->interest->interested\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "        suggest_ = suggest_ + 'ed'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)  \n",
    "            \n",
    "    elif word[-4:] == 'less': #举例 careless -> care\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'ly':  #举例： friendly -> friend , lovely -> love\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    " \n",
    "    elif word[-1:] == 't': #举例 different -> different\n",
    "        suggest_ = word[:-1]\n",
    "        suggest_ = suggest_ + 'ce'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ous': #举例 dangerous -> danger\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'al': #举例 original -> origin\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-4:] == 'able':\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'en': #举例 woolen -> wool\n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-2:] == 'ic': \n",
    "        suggest_ = word + 'al'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)  \n",
    "        suggest_ = word[:-2]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)   \n",
    "    elif word[-3:] == 'ish':\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word == None:\n",
    "            suggest_ = word[:-3]\n",
    "            suggest_ = suggest_ + 'and'\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer) \n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ese':\n",
    "        suggest_ = word[:-3]\n",
    "        suggest_ = suggest_ + 'a'\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)  \n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    elif word[-3:] == 'ian':\n",
    "        suggest_ = word[:-1]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word == None:\n",
    "            suggest_ = word[:-3]\n",
    "            suggest_ = suggest_ + 'y'\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    if suggest_word == None:\n",
    "        HouZhui_list = ['ment','ness','tion','ture','sion','ty','y','tive','sive']\n",
    "        for HouZhui in HouZhui_list:\n",
    "            suggest_ = word + HouZhui\n",
    "            new_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if new_word != None:\n",
    "                suggest_word = new_word\n",
    "                suggest_list.append(suggest_word)\n",
    "    suggest_list = list(set(suggest_list))      \n",
    "    return suggest_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_to_anything(word):#名词变成其他词性\n",
    "    suggest_list = []\n",
    "    list_HouZhui = ['y','ful','tive','sive','ed','ing','less','ly','ous','al','able','en','tic','ish','ance','er','or']\n",
    "    list_QianZhui = ['a']\n",
    "    if word[-4:] in ['ment','ness','tion','ture','sion','tive','sive']:\n",
    "        suggest_ = word[:-4]\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    else:\n",
    "        for HouZhui in list_HouZhui:\n",
    "            suggest_ = word + HouZhui\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)\n",
    "        for QianZhui in list_QianZhui:\n",
    "            suggest_ = QianZhui + word\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)\n",
    "        if word[-2:] == 'ce':\n",
    "            suggest_ = word[:-2]\n",
    "            suggest_ = suggest_ + 't'\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)        \n",
    "        elif word[-4:] == 'land':\n",
    "            suggest_ = word[:-4]\n",
    "            suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word == None:\n",
    "                suggest_ = suggest_ + 'lish'\n",
    "                suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "            if suggest_word != None:\n",
    "                suggest_list.append(suggest_word)  \n",
    "        #print(suggest_list)\n",
    "    suggest_list = list(set(suggest_list))\n",
    "    return suggest_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_to_anything(word):#动词变成其他词性\n",
    "    suggest_word = None\n",
    "    suggest_list = []\n",
    "\n",
    "    HouZhui_list = ['ful','tive','sive','ed','less','ly','ous','al','able','en','tic','ish','ance','tion','sion','ment','er','or','ee']\n",
    "    for HouZhui in HouZhui_list:\n",
    "        suggest_ = word + HouZhui\n",
    "        suggest_word = word_convert(word,suggest_,PorterStemmer)\n",
    "        if suggest_word != None:\n",
    "            suggest_list.append(suggest_word)\n",
    "    suggest_list = list(set(suggest_list))\n",
    "    return suggest_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    功能：\n",
    "        生成形容词，副词关联词表\n",
    "    输入：\n",
    "        word：形容词/副词\n",
    "    输出：\n",
    "        list_word：为没有添加词的其他形式，包括三音节以下词的比较级最高级\n",
    "        list_word2：为三音节及以上的词的比较级最高级，如果输入形容词比较级最高级没有more/most，该列表为空\n",
    "    说明：\n",
    "        由于三音节形容词/副词的比较级，最高级为more/most+原形容词/副词，所以特别把形容词/副词和其他词性变形区分出来\n",
    "'''\n",
    "\n",
    "def build_like_word_adj(word): #创建类似形容词列表\n",
    "    list_word = []\n",
    "    list_word2 = [] #把比较级最高级带more的放在这里\n",
    "    lemmas = lemmatizer(word, u'adj')\n",
    "    #print(lemmas)\n",
    "    for i in lemmas:\n",
    "        list_word.append(i)\n",
    "        word_er = comparative(i)\n",
    "        if \"more\" in word_er:  #把比较级带more，most的词放在另一个列表list_word2\n",
    "            list_word2.append(word_er)\n",
    "        else:\n",
    "            list_word.append(word_er)\n",
    "        word_est = superlative(i)\n",
    "        if \"most\" in word_est:\n",
    "            list_word2.append(word_est)\n",
    "        else:\n",
    "            list_word.append(word_est)\n",
    "        word_adv = adj_to_adv(i)\n",
    "        if word_adv != None:\n",
    "            list_word.append(word_adv)\n",
    "    list_N = adj_to_anything(word)\n",
    "    for N in list_N:\n",
    "        list_word.append(N)\n",
    "        \n",
    "    list_word = list(set(list_word))\n",
    "    return list_word,list_word2\n",
    "\n",
    "def build_like_word_adv(word): #创建类似形容词列表\n",
    "    list_word = []\n",
    "    list_word2 = []\n",
    "    list_special = ['however','seldom','often','never','otherwise']\n",
    "    if word in list_special:\n",
    "        list_word = [word]\n",
    "        list_word2 = []\n",
    "    else:\n",
    "        lemmas = lemmatizer(word, u'adj')\n",
    "        #print(lemmas)\n",
    "        for i in lemmas:\n",
    "            list_word.append(i)\n",
    "            word_er = comparative(i)\n",
    "            if \"more\" in word_er:\n",
    "                list_word2.append(word_er)\n",
    "            else:\n",
    "                list_word.append(word_er)\n",
    "            word_est = superlative(i)\n",
    "            if \"most\" in word_est:\n",
    "                list_word2.append(word_est)\n",
    "            else:\n",
    "                list_word.append(word_est)\n",
    "            word_adv = adv_to_adj(i)\n",
    "            if word_adv != None:\n",
    "                list_word.append(word_adv)\n",
    "    list_word = list(set(list_word))\n",
    "    return list_word,list_word2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    功能：\n",
    "        根据检查的位置整理出放入BERT模型的input_ids,input_type_ids以及检查位置在input_ids中的下标位置\n",
    "        pre_training_input_in_sentence得到检查位置所在句子的信息\n",
    "        pre_training_input_entire得到检查位置在完整text中的信息\n",
    "    输入：\n",
    "        index：在完整text中的位置\n",
    "    输出：\n",
    "        word：该下标下的单词\n",
    "        input_ids：tokens的对应字典id列表\n",
    "        input_type_ids：零列表\n",
    "        id_in_sen：检查位置在句子中的下标(pre_training_input_in_sentence的返回)\n",
    "        index：检查位置在完整text中的下标，其实就是输入的下标\n",
    "'''\n",
    "def pre_training_input_in_sentence(index): \n",
    "    sentence_id = in_sentence[index][0]\n",
    "    id_in_sen = in_sentence[index][1]\n",
    "    word = input_ids_sen[sentence_id][id_in_sen]\n",
    "    word = tokenizer.ids_to_tokens[word]\n",
    "    input_ids = copy.deepcopy(input_ids_sen[sentence_id])\n",
    "    input_type_ids = copy.deepcopy(input_type_ids_sen[sentence_id])\n",
    "\n",
    "    return word,input_ids,input_type_ids,id_in_sen\n",
    "\n",
    "def pre_training_input_entire(index): \n",
    "    word = entire_ids[index]\n",
    "    word = tokenizer.ids_to_tokens[word]\n",
    "    input_ids = copy.deepcopy(entire_ids)\n",
    "    input_type_ids = copy.deepcopy(entire_type_ids)\n",
    "\n",
    "    return word,input_ids,input_type_ids,index\n",
    "\n",
    "#[101, 1045, 2572, 3153, 2006, 1996, 2754, 1012, 102]\n",
    "#[101, 1045, 2572, 3153, 2006, 1996, 2754, 1012, 1045, 2018, 1037, 2200, 2204, 2835, 1012, 1996, 2377, 2001, 2200, 5875, 1012, 102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pattern import en\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE\n",
    "\n",
    "\n",
    "'''\n",
    "    功能：\n",
    "        1.judge_and_suggestion系列函数，这个系列函数是在analyse之前做的一个预先判断处理，判断的是该位置原来词的相关词中有没有可以代替它的词\n",
    "        2.当相关词中有词的可能性和原词的可能性的差距大于阈值，则认为原词是错的，可以用相关词替换\n",
    "        3.替换词的gap还要经过后续的检查才能决定他是不是最好的推荐，这一步骤放在了show_abnormals里\n",
    "    输入：\n",
    "        prob：该位置可能性列表\n",
    "        original：该位置原先的词\n",
    "        list_word：该位置相关词表\n",
    "        threhold：门槛，也就是阈值\n",
    "    输出：\n",
    "        judge：判断原来的词是否正确，0表示需要换词，1表示不需要换词或者说相关词里面没一个合适的\n",
    "        suggestion：相关词中最好的推荐\n",
    "        gap_with_totally_top:备选词中概率最高的和所有词中概率最高的之间的gap,可以换的词也有可能因为gap太大而遭到拒绝\n",
    "'''\n",
    "def judge_and_suggestion(prob,original,list_word,threhold):\n",
    "    top_prob = 0\n",
    "    list_word = list_word + [original]\n",
    "    original_prob = prob[tokenizer.vocab[original]]\n",
    "    best = None\n",
    "    suggestion = None\n",
    "    for word in list_word:\n",
    "        try:\n",
    "            word_id = tokenizer.vocab[word]\n",
    "            prob_word = prob[word_id]\n",
    "            if prob_word > top_prob:\n",
    "                top_prob = prob_word\n",
    "                best_word = word\n",
    "        except KeyError:#有的词enchant认为是正确的拼写，bert的词典里却没有，比如tiring，这种情况暂时没法解决，但是实际上bert不认的词会自动分词\n",
    "            pass\n",
    "\n",
    "    totally_top = prob.max().item() #最高的概率（不需要知道概率最大的词是哪一个）\n",
    "    gap_with_origin = math.log(top_prob) - math.log(original_prob) #备选词中最大概率和原来的词的概率的差\n",
    "    gap_with_totally_top = math.log(totally_top) - math.log(top_prob) #所有词中最高的概率和备选词中最高的概率的差\n",
    "    \n",
    "    if gap_with_origin > threhold:\n",
    "        suggestion = best_word\n",
    "        return 0,suggestion,gap_with_totally_top\n",
    "    else:\n",
    "        return 1,suggestion,gap_with_totally_top\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分析各种词性系列函数\\n    功能：对第一遍检查得出的有问题的位置的单词，根据不同的词性进行不同步骤的分析\\n    输入：\\n        index：在原文中的错误位置\\n        prob：该位置可能性列表\\n        gap：原文该位置的词和概率最高的词之间的gap\\n        top_word：概率最高的词\\n        threshold:免检查门槛\\n        threshold2:免修正门槛(勉强不算错)\\n        threshold3:用推荐词替换的最低要求，大于该阈值才可以替换\\n    输出：\\n        suggestion:给出的修改建议，修改建议不局限于错误位置\\n    说明：\\n        不仅局限于错误位置的分析是通过预添加或者去掉一个token，多进行一次model计算\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''分析各种词性系列函数\n",
    "    功能：对第一遍检查得出的有问题的位置的单词，根据不同的词性进行不同步骤的分析\n",
    "    输入：\n",
    "        index：在原文中的错误位置\n",
    "        prob：该位置可能性列表\n",
    "        gap：原文该位置的词和概率最高的词之间的gap\n",
    "        top_word：概率最高的词\n",
    "        threshold:免检查门槛\n",
    "        threshold2:免修正门槛(勉强不算错)\n",
    "        threshold3:用推荐词替换的最低要求，大于该阈值才可以替换\n",
    "    输出：\n",
    "        suggestion:给出的修改建议，修改建议不局限于错误位置\n",
    "    说明：\n",
    "        不仅局限于错误位置的分析是通过预添加或者去掉一个token，多进行一次model计算\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    这是一个相关代词的词典，容易混淆的词放在一个列表中\n",
    "\n",
    "'''\n",
    "like_he = ['he','his','him','himself','who', 'whom', 'whose']\n",
    "like_she = ['she','her','herself','hers','who', 'whom', 'whose']\n",
    "like_it = ['it','its','itself','who', 'whom', 'whose']\n",
    "like_i = ['i','me','my','myself','mine']\n",
    "like_you = ['you','your','yourself','yourselves']\n",
    "like_we = ['we','us','our','ours','ourselves']\n",
    "like_they = ['they','them','their','theirs']\n",
    "\n",
    "like_this = ['this', 'these'] \n",
    "like_that = ['that','those'] \n",
    "pronoun_Question = ['who', 'whom', 'whose', 'which', 'what', 'whoever', 'whichever', 'whatever'] #疑问代词\n",
    "pronoun_relation =  ['that', 'which', 'who', 'whom', 'whose', 'as'] #关系代词\n",
    "like_some = ['some','any']\n",
    "like_few = ['few','little']\n",
    "like_many = ['many','much']\n",
    "like_other = ['another','other']\n",
    "\n",
    "pronoun = [like_he,like_she,like_it,like_i,like_you,like_we,like_they,like_this,like_that,pronoun_Question,pronoun_relation,like_some,like_few,like_many,like_other]\n",
    "pronoun_dictionary = {}\n",
    "pronoun_list = []\n",
    "for list_word in pronoun:\n",
    "    pronoun_list = pronoun_list + list_word\n",
    "    for word in list_word:\n",
    "        pronoun_dictionary.update({word:list_word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,PRESENT,SG,INFINITIVE, PRESENT, PAST, FUTURE, PROGRESSIVE\n",
    "\n",
    "def analyse_V(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "#这是一个处理动词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "    if gap < threshold:\n",
    "        return None\n",
    "    #******************************top_word暗示我应该是不定式**************************\n",
    "    if top_word in [\"to\",\"for\"]:\n",
    "        wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "        input_ids.insert(index,tokenizer.vocab['to'])\n",
    "        input_type_ids.append(0)\n",
    "        list_word = [conjugate(verb=wordV,tense=PRESENT,person = 1)]\n",
    "        suggestion,need,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,5) \n",
    "        if need == 1:\n",
    "            return 'to ' + suggestion \n",
    "        \n",
    "    #*****************************判断是不是时态或者拼写错误，又或者是其他词性********\n",
    "    wordV = get_word(index)\n",
    "    #这三种是不涉及位置变化的检查，根据生成词表的速度从快到慢依次检查，之后也不需要再生成词表\n",
    "\n",
    "    list_V = lexeme(wordV)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordV,list_V,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion\n",
    "    \n",
    "    list_others = V_to_anything(conjugate(verb=wordV,tense=PRESENT,person = 1))\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordV,list_others,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    list_spell_correct = d.suggest(wordV)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordV,list_spell_correct,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion\n",
    "    \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    \n",
    "    front_word = get_word(index - 1)\n",
    "    behind_word = get_word(index + 1)\n",
    "    #**************************************判断是不是缺介词***************************\n",
    "    list_IN = [\"to\",\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\",\"of\"]\n",
    "    if behind_word not in list_IN:\n",
    "        print(\"检查点\")\n",
    "        wordV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "        input_ids.insert(id_in_sen + 1,tokenizer.vocab['at'])#就随便插入一个东西，占位子\n",
    "        input_type_ids.append(0)\n",
    "        suggestion_IN,need_IN,_,_ = give_suggestion(input_ids,input_type_ids,id_in_sen + 1,list_IN,2)\n",
    "        if need_IN == 1:\n",
    "            input_ids[id_in_sen + 1] = tokenizer.vocab[suggestion_IN]\n",
    "            list_word = list_V\n",
    "            suggestion_V,need,_,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,5)\n",
    "            if need == 1:\n",
    "                suggestion = suggestion_V + ' ' + suggestion_IN\n",
    "                return suggestion\n",
    "    \n",
    "    need_to_will = need_be = 0\n",
    "    \n",
    "    #**************************************判断是不是不定式或者将来时***************************  \n",
    "    if front_word not in [\"to\",\"will\"]:\n",
    "        wordV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "        input_ids.insert(id_in_sen,tokenizer.vocab['to'])#就随便插入一个东西，占位子\n",
    "        input_type_ids.append(0)\n",
    "        try:\n",
    "            input_ids[id_in_sen + 1] = tokenizer.vocab[conjugate(verb=wordV,tense=PRESENT,person = 1)]\n",
    "            suggestion_to_will,need_to_will,prob0,_ = give_suggestion(input_ids,input_type_ids,id_in_sen,[\"to\",\"will\"],1)\n",
    "        except KeyError:\n",
    "            need_to_will = 0\n",
    "    #**************************************判断是不是被动语态或者进行时*******************   \n",
    "    list_be = lexeme('be')\n",
    "    list_be = lexeme('be')[:8] #把否定去掉  \n",
    "    #********************是不是被动语态****************   \n",
    "\n",
    "    wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "    input_ids.insert(index,tokenizer.vocab['be'])#就随便插入一个东西，占位子\n",
    "    input_type_ids.append(0)\n",
    "    try:\n",
    "        input_ids[index + 1]=tokenizer.vocab[conjugate(verb=wordV,tense=PAST,aspect=PROGRESSIVE)]\n",
    "        suggestion1,need_be1,prob1,_ = give_suggestion(input_ids,input_type_ids,index,list_be,1)\n",
    "    except KeyError:\n",
    "        need_be1 = 0\n",
    "        \n",
    "    #********************是不是现在分词****************   \n",
    "    try:\n",
    "        input_ids[index + 1]=tokenizer.vocab[conjugate(verb=wordV,tense=PRESENT,aspect=PROGRESSIVE)]\n",
    "        suggestion2,need_be2,prob2,_ = give_suggestion(input_ids,input_type_ids,index,list_be,1)\n",
    "        #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    except KeyError:\n",
    "        need_be2 = 0\n",
    "\n",
    "    #***************************选择是不定式还是被动语态还是进行时****************************\n",
    "    prob_max = 0\n",
    "    if need_to_will == 1:\n",
    "        prob_max = max(prob_max,prob0)\n",
    "    if need_be1 == 1:\n",
    "        prob_max = max(prob_max,prob1)\n",
    "    if need_be2 == 1:\n",
    "        prob_max = max(prob_max,prob2)\n",
    "\n",
    "    if need_to_will == 1 and prob_max == prob0:\n",
    "        need_be = 0\n",
    "    if need_be1 == 1 and prob_max == prob1:\n",
    "        need_to_will = 0\n",
    "        need_be = 1\n",
    "        be_ = suggestion1\n",
    "    if need_be2 == 1 and prob_max == prob2:\n",
    "        need_to_will = 0\n",
    "        need_be = 1\n",
    "        be_ = suggestion2\n",
    "    #*************************************************处理各种语法******************************************************************\n",
    "    if need_to_will == 1:\n",
    "        wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "        input_ids.insert(index,tokenizer.vocab[suggestion_to_will])\n",
    "        input_type_ids.append(0)\n",
    "        list_word = [conjugate(verb=wordV,tense=PRESENT,person = 1),conjugate(verb=wordV,tense=PRESENT,aspect=PROGRESSIVE)]\n",
    "        suggestion,need,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,5)\n",
    "        if need == 1:\n",
    "            return 'to ' + suggestion\n",
    "        else:\n",
    "            return top_word\n",
    "\n",
    "    elif need_be == 1:\n",
    "        #********************************被动语态或者进行时*****************\n",
    "        wordV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "        input_ids.insert(index,tokenizer.vocab[be_])\n",
    "        input_type_ids.append(0)\n",
    "        list_word = lexeme(wordV)\n",
    "        suggestion,need,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,5)\n",
    "        if need == 1:\n",
    "            return be_ + ' '+ suggestion\n",
    "        else:\n",
    "            return top_word\n",
    "    else:\n",
    "        return top_word\n",
    "    \n",
    "    return suggestion\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyse_adj(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    if gap < threshold:\n",
    "        return None\n",
    "    wordADJ = get_word(index)\n",
    "    #*****************************判断是不是时态或者拼写错误，又或者是其他词性********\n",
    "    list_word,list_word2 = build_like_word_adj(wordADJ)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordADJ,list_word,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    list_spell_correct = d.suggest(wordADJ)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordADJ,list_spell_correct,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion\n",
    "    \n",
    "    #list_word = list_word + list_spell_correct\n",
    "    front_word = get_word(index - 1)\n",
    "    behind_word = get_word(index + 1)\n",
    "    if front_word in ['more','most'] and len(list_word2) == 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级不需要加more/most，但是前面有more/most\n",
    "        wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        del input_ids[id_in_sen - 1]\n",
    "        del input_type_ids[0]\n",
    "        suggestion3,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen - 1,list_word,min(threshold2, gap - threshold3))\n",
    "        return '去掉前面 ' + get_word(index - 1)+ ' 原位置改成 ' + suggestion3\n",
    "    \n",
    "    elif behind_word in ['##er','##r'] and len(list_word2) != 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级需要more/most，但是错写成形容词+er/est\n",
    "        wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids[id_in_sen] = tokenizer.vocab['more']\n",
    "        suggestion5,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen + 1,list_word,min(threshold2, gap - threshold3))\n",
    "        return '去掉后面 '+ get_word(index + 1) + ' 原位置改成 '+ 'more' + ' ' + suggestion5  \n",
    "    \n",
    "    elif behind_word in ['##est','##st'] and len(list_word2) != 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级需要more/most，但是错写成形容词+er/est\n",
    "        wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids[id_in_sen] = tokenizer.vocab['most']\n",
    "        suggestion5,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen + 1,list_word,min(threshold2, gap - threshold3))\n",
    "        return '去掉后面 '+ get_word(index + 1) + ' 原位置改成 '+ 'most' + ' ' + suggestion5  \n",
    "        \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    \n",
    "    if front_word not in ['this','that','these','those','more','most']:#检查形容词前面是否需要加冠词或者是需要more，most的比较级，最高级抑或是be动词\n",
    "        wordADJ,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "        input_type_ids.append(0)\n",
    "        list_front = ['the','a','an','this','that','these','those','some','any','all','more','most','am','is','are','was','were'] \n",
    "        suggestion,need_front,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_front,2)\n",
    "        if need_front == 1:\n",
    "            wordADJ,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "            input_ids.insert(index,tokenizer.vocab[suggestion])\n",
    "            input_type_ids.append(0)\n",
    "            suggestion2,need,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,min(threshold2, gap - threshold3))     \n",
    "            if need == 1:\n",
    "                return suggestion + ' ' + suggestion2\n",
    "            else:\n",
    "                return top_word\n",
    "        \n",
    "    return top_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_adv(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    if gap < threshold:\n",
    "        return None\n",
    "    \n",
    "    wordADV = get_word(index)\n",
    "    if wordADV in ['not']:\n",
    "        return None\n",
    "    #*****************************判断是不是时态或者拼写错误，又或者是其他词性********\n",
    "    \n",
    "    list_word,list_word2 = build_like_word_adv(wordADV)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordADV,list_word,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    list_spell_correct = d.suggest(wordADV)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordADV,list_spell_correct,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion\n",
    "\n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    \n",
    "    #list_word = list_word + list_spell_correct\n",
    "    if get_word(index - 1) in ['more','most'] and len(list_word2) == 0:\n",
    "        #判断是不是比较级使用错误,这个if语句处理：该形容词比较级/最高级不需要加more/most，但是前面有more/most \n",
    "        wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        del input_ids[id_in_sen - 1]\n",
    "        del input_type_ids[0]\n",
    "        suggestion3,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen - 1,list_word,5)\n",
    "        return '去掉前面 ' + get_word(index - 1)+ ' 原位置改成 ' + suggestion3\n",
    "    \n",
    "    elif get_word(index + 1) in ['##er','##r'] and len(list_word2) != 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级需要more/most，但是错写成形容词+er/est\n",
    "        wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids[id_in_sen] = tokenizer.vocab['more']\n",
    "        suggestion5,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen+1,list_word,5)\n",
    "        return '去掉后面 '+ get_word(index + 1) + ' 原位置改成 '+ 'more' + ' ' + suggestion5  \n",
    "    \n",
    "    elif get_word(index + 1) in ['##est','##st'] and len(list_word2) != 0:\n",
    "        #判断是不是比较级使用错误,如果该形容词比较级/最高级需要more/most，但是错写成形容词+er/est\n",
    "        wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index) \n",
    "        input_ids[id_in_sen] = tokenizer.vocab['most']\n",
    "        suggestion5,need_adj,prob,_ = give_suggestion(input_ids,input_type_ids,id_in_sen+1,list_word,5)\n",
    "        return '去掉后面 '+ get_word(index + 1) + ' 原位置改成 '+ 'most' + ' ' + suggestion5  \n",
    "\n",
    "    else:\n",
    "        #检查形容词前面是否需要加冠词或者是需要more，most的比较级，最高级，be动词\n",
    "        wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "        input_ids.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "        input_type_ids.append(0)\n",
    "        list_front = ['the','a','an','this','that','these','those','some','any','all','more','most','am','is','are','was','were'] \n",
    "        suggestion,need_front,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_front,2)\n",
    "        if need_front == 1:\n",
    "            wordADV,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "            input_ids.insert(index,tokenizer.vocab[suggestion])\n",
    "            input_type_ids.append(0)\n",
    "            #print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "            suggestion2,need,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_word,5)   \n",
    "            if need == 1:\n",
    "                return suggestion + ' ' + suggestion2\n",
    "            else:\n",
    "                return top_word\n",
    "        else:\n",
    "            wordADV,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "            input_ids.insert(id_in_sen + 1,tokenizer.vocab[\",\"])\n",
    "            input_type_ids.append(0)\n",
    "            suggestion3,need_douhao,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_word,2)\n",
    "            if need_douhao == 1:\n",
    "                return suggestion3 + ' ,'\n",
    "            else:\n",
    "                return top_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grandparents', 'grandpas']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_to_anything(\"grandpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import article,referenced,pluralize, singularize\n",
    "import nltk\n",
    "def analyse_N(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    #这是一个处理名词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "    if gap < threshold:\n",
    "        return None\n",
    "    \n",
    "    wordN = get_word(index)\n",
    "    #*****************************判断是不是时态或者拼写错误，又或者是其他词性********\n",
    "    word_tag = nltk.pos_tag([wordN])\n",
    "    if word_tag[0][1] == \"NN\":\n",
    "        N_ = wordN\n",
    "        N_s= pluralize(wordN)\n",
    "    else:\n",
    "        N_ = singularize(wordN)\n",
    "        N_s= wordN\n",
    "    list_N = [N_,N_s]\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordN,list_N,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion\n",
    "    \n",
    "    list_others = N_to_anything(N_)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordN,list_others,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "        \n",
    "    list_spell_correct = d.suggest(wordN)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordN,list_spell_correct,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion\n",
    "\n",
    "    #***********************************************************************************************************************************\n",
    "    need_DT = 0 #表示是否需要在前面加冠词 \n",
    "    wordN,input_ids,input_type_ids,id_in_sen = pre_training_input_in_sentence(index)\n",
    "\n",
    "    #*****************************************判断是否需要冠词或介词************************************************************************   \n",
    "    list_DT = ['the','a','an']\n",
    "    front_word = get_word(index - 1)\n",
    "    if front_word in list_DT:#如果前一个词就是冠词，那么一定不需要再往前面加介词或冠词\n",
    "        if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "            return None\n",
    "        else:\n",
    "            return top_word\n",
    "    \n",
    "    input_ids.insert(id_in_sen,tokenizer.vocab[\"[MASK]\"])\n",
    "    input_type_ids.append(0)\n",
    "    list_IN = [\"of\",'to',\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\"]\n",
    "    list_DT_IN = list_DT + list_IN\n",
    "    suggestion,need_DT_IN,_,_= give_suggestion(input_ids,input_type_ids,id_in_sen,list_DT_IN,2)\n",
    "    if need_DT_IN == 0:#不需要冠词或介词\n",
    "        if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "            return None\n",
    "        else:\n",
    "            return top_word\n",
    "        \n",
    "    elif need_DT_IN == 1:#需要冠词或介词\n",
    "        wordN,input_ids,input_type_ids,index = pre_training_input_entire(index)\n",
    "        input_ids.insert(index,tokenizer.vocab[suggestion])\n",
    "        input_type_ids.append(0)\n",
    "        suggestion2,need,_,_= give_suggestion(input_ids,input_type_ids,index + 1,list_N ,min(9.5,gap - threshold3))\n",
    "        if need == 1:\n",
    "            return suggestion + ' ' + suggestion2\n",
    "        \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    else:\n",
    "        return top_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_pronoun(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    #这是一个处理代词语法问题的函数，输入为问题词在text的token中的下标index\n",
    "    if gap < threshold:\n",
    "        return None\n",
    "    \n",
    "    wordPROP = get_word(index)\n",
    "    #*****************************判断是不是时态或者拼写错误，又或者是其他代词********\n",
    "    try:\n",
    "        list_PROP = pronoun_dictionary[wordPROP]\n",
    "    except:\n",
    "        list_PROP = []\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordPROP,list_PROP,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "\n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    else:\n",
    "        judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordPROP,pronoun_list,threshold3)#在所有代词里面选择\n",
    "        if judge==0 and gap_with_totally_top < threshold2:\n",
    "            return suggestion \n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_DT(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    if gap < threshold:\n",
    "        return None  \n",
    "    \n",
    "    wordDT = get_word(index)\n",
    "    if wordDT in [\"every\",'per','each','no']:#有实际意义，不做修改\n",
    "        return None\n",
    "\n",
    "    if wordDT in ['some']:\n",
    "        list_word = ['some','any','a','an']\n",
    "    elif wordDT in ['any']:\n",
    "        list_word = ['some','any',\"every\",'per','each']\n",
    "    elif wordDT in ['this','that','these','those']:\n",
    "        list_word = ['this','that','these','those']\n",
    "    elif wordDT in ['the','a','an']:\n",
    "        list_word = ['the','a','an','some','any']\n",
    "    elif wordDT in ['another','other']:\n",
    "        list_word = ['another','other']\n",
    "    elif wordDT in ['all','both']:\n",
    "        list_word = ['all','both']\n",
    "    else:\n",
    "        list_word = [wordDT]\n",
    "    \n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordDT,list_word,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    \n",
    "    elif top_word in [\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\",\"of\",'to']:\n",
    "        return top_word + ' ' + wordDT\n",
    "    else:\n",
    "        if top_word in ['some','any','this','that','these','those','the','a','an']:\n",
    "            return top_word\n",
    "        elif wordDT in ['another','other','all','both']:\n",
    "            return None\n",
    "        else:\n",
    "            return \"去掉 \" + wordDT\n",
    "#print(analyse_DT(77))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_IN(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    #检查介词，确认需不需要删掉或者换介词\n",
    "    if gap < threshold:\n",
    "        return None  \n",
    "    \n",
    "    wordIN = get_word(index)\n",
    "    if wordIN in ['before',\"after\",\"above\",\"below\",\"underneath\",\"beneath\",\"without\"]:#有实际意义，不做修改\n",
    "        return None\n",
    "    list_word = [\"at\",\"in\",\"on\",\"by\",\"for\",\"from\",\"with\",\"about\",\"against\",\"along\",\"among\",\"around\",\"as\",\"before\",\"behind\",\"below\",\"beside\",\"between\",\"during\",\"besides\",\"into\",\"near\",\"over\",\"through\",\"under\",\"without\",\"after\",\"above\",\"of\",'to']\n",
    "    \n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordIN,list_word,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    list_spell_correct = d.suggest(wordIN)\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordIN,list_spell_correct,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion    \n",
    "    \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    elif top_word in u',.!?[]()<>\"\\'':\n",
    "        return top_word\n",
    "    else:\n",
    "        return \"去掉 \" + wordIN\n",
    "#print(analyse_IN(76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_CC(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    if gap < threshold:\n",
    "        return None  \n",
    "    \n",
    "    wordCC = get_word(index)\n",
    "    list_CC = [\"but\",\"because\",\"yet\",\"still\",\"however\",\"although\",\"so\",\"thus\",\"and\",\"or\",\"too\",\"either\",\"or\",\"neither\",\"nor\",\"when\",\"while\",\"as\",\"whenever\",\"since\",\"until\",\"till\",\",\"]\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordCC,list_CC,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_MD(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    if gap < threshold:\n",
    "        return None     \n",
    "    \n",
    "    wordMD = get_word(index)\n",
    "    if wordMD in ['can','could']:\n",
    "        list_MD = ['can','could']\n",
    "    elif wordMD in ['may','might']:\n",
    "        list_MD = ['may','might']\n",
    "    elif wordMD in ['shall','should']:\n",
    "        list_MD = ['shall','should']   \n",
    "    elif wordMD in ['will','would']:\n",
    "        list_MD = ['will','would']  \n",
    "    elif wordMD in ['dare','dared']:\n",
    "        list_MD = ['dare','dared']  \n",
    "    else:\n",
    "        list_MD = [wordMD]\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,wordMD,list_MD,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_biaodian(index,prob,gap,top_word,threshold,threshold2,threshold3):\n",
    "    if gap < threshold:\n",
    "        return None     \n",
    "    \n",
    "    biaodian = get_word(index) \n",
    "    biaodian_list = ['.',',',';','!','?','\"',\"'\",'，','。','’','‘','“','”','and','but']\n",
    "    judge,suggestion,gap_with_totally_top = judge_and_suggestion(prob,biaodian,biaodian_list,threshold3)\n",
    "    if judge==0 and gap_with_totally_top < threshold2:\n",
    "        return suggestion  \n",
    "    \n",
    "    if gap < threshold2:#没有可以替换的词，而且原本该位置的词就勉强符合要求\n",
    "        return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    功能：\n",
    "        这是几个和拼写检查相关函数\n",
    "        correct_spelling：用于发现text中拼写错误，写成不存在的词的情况，并暂时把它改成存在的词，这样再放入模型训练，完成之后的步骤\n",
    "        token_Align：展示拼写错误时需要将原来错误的词显示出来，由于BERT的tokenize会把错误的词分段，造成未知序号的混乱，因而需要将原来的token和被correct的token位置对齐\n",
    "        这两个函数需要配合使用\n",
    "'''\n",
    "import enchant\n",
    "import re\n",
    "d = enchant.Dict(\"en_US\")\n",
    "from pattern.en import suggest\n",
    "\n",
    "def C_trans_to_E(string): #标点符号转换函数\n",
    "    E_pun = u',.!?[]()<>\"\\'\"\\'.:;'\n",
    "    C_pun = u'，。！？【】（）《》“‘”’．：'\n",
    "    table= {ord(f):ord(t) for f,t in zip(C_pun,E_pun)}\n",
    "    return string.translate(table)\n",
    "\n",
    "def process_biaodian(text):#把标点和字母分开，使得用split分词能把标点分成单独的token,顺便把中文标点变成英文标点\n",
    "    text1 = ''\n",
    "    for character in text[0]: \n",
    "        if character in u',.!?[]()<>\"\\':-;，。！？【】（）《》“‘”’．%':\n",
    "            character1 = C_trans_to_E(character)\n",
    "            text1 = text1 + ' '+character1+' '\n",
    "        else:\n",
    "            text1 = text1 + character \n",
    "    return [text1]\n",
    "\n",
    "def correct_spelling(text):\n",
    "    #text:原本可能带有拼写错误的文本\n",
    "    #返回[correct_text]：不带拼写错误的文本,外面套上中括号，保持列表的形式\n",
    "    global suggestions\n",
    "    correct_text = ''\n",
    "    text0 = text\n",
    "    text1 = ''\n",
    "    \n",
    "    tokens = text.split(' ')\n",
    "    for token in tokens: #给拼写错误的单词标上‘错’\n",
    "        if token not in ['.',',',';','!','?','\"',\"'\",'，','。','’','‘','“','”',\"\\r\\n\",\"\"]:\n",
    "            if d.check(token)==False and token != suggest(token)[0][0]:\n",
    "                word =  '不' + suggest(token)[0][0] #pattern的suggestion \n",
    "            else:\n",
    "                word = token\n",
    "        elif token == \"\\r\\n\":\n",
    "            word = '换'\n",
    "        else:\n",
    "            word = token\n",
    "        correct_text = correct_text + ' ' + word\n",
    "    tokens = tokenizer.tokenize(correct_text) \n",
    "    length = len(tokens)\n",
    "    correct_text = \"\"\n",
    "    i = 0\n",
    "    while(i < length):\n",
    "\n",
    "        if tokens[i] == '不':#中文乱码\n",
    "            suggestions.update({i+1:tokens[i+1]})#给外部变量suggestions添加错误\n",
    "            del tokens[i]\n",
    "            length = length - 1\n",
    "        elif tokens[i][0:2] == '##':\n",
    "            word = tokens[i][2:]\n",
    "            correct_text = correct_text + word  \n",
    "            i = i+1\n",
    "        else:\n",
    "            token = tokens[i]\n",
    "            if token not in [\"'\"]:\n",
    "                word = ' '+ token\n",
    "            else:\n",
    "                word = token\n",
    "                \n",
    "            correct_text = correct_text + word  \n",
    "            i = i+1\n",
    "    return [correct_text]\n",
    "\n",
    "\n",
    "def token_Align(tokens,text): \n",
    "    #tokens是拼写修正之后的文本的分词结果\n",
    "    #text是原本可能带有拼写错误的文本\n",
    "    #返回的是text的分词结果\n",
    "    original_tokens = tokenizer.tokenize(text)\n",
    "    original_tokens = ['[CLS]'] + original_tokens + ['[SEP]']\n",
    "    print(original_tokens)\n",
    "    length = len(tokens)\n",
    "    i = 0\n",
    "    while(i < min(length - 1,len(original_tokens) - 1)):\n",
    "        if original_tokens[i] == tokens[i] or original_tokens[i+1] == tokens[i+1] or original_tokens[i+2] == tokens[i+2] or original_tokens[i+3] == tokens[i+3]:\n",
    "            i = i+1\n",
    "            continue\n",
    "        else:\n",
    "            if original_tokens[i][:2] == \"##\":\n",
    "                original_tokens[i-1] = original_tokens[i-1] + original_tokens[i][2:]\n",
    "                del original_tokens[i]\n",
    "            elif original_tokens[i+1][:2] == \"##\":\n",
    "                original_tokens[i] = original_tokens[i] + original_tokens[i+1][2:]\n",
    "                del original_tokens[i+1]            \n",
    "            elif tokens[i] == '[UNK]':\n",
    "                original_tokens.insert(i,'[UNK]')\n",
    "            else:\n",
    "                if original_tokens[i+1] == tokens[i] or original_tokens[i+2] == tokens[i+1] or original_tokens[i+3] == tokens[i+2]:\n",
    "                    if re.match(r'[a-z]',original_tokens[i]) == None :\n",
    "                        original_tokens[i] = original_tokens[i] + original_tokens[i+1]\n",
    "                        del original_tokens[i+1] \n",
    "                elif original_tokens[i] == tokens[i+1] or original_tokens[i+1] == tokens[i+2] or original_tokens[i+2] == tokens[i+3]:\n",
    "                    original_tokens.insert(i,' ')\n",
    "                i = i + 1\n",
    "                \n",
    "    return original_tokens\n",
    "\n",
    "def split_text(text0,threshold1,threshold2):\n",
    "    #把文章分成一定长度的文段，保证GPU可以正常使用以及BERT模型不会超过最大的embeding\n",
    "    #当计数大于threshold1并且达到句尾时，将文本分开\n",
    "    #当计数大于threshold2并且达到分段位置时，将文本分开\n",
    "    #我们希望尽量能按照段落分，因此threshold2要比threshold1稍小一些\n",
    "    texts = []\n",
    "    text = ''\n",
    "    tokens = text0[0].split(' ')\n",
    "    count_tokens = 0\n",
    "    last_HuanHang = -1\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == '':\n",
    "            continue\n",
    "        count_tokens = count_tokens + 1\n",
    "        text = text + ' '+ token\n",
    "        if (token == '.'and count_tokens > threshold1) or (token == '\\r\\n' and count_tokens > threshold2):\n",
    "            texts.append([text])\n",
    "            text = ''\n",
    "            count_tokens = 0\n",
    "    if count_tokens > 0:        \n",
    "        texts.append([text])        \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' i drive at home .']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I arive at home.\"\n",
    "correct_spelling(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'that', 'will', 'generate', 'the', 'ur', '##l', 'string', '/', 'about', '?', 'name', '=', 'ze', '##it', ',', 'you', 'can', 'use', 'every', 'property', 'as', 'defined', 'in', 'the', 'node', '.', 'j', '##s', 'ur', '##l', 'module', 'documentation', '.', '[SEP]']\n",
      "[CLS] [CLS]\n",
      "that that\n",
      "will will\n",
      "generate generate\n",
      "the the\n",
      "curl url\n",
      "string string\n",
      "about /about\n",
      "? ?\n",
      "name name\n",
      "= =\n",
      "ze ze\n",
      "##st ##it\n",
      ", ,\n",
      "you you\n",
      "can can\n",
      "use use\n",
      "every every\n",
      "property property\n",
      "as as\n",
      "defined defined\n",
      "in in\n",
      "the the\n",
      "node node\n",
      ". .\n",
      "is js\n",
      "curl url\n",
      "module module\n",
      "documentation documentation\n",
      ". .\n",
      "[SEP] [SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[' that will generate the url string /about ? name = zeit , you can use every property as defined in the node .'],\n",
       " [' js url module documentation .']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"that will generate the url string /about ? name = zeit , you can use every property as defined in the node .js url module documentation .\"]\n",
    "text = process_biaodian(text)\n",
    "new_text = correct_spelling(text[0])\n",
    "tokens = tokenizer.tokenize(new_text[0])\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "original_tokens = token_Align(tokens,text[0])\n",
    "len_ = len(tokens)\n",
    "for i in range(0,len_):\n",
    "    print(tokens[i],original_tokens[i])\n",
    "    \n",
    "split_text(text,20,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pattern.en import conjugate, lemma, lexeme,PRESENT,SG\n",
    "'''\n",
    "    这是一个输出BERT模型训练结果的函数，方便查看调试\n",
    "'''\n",
    "def show_lm_probs(tokens, input_ids, probs, topk=5, firstk=20): #输出结果的函数，要最高概率topk个输出\n",
    "    def print_pair(token, prob, end_str='', hit_mark=' '):\n",
    "        if i < firstk:\n",
    "            # token = token.replace('</w>', '').replace('\\n', '/n')\n",
    "            print('{}{: >3} | {: <12}'.format(hit_mark, int(round(prob*100)), token), end=end_str)\n",
    "    \n",
    "    ret = None\n",
    "    for i in range(len(tokens)):\n",
    "        ind_ = input_ids[i].item() if input_ids is not None else tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item() #这个probs是该字符串第i个位置上填上词典上各个词的概率，prob_是词典上原来天的这个词的概率\n",
    "        print_pair(tokens[i], prob_, end_str='\\t')\n",
    "        values, indices = probs[i].topk(topk)\n",
    "        #print(values, indices)\n",
    "        #print(\"****************************************************************************************************\")\n",
    "        top_pairs = []\n",
    "        for j in range(topk):\n",
    "            ind, prob = indices[j].item(), values[j].item()\n",
    "            hit_mark = '*' if ind == ind_ else ' '\n",
    "            token = tokenizer.ids_to_tokens[ind]\n",
    "            print_pair(token, prob, hit_mark=hit_mark, end_str='' if j < topk - 1 else '\\n')\n",
    "            top_pairs.append((token, prob))\n",
    "        if tokens[i] == \"[MASK]\":\n",
    "            ret = top_pairs\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prob(prob,token):\n",
    "    ind_ = tokenizer.vocab[token]\n",
    "    prob_ = prob[ind_].item()\n",
    "    top_prob = prob.max().item()\n",
    "    top_ind = prob.argmax().item()\n",
    "    top_word = tokenizer.ids_to_tokens[top_ind] #可能性最高的词\n",
    "    gap = math.log(top_prob) - math.log(prob_) #计算两个词之间的差距 \n",
    "    return top_word,gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def analyse_词性(token,tag):\n",
    "    if 'VB' in tag: #如果是动词的各种时态\n",
    "        tag0 = \"v\"\n",
    "    elif \"JJ\" in tag : #形容词\n",
    "        tag0 = \"a\"\n",
    "    elif \"RB\" in tag: #副词\n",
    "        tag0 = \"r\"\n",
    "    elif \"NN\" in tag: #名词\n",
    "        tag0 = \"n\"\n",
    "    else:\n",
    "        return tag\n",
    "    if wn.morphy(token, tag0)==None:\n",
    "        tag = nltk.pos_tag([token])[0][1]\n",
    "    return tag\n",
    "    \n",
    "def show_abnormals(tokens,probs,text,show_suggestions=False): #多加了一个参数text，用来生成原来的token的\n",
    "    global suggestions\n",
    "    global original_tokens\n",
    "    original_tokens = token_Align(tokens,text)\n",
    "    def gap2color(mode):\n",
    "        if mode == 1:\n",
    "            return 'yellow_1'\n",
    "        elif mode == 2:\n",
    "            return 'orange_1'\n",
    "        else:\n",
    "            return 'red_1'\n",
    "        \n",
    "    def print_token(token, suggestion, gap ,mode):\n",
    "        if gap == 0 and mode == 1:\n",
    "            print(stylize(token + ' ', colored.fg('white') + colored.bg('black')), end='')\n",
    "        else:\n",
    "            print(stylize(token, colored.fg(gap2color(mode)) + colored.bg('black')), end='')\n",
    "            if show_suggestions and mode > 1:\n",
    "                print(stylize('/' + str(suggestion) + ' ', colored.fg('green' if gap > 10 else 'cyan') + colored.bg('black')), end='')\n",
    "            else:\n",
    "                print(stylize(' ', colored.fg(gap2color(mode)) + colored.bg('black')), end='')\n",
    "\n",
    "        \n",
    "    avg_gap = 0.\n",
    "    tokens_tag = nltk.pos_tag(tokens) #给整个text做词性标注\n",
    "    for i in range(1, len(tokens) - 1):  # skip first [CLS] and last [SEP]\n",
    "        if tokens[i]=='[UNK]':\n",
    "            continue\n",
    "        top_word,gap = analyse_prob(probs[i],tokens[i])\n",
    "        print()\n",
    "        print(\"*******************************************************************************************************************\")\n",
    "        print(i)\n",
    "        print(gap)\n",
    "        avg_gap += gap\n",
    "        suggestion = None\n",
    "        tag = tokens_tag[i][1]#当前tokens的词性\n",
    "        tag = analyse_词性(tokens[i],tag)\n",
    "        print(tag)\n",
    "        \n",
    "        if 'VB' in tag: #如果是动词的各种时态\n",
    "            suggestion = analyse_V(i,probs[i],gap,top_word,2.5 ,8 ,1.8)\n",
    "                \n",
    "        elif \"DT\" == tag: #如果是冠词（冠词原则上不改变词性）\n",
    "            suggestion = analyse_DT(i,probs[i],gap,top_word,3 ,4 ,1)\n",
    "            \n",
    "        elif \"JJ\" in tag : #形容词\n",
    "            suggestion = analyse_adj(i,probs[i],gap,top_word,6 ,8 ,2)\n",
    "                \n",
    "        elif \"RB\" in tag: #副词\n",
    "            suggestion = analyse_adv(i,probs[i],gap,top_word,5 ,8 ,2)\n",
    "            \n",
    "        elif \"PRP\" in tag: #代词\n",
    "            suggestion = analyse_pronoun(i,probs[i],gap,top_word,4 ,5 ,1.5)\n",
    "            \n",
    "        elif \"NN\" in tag: #名词\n",
    "            suggestion = analyse_N(i,probs[i],gap,top_word,4 ,10 ,2.2)\n",
    "                    \n",
    "        elif \"CC\" in tag: #连词\n",
    "            suggestion = analyse_CC(i,probs[i],gap,top_word,2 ,2.5 ,1.5)\n",
    "                \n",
    "        elif \"IN\" == tag or 'TO' == tag: #介词\n",
    "            suggestion = analyse_IN(i,probs[i],gap,top_word,3.5 ,4 ,1.5)\n",
    "                \n",
    "        elif 'MD' in tag: #情态动词\n",
    "            suggestion = analyse_MD(i,probs[i],gap,top_word,3 ,4 ,1.5)\n",
    "                \n",
    "        elif \"CD\" in tag: #数词直接pass\n",
    "            pass \n",
    "            \n",
    "        elif \"WDT\" == tag and gap > 3.5: #who，which，that那些\n",
    "            suggestion = top_word #推荐的词一般比较准\n",
    "          \n",
    "        elif tokens[i] in u',.!?[]()<>\"\\':，。！？【】（）《》“‘”’．':\n",
    "            suggestion = analyse_biaodian(i,probs[i],gap,top_word,1.3 ,2 ,1)\n",
    "            \n",
    "        elif gap > 5:\n",
    "            suggestion = top_word\n",
    "        \n",
    "        if (suggestion != None and suggestion.lower() != tokens[i] and suggestion.lower() != original_tokens[i]): #修改存在并且是另外一个词\n",
    "            suggestions.update({i:suggestion})\n",
    "            mode = 2\n",
    "        elif suggestions.__contains__(i)==True: #这是因为之前在拼写检查时已经修改了该位置的单词\n",
    "            if original_tokens[i] == tokens[i]:\n",
    "                del suggestions[i]\n",
    "                mode = 1\n",
    "            else:\n",
    "                mode = 2\n",
    "                suggestion = suggestions[i]\n",
    "        else:\n",
    "            if original_tokens[i] != tokens[i]:\n",
    "                mode = 2\n",
    "                suggestions[i] = tokens[i]\n",
    "                suggestion = tokens[i]\n",
    "            else:\n",
    "                mode = 1\n",
    "        \n",
    "        print_token(original_tokens[i], suggestion, gap, mode)\n",
    "        print()\n",
    "        print(original_tokens[i],tokens[i],suggestion,mode)\n",
    "    avg_gap /= (len(tokens) - 2)\n",
    "    print()\n",
    "    print('平均gap:'+ str(avg_gap))\n",
    "    return avg_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_part_text(text, masked_tokens=None, show_suggestions=True, show_firstk_probs=500):\n",
    "    step = 15 #用于训练加速的步长，每15个token被mask一个位置\n",
    "    global input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids,suggestions,original_tokens\n",
    "    suggestions = {}#清空全局变量\n",
    "    text = process_biaodian(text)\n",
    "    text0 = text  #保存有拼写错误的文本\n",
    "    text = correct_spelling(text[0]) #拼写修正过得文本\n",
    "    print(\"********************************\")\n",
    "    print(text)\n",
    "    print(\"********************************\")\n",
    "    #黄金搭档token_Align放在show_abnormals里面了\n",
    "    input_ids_sen,input_type_ids_sen,in_sentence,sentences,entire_ids,entire_type_ids = process_text(text[0])\n",
    "    \n",
    "    examples = convert_text_to_examples(text)\n",
    "    features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "    given_mask = \"[MASK]\" in features[0].tokens\n",
    "    if not given_mask or masked_tokens is not None:\n",
    "        assert len(features) == 1\n",
    "        features, batches = copy_and_mask_feature(features[0],step, masked_tokens=masked_tokens)\n",
    "        #print(len(features))\n",
    "\n",
    "    input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) #把input_ids增加了一个维度，变成[n_features,sequence_len]\n",
    "    #这里的n_features实际上是句子有多少批训练\n",
    "\n",
    "    input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "    input_ids = input_ids.to(device) \n",
    "    input_type_ids = input_type_ids.to(device)\n",
    "    \n",
    "    time_start=time.time()\n",
    "    mlm_logits= model(input_ids)\n",
    "    time_end=time.time()\n",
    "    print('time cost1',time_end-time_start,'s')\n",
    "    \n",
    "    mlm_probs = F.softmax(mlm_logits, dim=-1) \n",
    "    tokens = features[0].tokens #为了输出，[mask]在input_ids里面表示出来，features的token都一样\n",
    "    print(tokens)\n",
    "    if not given_mask or masked_tokens is not None:\n",
    "        bsz, seq_len, vocab_size = mlm_probs.size() #三个维度分别是batch_size, sequence_length, vocab_size\n",
    "        assert bsz == len(batches)\n",
    "        reduced_mlm_probs = torch.Tensor(1, len(tokens), vocab_size)\n",
    "        for i in batches:\n",
    "            pos = i\n",
    "            while pos < len(tokens):\n",
    "                reduced_mlm_probs[0, pos] = mlm_probs[i, pos]\n",
    "                pos = pos + step\n",
    "        mlm_probs = reduced_mlm_probs #压缩一下大小，节约不必要浪费的空间（只需要第i个batch里面[mask]位置的词汇表概率即可）\n",
    "    top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=show_firstk_probs) #传入的probs是二维的\n",
    "    if not given_mask:\n",
    "        avg_gap = show_abnormals(tokens,mlm_probs[0],text0[0], show_suggestions=show_suggestions)\n",
    "    return suggestions,original_tokens,avg_gap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text, masked_tokens=None, show_suggestions=True, show_firstk_probs=500):\n",
    "    suggestions = {}\n",
    "    avg_gap = 0\n",
    "    new_part_suggestions = {}\n",
    "    original_tokens = ['[CLS]','[SEP]']\n",
    "    text = process_biaodian(text)\n",
    "    text0 = text  #保存有拼写错误的文本\n",
    "    texts = split_text(text,50,40)\n",
    "    accumulate_length = 0\n",
    "    remainer = 2 #[CLS]和[SEP]\n",
    "    for text0 in texts:\n",
    "        part_suggestions,part_original_tokens,part_avg_gap = analyze_part_text(text0, masked_tokens, show_suggestions, show_firstk_probs)\n",
    "        for key in part_suggestions:\n",
    "            new_part_suggestions[key + accumulate_length] = part_suggestions[key]\n",
    "        tokens_length = len(part_original_tokens)\n",
    "        accumulate_length = accumulate_length + tokens_length - remainer\n",
    "        suggestions.update(new_part_suggestions)\n",
    "        original_tokens = original_tokens[:-1] + part_original_tokens[1:]\n",
    "        avg_gap = avg_gap + part_avg_gap*(tokens_length - 2)\n",
    "    avg_gap = avg_gap/(accumulate_length-1)\n",
    "    return suggestions,original_tokens,avg_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2019 16:02:15 - INFO - examples.extract_features -   tokens: [CLS] when i was little , friday ' s night was our family game night . after supper , we would play card games of all sort in the sitting room . as the kid , i loved to watch cartoons , but no matter how many times i asked for watching them , my parents would not to let me . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "[\" when i was little , friday' s night was our family game night . after supper , we would play card games of all sort in the sitting room . as the kid , i loved to watch cartoons , but no matter how many times i asked for watching them , my parents would not to let me .\"]\n",
      "********************************\n",
      "time cost1 0.5431368350982666 s\n",
      "['[CLS]', 'when', 'i', 'was', 'little', ',', 'friday', \"'\", 's', 'night', 'was', 'our', 'family', 'game', 'night', '.', 'after', 'supper', ',', 'we', 'would', 'play', 'card', 'games', 'of', 'all', 'sort', 'in', 'the', 'sitting', 'room', '.', 'as', 'the', 'kid', ',', 'i', 'loved', 'to', 'watch', 'cartoons', ',', 'but', 'no', 'matter', 'how', 'many', 'times', 'i', 'asked', 'for', 'watching', 'them', ',', 'my', 'parents', 'would', 'not', 'to', 'let', 'me', '.', '[SEP]']\n",
      "   0 | [CLS]       \t   2 | .              1 | the            1 | )              1 | ,              1 | \"           \n",
      "  97 | when        \t* 97 | when           3 | since          0 | until          0 | while          0 | as          \n",
      "  88 | i           \t* 88 | i              7 | she            2 | he             0 | we             0 | cassie      \n",
      " 100 | was         \t*100 | was            0 | were           0 | turned         0 | got            0 | became      \n",
      "   6 | little      \t  12 | twelve        11 | younger       10 | eight       *  6 | little         5 | older       \n",
      " 100 | ,           \t*100 | ,              0 | .              0 | again          0 | ...            0 | and         \n",
      "   0 | friday      \t  47 | valentine     14 | mother         5 | father         4 | grandma        4 | children    \n",
      " 100 | '           \t*100 | '              0 | ′              0 | `              0 | \"              0 | -           \n",
      " 100 | s           \t*100 | s              0 | til            0 | n              0 | d              0 | o           \n",
      "  38 | night       \t* 38 | night         13 | dinner         7 | game           6 | eve            4 | day         \n",
      "  93 | was         \t* 93 | was            7 | became         0 | is             0 | were           0 | ,           \n",
      "   6 | our         \t  79 | a             10 | the         *  6 | our            2 | my             0 | their       \n",
      "   2 | family      \t  68 | favorite       7 | first       *  2 | family         2 | favourite      2 | only        \n",
      "   2 | game        \t  12 | fun            9 | dinner         5 | christmas      4 | '              3 | entertainment\n",
      "  85 | night       \t* 85 | night          9 | day            1 | date           1 | tonight        1 | dinner      \n",
      "  96 | .           \t* 96 | .              3 | and            1 | ;              0 | -              0 | :           \n",
      "  82 | after       \t* 82 | after          5 | over           5 | during         4 | at             2 | before      \n",
      "   7 | supper      \t  38 | dinner        15 | school         9 | midnight    *  7 | supper         4 | lunch       \n",
      " 100 | ,           \t*100 | ,              0 | together       0 | ##time         0 | and            0 | time        \n",
      "  99 | we          \t* 99 | we             0 | they           0 | everyone       0 | people         0 | i           \n",
      "  98 | would       \t* 98 | would          2 | could          0 | did            0 | might          0 | helped      \n",
      "  86 | play        \t* 86 | play          10 | have           2 | watch          0 | enjoy          0 | hold        \n",
      "   8 | card        \t  44 | board         23 | video       *  8 | card           1 | family         1 | computer    \n",
      "  98 | games       \t* 98 | games          1 | game           0 | ##io           0 | tricks         0 | matches     \n",
      " 100 | of          \t*100 | of             0 | and            0 | in             0 | ,              0 | to          \n",
      "   0 | all         \t  91 | some           5 | any            1 | every          1 | a              0 | the         \n",
      "   2 | sort        \t  33 | kinds         25 | types         16 | sorts          4 | sizes          2 | ages        \n",
      "  99 | in          \t* 99 | in             0 | around         0 | inside         0 | outside        0 | at          \n",
      "  82 | the         \t* 82 | the           15 | our            1 | my             0 | a              0 | his         \n",
      "   0 | sitting     \t  52 | family        19 | living        14 | dining         2 | back           1 | common      \n",
      " 100 | room        \t*100 | room           0 | area           0 | rooms          0 | hall           0 | ##room      \n",
      " 100 | .           \t*100 | .              0 | and            0 | ;              0 | !              0 | ...         \n",
      "  43 | as          \t* 43 | as            16 | like          15 | with           7 | for            2 | unlike      \n",
      "   0 | the         \t 100 | a              0 | an             0 | another     *  0 | the            0 | one         \n",
      "  10 | kid         \t  39 | youngest      27 | child       * 10 | kid            5 | baby           3 | oldest      \n",
      "  99 | ,           \t* 99 | ,              0 | prodigy        0 | then           0 | now            0 | here        \n",
      "  98 | i           \t* 98 | i              0 | he             0 | we             0 | she            0 | dad         \n",
      "  27 | loved       \t  36 | wanted      * 27 | loved         17 | used          13 | liked          2 | tried       \n",
      " 100 | to          \t*100 | to             0 | and            0 | playing        0 | watching       0 | being       \n",
      "  99 | watch       \t* 99 | watch          1 | see            0 | watched        0 | play           0 | watching    \n",
      "   1 | cartoons    \t  44 | games         18 | them          14 | movies         8 | cards          3 | kids        \n",
      "  95 | ,           \t* 95 | ,              4 | .              0 | -              0 | ;              0 | ...         \n",
      "  38 | but         \t  58 | and         * 38 | but            2 | so             2 | because        0 | though      \n",
      " 100 | no          \t*100 | no             0 | little         0 | the            0 | zero           0 | without     \n",
      " 100 | matter      \t*100 | matter         0 | to             0 | telling        0 | idea           0 | tell        \n",
      " 100 | how         \t*100 | how            0 | what           0 | however        0 | the            0 | who         \n",
      " 100 | many        \t*100 | many           0 | often          0 | few            0 | numerous       0 | several     \n",
      "  99 | times       \t* 99 | times          0 | kids           0 | nights         0 | years          0 | days        \n",
      "  97 | i           \t* 97 | i              1 | we             0 | he             0 | people         0 | she         \n",
      "   4 | asked       \t  32 | apologized    15 | begged        13 | paid           4 | wished      *  4 | asked       \n",
      "  25 | for         \t  41 | about       * 25 | for            7 | while          6 | after          3 | without     \n",
      "  86 | watching    \t* 86 | watching       1 | to             1 | just           1 | for            1 | playing     \n",
      "  72 | them        \t* 72 | them           9 | cartoons       4 | it             3 | one            3 | movies      \n",
      " 100 | ,           \t*100 | ,              0 | -              0 | .              0 | ...            0 | and         \n",
      "  99 | my          \t* 99 | my             0 | his            0 | the            0 | our            0 | her         \n",
      "  32 | parents     \t* 32 | parents       22 | mother        21 | father        11 | mom            4 | dad         \n",
      "   0 | would       \t  33 | decided       29 | chose          8 | tried          6 | seemed         5 | knew        \n",
      "   0 | not         \t  48 | refuse        36 | have           7 | agree          2 | promise        1 | want        \n",
      "   0 | to          \t  58 | always        26 | have           4 | ever           3 | even           2 | really      \n",
      "  56 | let         \t* 56 | let            9 | believe        8 | bother         4 | stop           3 | tell        \n",
      "  97 | me          \t* 97 | me             2 | go             0 | up             0 | it             0 | on          \n",
      " 100 | .           \t*100 | .              0 | ;              0 | !              0 | ?              0 | ...         \n",
      "   0 | [SEP]       \t  20 | \"             15 | but            7 | and            5 | so             3 | for         \n",
      "['[CLS]', 'when', 'i', 'was', 'little', ',', 'friday', \"'\", 's', 'night', 'was', 'our', 'family', 'game', 'night', '.', 'after', 'supper', ',', 'we', 'would', 'play', 'card', 'games', 'of', 'all', 'sort', 'in', 'the', 'sitting', 'room', '.', 'as', 'the', 'kid', ',', 'i', 'loved', 'to', 'watch', 'cartoons', ',', 'but', 'no', 'matter', 'how', 'many', 'times', 'i', 'asked', 'for', 'watching', 'them', ',', 'my', 'parents', 'would', 'not', 'to', 'let', 'me', '.', '[SEP]']\n",
      "\n",
      "*******************************************************************************************************************\n",
      "1\n",
      "0.0\n",
      "WRB\n",
      "\u001b[38;5;15m\u001b[48;5;0mwhen \u001b[0m\n",
      "when when None 1\n",
      "\n",
      "*******************************************************************************************************************\n",
      "2\n",
      "0.0\n",
      "NN\n",
      "\u001b[38;5;15m\u001b[48;5;0mi \u001b[0m\n",
      "i i None 1\n",
      "\n",
      "*******************************************************************************************************************\n",
      "3\n",
      "0.0\n",
      "VBD\n",
      "\u001b[38;5;15m\u001b[48;5;0mwas \u001b[0m\n",
      "was was None 1\n",
      "\n",
      "*******************************************************************************************************************\n",
      "4\n",
      "0.6453734100348458\n",
      "JJ\n",
      "\u001b[38;5;226m\u001b[48;5;0mlittle\u001b[0m\u001b[38;5;226m\u001b[48;5;0m \u001b[0m\n",
      "little little None 1\n",
      "\n",
      "*******************************************************************************************************************\n",
      "5\n",
      "0.0\n",
      ",\n",
      "\u001b[38;5;15m\u001b[48;5;0m, \u001b[0m\n",
      ", , None 1\n",
      "\n",
      "*******************************************************************************************************************\n",
      "6\n",
      "5.662634394823419\n",
      "NN\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-48412797155b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#text = [\"During my last winter holiday, I went to countryside with my father to visit my grandparents. I find a big change there. The first time I went there, they were living in a small house with dogs, ducks, and another animals. Last winter when I went here again, they had a big separate house to raise dozens of chicken. They also had a small pond which they raised fish. My grandpa said last summer they earned quite a lot by sell the fish. I felt happily that their life had improved. At the end of our trip，I told my father that I planned to return for every two years, but he agreed.\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_firstk_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtime_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time cost'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-7469b74dce47>\u001b[0m in \u001b[0;36manalyze_text\u001b[0;34m(text, masked_tokens, show_suggestions, show_firstk_probs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mremainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m#[CLS]和[SEP]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpart_suggestions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpart_original_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpart_avg_gap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_part_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_suggestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_firstk_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart_suggestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mnew_part_suggestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maccumulate_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart_suggestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-5c7ac4f7849d>\u001b[0m in \u001b[0;36manalyze_part_text\u001b[0;34m(text, masked_tokens, show_suggestions, show_firstk_probs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtop_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_lm_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlm_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirstk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_firstk_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#传入的probs是二维的\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgiven_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mavg_gap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_abnormals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmlm_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_suggestions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_suggestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_gap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-544dddf2eb6b>\u001b[0m in \u001b[0;36mshow_abnormals\u001b[0;34m(tokens, probs, text, show_suggestions)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"NN\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#名词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0msuggestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyse_N\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"CC\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#连词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-2a78fad9660d>\u001b[0m in \u001b[0;36manalyse_N\u001b[0;34m(index, prob, gap, top_word, threshold, threshold2, threshold3)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mlist_IN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"of\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'to'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"at\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"in\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"on\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"by\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"for\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"from\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"with\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"about\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"against\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"along\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"among\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"around\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"as\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"before\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"behind\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"below\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"beside\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"between\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"during\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"besides\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"into\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"near\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"over\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"through\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"under\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"without\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"after\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"above\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mlist_DT_IN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_DT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist_IN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0msuggestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneed_DT_IN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgive_suggestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid_in_sen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_DT_IN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mneed_DT_IN\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#不需要冠词或介词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgap\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#没有可以替换的词，而且原本该位置的词就勉强符合要求\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-6ca675973e35>\u001b[0m in \u001b[0;36mgive_suggestion\u001b[0;34m(input_ids_, input_type_ids_, id_in_sen, alternative_word, threshold)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mT_input_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT_input_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmlm_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_input_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmlm_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlm_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mreduced_mlm_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlm_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_in_sen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# text = [\"Who was Jim Henson? Jim Henson _ a puppeteer.\"]\n",
    "#text = [\"Last week I went to the theater. There are many person . Luckily , I had very good seat. The plays was very interesting. However, I didn't enjoy it. A young man and a young woman were sitting behind me. They were talk loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angry. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "#text = [\"He is my friend.\"]\n",
    "text = [\"When I was little, Friday's night was our family game night. After supper, we would play card games of all sort in the sitting room. As the kid, I loved to watch cartoons,but no matter how many times I asked for watching them, my parents would not to let me.They would say to us that playing card games would help my brain. Still I unwilling to play the games for them sometimes. \"]\n",
    "\n",
    "#text = [\"Last week I went to the theater. I had very good seat. The plays was very interesting. However, I didn't enjoy it. A young man and a young woman were sitting behind me. They were talk loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angry. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "# text = [\"After the outbreak of the disease, the Ministry of Agriculture and rural areas immediately sent a supervision team to the local. Local Emergency Response Mechanism has been activated in accordance with the requirements, to take blockade, culling, harmless treatment, disinfection and other treatment measures to all disease and culling of pigs for harmless treatment. At the same time, all live pigs and their products are prohibited from transferring out of the blockade area, and live pigs are not allowed to be transported into the blockade area. At present, all the above measures have been implemented.\"]\n",
    "# text = [\"Early critics of Emily Dickinson's poetry mistook for simplemindedness the surface of artlessness that in fact she constructed with such innocence.\"]\n",
    "#text = [\"The journey was long and tired. We left London at five o'clock in the evening and spend eight hours in the train. We had been travelled for 3 hours after someone appeared selling food and drinks. It was darkness all the time we were crossing Wales, but we could see nothing through the windows. When we finally arrived Holyhead nearly , everyone was sleeping. As soon as the train stopped, everybody come to life, grabbing their suitcases and rushing onto the platform.\"]\n",
    "#text = [\"When I was little, Friday's night was our family game night. After supper, we would play card games of all sort in the sitting room. As the kid, I loved to watch cartoons，but no matter how many times I asked to watching them， my parents would not to let me. They would say to us that playing card games would help my brain. Still I unwilling to play the games for them sometimes. I didn't realize how right my parents are until I entered high school. The games my parents taught me where I was a child turned out to be very useful later in my life.\"]\n",
    "#text = [\"Mr. and Mrs.Zhang all work in our school. They live far from the school, and it takes them about a hour and a half to go to work every day. In their spare time, they are interesting in planting vegetables in their garden, that is on the rooftop of their house. They often get up earlier and water the vegetables together. They have also bought for some gardening tools.beside, they often get some useful informations from the internet. When summer came, they will invite their students pick the vegetables！\"]\n",
    "#text = ['The question is more easy than that.']\n",
    "#text = [\"Last week I go to the zoo. I had a very good seat. The play was very interesting.\"]\n",
    "#text =[\"Last week I went to the theater. I had very good seat. The play was very interesting.But I didn't enjoy it. A young man and a young woman were sitting behind me.They were talking loudly. I got very angry.\"]#因为外面有中括号，所以是二维的\n",
    "#text = ['It was Monday morning, and the writeing class had just begin.We were tiring. Everyone was silent, wait to see who would be called upon to read his and her paragraph aloud. Some of us were confidont and eagerly take part in the class activity, others were nervous and anxious. I had done myself homework but I was shy. I was afraid that to speak in front of a larger group of people. At that moment, I remembered that my father once said, \"The classroom is a place for learning and that include leaning from textbooks, and mistake as well.\" Immediate, I raised my hand.']\n",
    "#text = ['During my last winter holiday, I went to countryside with my father to visit my grandparents. I find a big change there. The first time I went there, they were living in a small house with dogs, ducks, and another animals. Last winter when I went here again, they had a big separate house to raise dozens of chicken. They also had a small pond which they raised fish. My grandpa said last summer they earned quite a lot by sell the fish. I felt happily that their life had improved. At the end of our trip，I told my father that I planned to return for every two years, but he agreed.']\n",
    "#text = [\"what is justice ? what is good ? what kind of life is a happy life ? how can a justice ' s life benefit human beings ? is it certain that a justice ' s life must lead to happiness ? these problems have already been questioned thousands and hundreds years . they will continue to be questioned . this dissertation tries to discuss the connection between the city - state and the citizen . in the first and the second part of the dissertation the writer tries to make it clear what city - state , citizen and justice mean in the republic . plato ' s idea theory is explained in the third part . and how can his idea theory apply to the education system of the city - state and the happiness of the citizens . the fourth part reviewed old education system and educators , which includes poets and wise men . the poets are criticized for their negative effects to the youth . the fifth part is the education lawmaking of the ideal city - state , together with education means and education principle . the sixth and the seventh parts explain how can the city - state educate qualified soldiers and philosophers . they receive the same nation educate at first which is poetry educatdion and athletics education . some excellent soldiers go into higher category by the selection . they will receive philosopher ' s education , studying some specified subjects . then it makes a conclusion that the education is the only means to attain an ideal city - state .\"]\n",
    "#text = ['The head of state immunity principle is an ancient principle of customary international law. Diplomatic privileges and immunity, monarchy personal exemption, and state immunity theory has a close connection. By analyzing the interrelation of the three concepts ,them are closely related.and has important effects on the head of state immunity principle.The head of state immunity in criminal is also a widespread international recognition. However, from the beginning of the last century, with the development of international criminal law, the principle has been impacted by the international criminal law. Because the punishments by international criminal institutions, and the individual criminal responsibility shall be investigated for. And the head of state is particular, the implementation of the international crimes is different with general international crime.So,it’s cause some controversial issue. In the part two,according to discusses the main cases about the head of state.After the world war II.We can known that although the practices of international criminal justice institution repeatedly emerge the judgment of the head of state.but,the principls as such as \"official identity independence\" and \"individual criminal responsibility\" emphasize the criminal responsibility of the head of state.Seems the criminal jurisdiction of heads of state immunity can no longer competed the criminal responsibility. But in fact, there still not an common answer to solve the debate. The part three summarizes the reasons of the conflict and description the heads of state immunity is necessity. In the new international situation it’s necessary to reserve the head of state immunity in the criminal rationally. And find some ways to solved this contradiction from the standpoint of draft norm of international law. For example the international community should be improving the international force law norms.']\n",
    "#text = [\"During my last winter holiday, I went to countryside with my father to visit my grandparents. I find a big change there. The first time I went there, they were living in a small house with dogs, ducks, and another animals. Last winter when I went here again, they had a big separate house to raise dozens of chicken. They also had a small pond which they raised fish. My grandpa said last summer they earned quite a lot by sell the fish. I felt happily that their life had improved. At the end of our trip，I told my father that I planned to return for every two years, but he agreed.\"]\n",
    "time_start=time.time()\n",
    "analyze_text(text, show_firstk_probs=500)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    功能：对suggestions进行修改，由于某处位置改变造成suggestions后面的错误位置都相应移动\n",
    "    输入：\n",
    "        index：开始移动的位置\n",
    "        direction：移动的方向，1表示向右边移，-1表示向左边移\n",
    "'''\n",
    "def modify_suggestions(index,direction):\n",
    "    global suggestions\n",
    "    new_suggestions = {};\n",
    "    if direction == 0:\n",
    "        pass\n",
    "    elif direction == 1:\n",
    "        for key in suggestions:\n",
    "            if key < index:\n",
    "                new_suggestions.update({key:suggestions[key]})\n",
    "            else:\n",
    "                new_suggestions.update({key+1:suggestions[key]})\n",
    "    elif direction == -1:\n",
    "        for key in suggestions:\n",
    "            if key < index:\n",
    "                new_suggestions.update({key:suggestions[key]})\n",
    "            else:\n",
    "                new_suggestions.update({key-1:suggestions[key]})       \n",
    "    suggestions = new_suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    功能：\\n        修改文本，tokens，suggestions\\n    输入：\\n        index：修改的位置\\n        text：被修改前的原文\\n    输出：\\n        [text]：修改后的文本\\n        new_tokens：修改后的新tokens\\n        suggestions：修改后新的建议字典\\n'"
      ]
     },
     "execution_count": 1717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(suggestions)\n",
    "def display_suggestion():\n",
    "    print(\"**********************************display_suggestions********************************************************\")\n",
    "    print(\"| {:50} : {}\".format(\"suggestion\",\"position in text\"))\n",
    "    print(\"---------------------------------------------------------------------------------------\")\n",
    "    for key in suggestions:\n",
    "        print(\"| {:<50} : {}\".format(suggestions[key] ,key))\n",
    "    print(\"*************************************************************************************************************\")\n",
    "#display_suggestion()\n",
    "\n",
    "'''\n",
    "    功能：\n",
    "        修改文本，tokens，suggestions\n",
    "    输入：\n",
    "        index：修改的位置\n",
    "        text：被修改前的原文\n",
    "    输出：\n",
    "        [text]：修改后的文本\n",
    "        new_tokens：修改后的新tokens\n",
    "        suggestions：修改后新的建议字典\n",
    "'''\n",
    "def modify_text(index,text): #修改文本，tokens，以及suggestions\n",
    "    global suggestions,original_tokens\n",
    "    tokens = original_tokens\n",
    "    new_text = \"\"\n",
    "    suggestion = suggestions[index]\n",
    "    del(suggestions[index])\n",
    "    suggestion_tokens = suggestion.split(\" \")\n",
    "    #print(suggestion_tokens)\n",
    "    if '去掉前面' == suggestion_tokens[0]:\n",
    "        del tokens[index - 1]\n",
    "        del suggestion_tokens[0]\n",
    "        del suggestion_tokens[0]\n",
    "        modify_suggestions(index,-1)\n",
    "        index = index - 1\n",
    "    elif '去掉后面' == suggestion_tokens[0]:\n",
    "        del tokens[index + 1]\n",
    "        del suggestion_tokens[0]\n",
    "        del suggestion_tokens[0]\n",
    "        modify_suggestions(index+2,-1)\n",
    "    elif '去掉' == suggestion_tokens[0]:\n",
    "        del tokens[index]\n",
    "        del suggestion_tokens[0]\n",
    "        del suggestion_tokens[0]\n",
    "        modify_suggestions(index+1,-1)\n",
    "    if '原位置改成' in suggestion_tokens:\n",
    "        del suggestion_tokens[0]\n",
    "        \n",
    "        \n",
    "    len_suggest = len(suggestion_tokens)\n",
    "    if len_suggest == 1:\n",
    "        tokens[index] = suggestion_tokens[0]\n",
    "    elif len_suggest == 2:\n",
    "        tokens.insert(index,suggestion_tokens[0])\n",
    "        tokens[index + 1] = suggestion_tokens[1]\n",
    "        modify_suggestions(index+1,1)\n",
    "    final_len = len(tokens)\n",
    "\n",
    "    for i in range(1,len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        if word[0:2] == \"##\":\n",
    "            new_text = new_text + word[2:]\n",
    "        else:\n",
    "            new_text = new_text + ' ' + word\n",
    "            \n",
    "    original_tokens = tokens\n",
    "    return [text],tokens,suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook likunlin_final.ipynb to python\n",
      "[NbConvertApp] Writing 79979 bytes to likunlin_final.py\n"
     ]
    }
   ],
   "source": [
    "#变成py文件\n",
    "try:\n",
    "    !jupyter nbconvert --to python likunlin_final.ipynb\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
