{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import tokenization, BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, BertConfig\n",
    "from examples.extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2019 15:48:11 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/xd/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/14/2019 15:48:12 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/\n",
      "05/14/2019 15:48:12 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "args = Args()\n",
    "args.no_cuda = True\n",
    "\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "BERT_DIR = '/nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/'\n",
    "config_file = os.path.join(BERT_DIR, CONFIG_NAME)\n",
    "config = BertConfig.from_json_file(config_file)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')#do_lower_case：在标记化时将文本转换为小写。默认= True\n",
    "model = BertForPreTraining.from_pretrained(BERT_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2331, 2351, 2757, 3280, 5996, 8289]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['death','died','dead','die','dying','dies']\n",
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertForPreTraining：\n",
    "Outputs:\n",
    "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
    "            sentence classification loss.\n",
    "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
    "            Outputs a tuple comprising\n",
    "            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n",
    "            - the next sentence classification logits of shape [batch_size, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from_pretrained：\n",
    "Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
    "Download and cache the pre-trained model file if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2019 15:48:15 - INFO - examples.extract_features -   tokens: [CLS] i love you [SEP]\n",
      "05/14/2019 15:48:15 - INFO - examples.extract_features -   tokens: [CLS] hello everybody [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'you', '[SEP]']\n",
      "[101, 1045, 2293, 2017, 102]\n",
      "['[CLS]', 'hello', 'everybody', '[SEP]']\n",
      "[101, 7592, 7955, 102]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def convert_text_to_examples(text): #把每一行的句子变成一个实例，一个实例中包含text_a,text_b(text_b目前是没用的)\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    if True:\n",
    "        for line in text:\n",
    "            line = line.strip()\n",
    "            text_a = None\n",
    "            text_b = None\n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line) #想要匹配这样的字符串'You are my sunshine. ||| I love you.'\n",
    "            \n",
    "            if m is None:\n",
    "                text_a = line\n",
    "            else:\n",
    "                text_a = m.group(1) #匹配的第一句,比如You are my sunshine,my only sunshine.\n",
    "                text_b = m.group(2) #匹配的第二句，比如I love you.\n",
    "            \n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            unique_id += 1\n",
    "    return examples\n",
    "#疑问，当text是一行的时候，line是一个个字母 -> text是[\"***\"]的形式\n",
    "#print(convert_text_to_examples({\"I love you\",\"hello everybody\"})[0].text_a)\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, append_special_tokens=True, replace_mask=True, print_info=False):\n",
    "    #把实例变成一个特征\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a) #tokenizer的作用是\n",
    "        #print(example.unique_id) #*****************************\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        tokens = []\n",
    "        input_type_ids = [] #segment embedding\n",
    "        if append_special_tokens: #输入参数中默认为true\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            if replace_mask and token == '_':  # XD\n",
    "                token = \"[MASK]\"\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                if replace_mask and token == '_':  # XD\n",
    "                    token = \"[MASK]\"\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(1)\n",
    "            if append_special_tokens:\n",
    "                tokens.append(\"[SEP]\")\n",
    "                input_type_ids.append(1)\n",
    "        print(tokens) #*******************************\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) #把原来句子中的词语编成在字典中的编号\n",
    "        input_mask = [1] * len(input_ids) \n",
    "        print(input_ids)#***********************************\n",
    "        if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "#             logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\n",
    "#                 \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=example.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,#字符串中的每个单词在词典中的index序列\n",
    "                input_mask=input_mask, #一堆1\n",
    "                input_type_ids=input_type_ids)) #第0类和第一类，对text_a,text_b的区分\n",
    "    return features\n",
    "                \n",
    "examples = convert_text_to_examples({\"I love you\",\"hello everybody\"})\n",
    "features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "\n",
    "def copy_and_mask_feature(feature, masked_tokens=None):\n",
    "    import copy\n",
    "    tokens = feature.tokens\n",
    "    masked_positions = [tokens.index(t) for t in masked_tokens if t in tokens] \\\n",
    "        if masked_tokens is not None else range(len(tokens))\n",
    "    \n",
    "    assert len(masked_positions) > 0\n",
    "    masked_feature_copies = []\n",
    "    for masked_pos in masked_positions: #用[mask]依次掩盖每一个位置\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        feature_copy.input_ids[masked_pos] = tokenizer.vocab[\"[MASK]\"]\n",
    "        masked_feature_copies.append(feature_copy)\n",
    "    return masked_feature_copies, masked_positions\n",
    "\n",
    "#masked_feature_copies, masked_positions = copy_and_mask_feature(features[1])\n",
    "#print(masked_feature_copies[0].input_ids) #结果[101, 1045, 2293, 103, 102]\n",
    "#print(masked_positions) #结果是一个range(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lm_probs(tokens, input_ids, probs, topk=5, firstk=20): #输出结果的函数，要最高概率topk个输出\n",
    "    def print_pair(token, prob, end_str='', hit_mark=' '):\n",
    "        if i < firstk:\n",
    "            # token = token.replace('</w>', '').replace('\\n', '/n')\n",
    "            print('{}{: >3} | {: <12}'.format(hit_mark, int(round(prob*100)), token), end=end_str)\n",
    "    \n",
    "    ret = None\n",
    "    for i in range(len(tokens)):\n",
    "        ind_ = input_ids[i].item() if input_ids is not None else tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item() #这个probs是该字符串第i个位置上填上词典上各个词的概率\n",
    "        print_pair(tokens[i], prob_, end_str='\\t')\n",
    "        values, indices = probs[i].topk(topk)\n",
    "        top_pairs = []\n",
    "        for j in range(topk):\n",
    "            ind, prob = indices[j].item(), values[j].item()\n",
    "            hit_mark = '*' if ind == ind_ else ' '\n",
    "            token = tokenizer.ids_to_tokens[ind]\n",
    "            print_pair(token, prob, hit_mark=hit_mark, end_str='' if j < topk - 1 else '\\n')\n",
    "            top_pairs.append((token, prob))\n",
    "        if tokens[i] == \"[MASK]\":\n",
    "            ret = top_pairs\n",
    "    return ret #返回的这是个啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "def show_abnormals(tokens, probs, show_suggestions=False):\n",
    "    def gap2color(gap):\n",
    "        if gap <= 5:\n",
    "            return 'yellow_1'\n",
    "        elif gap <= 10:\n",
    "            return 'orange_1'\n",
    "        else:\n",
    "            return 'red_1'\n",
    "        \n",
    "    def print_token(token, suggestion, gap):\n",
    "        \n",
    "        if gap == 0:\n",
    "            print(stylize(token + ' ', colored.fg('white') + colored.bg('black')), end='')\n",
    "        else:\n",
    "            print(stylize(token, colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "            if show_suggestions and gap > 5:\n",
    "                print(stylize('/' + suggestion + ' ', colored.fg('green' if gap > 10 else 'cyan') + colored.bg('black')), end='')\n",
    "            else:\n",
    "                print(stylize(' ', colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "                \n",
    "                # print('/' + suggestion, end=' ')\n",
    "            # print('%.2f' % gap, end=' ')\n",
    "        #print(gap)\n",
    "    avg_gap = 0.\n",
    "    for i in range(1, len(tokens) - 1):  # skip first [CLS] and last [SEP]\n",
    "        ind_ = tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item()\n",
    "        top_prob = probs[i].max().item()\n",
    "        top_ind = probs[i].argmax().item()\n",
    "        gap = math.log(top_prob) - math.log(prob_) #计算两个词之间的差距\n",
    "        #print(top_prob,prob_)\n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        print_token(tokens[i], suggestion, gap)\n",
    "        avg_gap += gap\n",
    "    avg_gap /= (len(tokens) - 2)\n",
    "    print()\n",
    "    print(avg_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_cache = {}\n",
    "\n",
    "def analyze_text(text, masked_tokens=None, show_suggestions=True, show_firstk_probs=20):\n",
    "    if text[0] in analyzed_cache: #分析过的缓存\n",
    "        features, mlm_probs = analyzed_cache[text[0]]\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "        tokens = features[0].tokens \n",
    "    else:\n",
    "        examples = convert_text_to_examples(text)\n",
    "        features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "        if not given_mask or masked_tokens is not None:\n",
    "            assert len(features) == 1\n",
    "            features, masked_positions = copy_and_mask_feature(features[0], masked_tokens=masked_tokens)\n",
    "\n",
    "        input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long) #把input_ids增加了一个维度，变成[n_features,sequence_len]\n",
    "        #这里的n_features实际上是句子有多少个单词位置，每个位置依次换成[mask]\n",
    "        input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long) #把input_type_ids增加了一个维度，其实每一行都一样\n",
    "        input_ids = input_ids.to(device) #拿去GPU\n",
    "        input_type_ids = input_type_ids.to(device)\n",
    "        \n",
    "        time_start=time.time()\n",
    "        mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "        time_end=time.time()\n",
    "        print('time cost1',time_end-time_start,'s')\n",
    "        \n",
    "        mlm_probs = F.softmax(mlm_logits, dim=-1) #最后一维，也就是vocab 换算成概率和为百分之百\n",
    "        #print(mlm_probs.size())#这里实验的是torch.Size([5, 5, 30522])\n",
    "        tokens = features[0].tokens #不知道要干嘛\n",
    "        if not given_mask or masked_tokens is not None:\n",
    "            bsz, seq_len, vocab_size = mlm_probs.size() #三个维度分别是batch_size, sequence_length, vocab_size\n",
    "            assert bsz == len(masked_positions)\n",
    "            # reduced_mlm_probs = torch.Tensor(1, seq_len, vocab_size)\n",
    "            # for i in range(seq_len):\n",
    "            #    reduced_mlm_probs[0, i] = mlm_probs[i, i]\n",
    "            reduced_mlm_probs = torch.Tensor(1, len(masked_positions), vocab_size)\n",
    "            for i, pos in enumerate(masked_positions):\n",
    "                reduced_mlm_probs[0, i] = mlm_probs[i, pos]\n",
    "            mlm_probs = reduced_mlm_probs #压缩一下大小，节约不必要浪费的空间（只需要第i个batch里面[mask]位置的词汇表概率即可）\n",
    "            tokens = [tokens[i] for i in masked_positions]\n",
    "        \n",
    "        analyzed_cache[text[0]] = (features, mlm_probs)\n",
    "        \n",
    "    top_pairs = show_lm_probs(tokens, None, mlm_probs[0], firstk=show_firstk_probs) #传入的probs是二维的\n",
    "    #print(\"************************************************************************************************************\")\n",
    "    #print(top_pairs) #******************************\n",
    "    if not given_mask:\n",
    "        show_abnormals(tokens, mlm_probs[0], show_suggestions=show_suggestions)\n",
    "    return top_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/21/2019 16:22:56 - INFO - examples.extract_features -   tokens: [CLS] he is dies . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'he', 'is', 'dies', '.', '[SEP]']\n",
      "[101, 2002, 2003, 8289, 1012, 102]\n",
      "time cost1 0.0779261589050293 s\n",
      "   0 | [CLS]       \t   4 | .              1 | ,              1 | the            1 | )              1 | \"           \n",
      "  19 | he          \t* 19 | he             8 | it             6 | she            3 | and            2 | the         \n",
      "   0 | is          \t  33 | then          15 | soon          12 | eventually     7 | later          4 | also        \n",
      "   0 | dies        \t   4 | dead           3 | alive          3 | right          2 | beautiful      2 | not         \n",
      "  93 | .           \t* 93 | .              6 | ;              1 | !              0 | ?              0 | |           \n",
      "   0 | [SEP]       \t  11 | \"              5 | he             2 | .              1 | and            1 | it          \n",
      "\u001b[38;5;15m\u001b[48;5;0mhe \u001b[0m\u001b[38;5;214m\u001b[48;5;0mis\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/then \u001b[0m\u001b[38;5;214m\u001b[48;5;0mdies\u001b[0m\u001b[38;5;6m\u001b[48;5;0m/dead \u001b[0m\u001b[38;5;15m\u001b[48;5;0m. \u001b[0m\n",
      "3.6602350262062977\n",
      "time cost 0.0883021354675293 s\n"
     ]
    }
   ],
   "source": [
    "# text = [\"Who was Jim Henson? Jim Henson _ a puppeteer.\"]\n",
    "#text = [\"Last week I went to the theatre. I had very good seat. The play was very interesting. But I didn't enjoy it. A young man and a young woman were sitting behind me. They were talking loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angry. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "#text = [\"After the outbreak of the disease, the Ministry of Agriculture and rural areas immediately sent a supervision team to the local. Local Emergency Response Mechanism has been activated in accordance with the requirements, to take blockade, culling, harmless treatment, disinfection and other treatment measures to all disease and culling of pigs for harmless treatment. At the same time, all live pigs and their products are prohibited from transferring out of the blockade area, and live pigs are not allowed to be transported into the blockade area. At present, all the above measures have been implemented.\"]\n",
    "#text = [\"The journey was long and tired. We left London at five o'clock in the evening and spend eight hours in the train. We had been travelled for 3 hours after someone appeared selling food and drinks. It was darkness all the time we were crossing Wales, but we could see nothing through the windows. When we finally arrived at Holyhead nearly , everyone was slept. As soon as the train stopped, everybody come to life, grabbing their suitcases and rushing onto the platform.\"]\n",
    "#text = [\"When I was little, Friday's night was our family game night. After supper, we would play card games of all sort in the sitting room. As the kid, I loved to watch cartoons,but no matter how many times I asked for watching them, my parents would not to let me.They would say to us that playing card games would help my brain. Still I unwilling to play the games for them sometimes. \"]\n",
    "#text = [\"After the outbreak of the disease, the Ministry of Agriculture and rural areas immediately sent a supervision team to the local. Local Emergency Response Mechanism has been activated in accordance with the requirements, to take blockade, culling, harmless treatment, disinfection and other treatment measures to all disease and culling of pigs for harmless treatment. At the same time, all live pigs and their products are prohibited from transferring out of the blockade area, and live pigs are not allowed to be transported into the blockade area. At present, all the above measures have been implemented.\"]\n",
    "# text = [\"Early critics of Emily Dickinson's poetry mistook for simplemindedness the surface of artlessness that in fact she constructed with such innocence.\"]\n",
    "#text = [\"During my last winter holiday, I went to the countryside with my father to visit my grandparents. I find a big change there. The first time I went there, they were living in a small house with dogs, ducks, and another animals. Last winter when I went here again, they had a big separate house to raise dozens of chicken. They also had a small pond which they raised fish. My grandpa said last summer they earned quite a lot by sell the fish. I felt happily that their life had improved. At the end of our trip，I told my father that I planned to return for every two years, but he agreed.\"]\n",
    "# text = ['The problem is difficult than that one.']\n",
    "#text = [\"It was Monday morning, and the writing class had just begin. Everyone was silent, wait to see who would be called upon to read his and her paragraph aloud. Some of us were confident and eagerly take part in the class activity, others were nervous and anxious. I had done myself homework but I was shy. I was afraid that to speak in front of a larger group of people. At that moment, I remembered that my father once said, 'The classroom is a place for learning and that include learning from the textbooks, and mistakes as well.' Immediate, I raised my hand.\"]\n",
    "text = [\"He is dies.\"]\n",
    "import time\n",
    "time_start=time.time()\n",
    "#text = [\"The play was very interesting.\"]\n",
    "#text = [\"The question is easy than that one.\"]\n",
    "#text =[\"The apple a eat by me. I had a very good seat. The play was very interesting.But I didn't enjoy it. A young man and a young woman were sitting behind me.They were talking loudly. I got very angry.\"]#因为外面有中括号，所以是二维的\n",
    "analyze_text(text, show_firstk_probs=200)\n",
    "#print(analyzed_cache)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.',\n",
       " 'Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    # same / different\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.\",\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.\",\n",
    "    \"Tom has yellow hair. Mary has black hair. John has black hair. Mary and _ have the same hair color.\",\n",
    "    # because / although\n",
    "    \"John is taller/shorter than Mary because/although _ is older/younger.\",\n",
    "    \"The red ball is heavier/lighter than the blue ball because/although the _ ball is bigger/smaller.\",\n",
    "    \"Charles did a lot better/worse than his good friend Nancy on the test because/although _ had/hadn't studied so hard.\",\n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thought that he would arrive earlier than Susan, but/and indeed _ was the first to arrive.\",\n",
    "    # reverse\n",
    "    \"John came then Mary came. They left in reverse order. _ left then _ left.\",\n",
    "    \"John came after Mary. They left in reverse order. _ left after _ .\",\n",
    "    \"John came first, then came Mary. They left in reverse order: _ left first, then left _ .\",\n",
    "    # compare\n",
    "    \"Though John is tall, Tom is taller than John. So John is _ than Tom.\",\n",
    "    \"Tom is taller than John. So _ is shorter than _.\",\n",
    "    # WSC-style: before /after\n",
    "    \"Mary came before/after John. _ was late/early .\",\n",
    "    # yes / no\n",
    "    \"Was Tom taller than Susan? Yes, _ was taller.\",\n",
    "    # right / wrong, epistemic modality\n",
    "    \"John said the rain was about to stop. Mary said the rain would continue. Later the rain stopped. _ was wrong.\",\n",
    "    \n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thanked Mary because  _ had given help to _ . \",\n",
    "    \"John felt vindicated/crushed when his longtime rival Mary revealed that _ was the winner of the competition.\",\n",
    "    \"John couldn't see the stage with Mary in front of him because _ is so short/tall.\",\n",
    "    \"Although they ran at about the same speed, John beat Sally because _ had such a bad start.\",\n",
    "    \"The fish ate the worm. The _ was hungry/tasty.\",\n",
    "    \n",
    "    \"John beat Mary. _ won the game/e winner.\",\n",
    "]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_switched_label.json') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WSC_child_problem.json') as f:\n",
    "    cexamples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    for s in ce['sentences']:\n",
    "        for a in s['answer0'] + s['answer1']:\n",
    "            a = a.lower()\n",
    "            if a not in tokenizer.vocab:\n",
    "                ce\n",
    "                print(a, 'not in vocab!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ce in cexamples:\n",
    "    if len(ce['sentences']) > 0:\n",
    "        e = examples[ce['index']]\n",
    "        assert ce['index'] == e['index']\n",
    "        e['score'] = all([s['score'] for s in ce['sentences']])\n",
    "        assert len(set([s['adjacent_ref'] for s in ce['sentences']])) == 1, 'adjcent_refs are different!'\n",
    "        e['adjacent_ref'] = ce['sentences'][0]['adjacent_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for e in examples:\n",
    "    if 'score' in e:\n",
    "        index = e['index']\n",
    "        if index < 252:\n",
    "            if index % 2 == 1:\n",
    "                index -= 1\n",
    "        elif index in [252, 253, 254]:\n",
    "            index = 252\n",
    "        else:\n",
    "            if index % 2 == 0:\n",
    "                index -= 1\n",
    "        groups[index].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'fit into:large/small', False),\n",
       " (4, 'thank:receive/give', False),\n",
       " (6, 'call:successful available', True),\n",
       " (8, 'ask:repeat answer', False),\n",
       " (10, 'zoom by:fast/slow', False),\n",
       " (12, 'vindicated/crushed:be the winner', False),\n",
       " (14, 'lift:weak heavy', False),\n",
       " (16, 'crash through:[hard]/[soft]', False),\n",
       " (18, '[block]:short/tall', False),\n",
       " (20, 'down to:top/bottom', False),\n",
       " (22, 'beat:good/bad', False),\n",
       " (24, 'roll off:anchored level', False),\n",
       " (26, 'above/below', False),\n",
       " (28, 'better/worse:study hard', False),\n",
       " (30, 'after/before:far away', False),\n",
       " (32, 'be upset with:buy from not work/sell not work', True),\n",
       " (34, '?yell at comfort:upset', False),\n",
       " (36, 'above/below:moved first', False),\n",
       " (38, 'although/because', False),\n",
       " (40, 'bully:punish rescue', False),\n",
       " (42, 'pour:empty/full', False),\n",
       " (44, 'know:nosy indiscreet', False),\n",
       " (46, 'explain:convince/understand', True),\n",
       " (48, '?know tell:so/because', True),\n",
       " (50, 'beat:younger/older', False),\n",
       " (56, 'clog:cleaned removed', True),\n",
       " (58, '?immediately follow:short delayed', False),\n",
       " (60, '?between:see see around', True),\n",
       " (64, 'but/and', False),\n",
       " (66, 'clean:put in the trash put in the drawer', False),\n",
       " (68, 'because/but', False),\n",
       " (70, 'out of:handy lighter', False),\n",
       " (72, 'put:tall high', False),\n",
       " (74, 'show:good famous', True),\n",
       " (76, 'pay for:generous grateful', False),\n",
       " (78, 'but', False),\n",
       " (80, 'if', False),\n",
       " (82, 'if', False),\n",
       " (84, 'fool:get/lose', False),\n",
       " (88, 'wait:impatient cautious', False),\n",
       " (90, 'give birth:woman baby', True),\n",
       " (92, '?stop normal/stop abnormal:strange', False),\n",
       " (96, 'eat:hungry tasty', False),\n",
       " (98, 'put ... into filled with ... :get in/get out', False),\n",
       " (100, 'up:at the bottom/at the top', False),\n",
       " (102, 'crash through:removed repaired', False),\n",
       " (104, 'stab:taken to the police station taken to the hospital', False),\n",
       " (106, 'hear ... humming and whistling:annoyed/annoying', True),\n",
       " (108, 'see ... juggling watermelons:impressed/impressive', True),\n",
       " (114, 'tell lies: truthful skeptical', True),\n",
       " (130, 'but:disappointed', True),\n",
       " (132, 'visit:invite come out/invite come in', True),\n",
       " (134, 'take classes from:eager known to speak it fluently', False),\n",
       " (138, 'cover:out gone', True),\n",
       " (144, 'tuck:work sleep', True),\n",
       " (150, 'influence:later/earlier', False),\n",
       " (152, 'can not cut:thick small', False),\n",
       " (154, 'attack:kill guard', False),\n",
       " (156, 'attack:bold nervous', False),\n",
       " (160, 'change:hard:easy', False),\n",
       " (166, 'alive:is/was', False),\n",
       " (168, 'infant:twelve years old twelve months old', False),\n",
       " (170, 'better equipped and large:defeated/victorious', False),\n",
       " (178, 'interview:persistent cooperative', False),\n",
       " (186, 'be full of:minority/majority', False),\n",
       " (188, 'like over:more/fewer', False),\n",
       " (190, 'place on all:not enough/too many', True),\n",
       " (192, 'stick:leave have', True),\n",
       " (196, 'follow:admire/influence', True),\n",
       " (198, 'fit through:wide/narrow', False),\n",
       " (200, 'trade:dowdy/great', False),\n",
       " (202, 'hire/hire oneself to:take care of', True),\n",
       " (204, 'promise/order', False),\n",
       " (208, 'mother:education place', True),\n",
       " (210, 'knock:get an answer/answer', True),\n",
       " (212, 'pay:receive/deliver', False),\n",
       " (218, '?', False),\n",
       " (220, 'say check:move take', False),\n",
       " (222, '?', False),\n",
       " (224, 'give a life:drive alone walk', False),\n",
       " (226, 'pass the plate:full/hungry', False),\n",
       " (228, 'pass:turn over turn next', False),\n",
       " (232, 'stretch pat', True),\n",
       " (234, 'accept share', False),\n",
       " (236, 'speak:break silence break concentration', False),\n",
       " (240, 'carry:leg ache leg dangle', True),\n",
       " (242, 'carry:in arms in bassinet', False),\n",
       " (244, 'hold:against chest against will', True),\n",
       " (250, 'stop', False),\n",
       " (252, 'even though/because/not', False),\n",
       " (255, 'give:not hungry/hungry', False),\n",
       " (259, 'ask for a favor:refuse/be refused`', False),\n",
       " (261, 'cede:less popular/more popular', False),\n",
       " (263, 'not pass although:see open/open', True),\n",
       " (271, 'suspect regret', True)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_dict(d, keys=['index', 'sentence', 'correct_answer', 'relational_word', 'is_associative', 'score']):\n",
    "    return {k: d[k] for k in d if k in keys}\n",
    "\n",
    "# ([[filter_dict(e) for e in eg] for eg in groups.values() if eg[0]['relational_word'] != 'none' and all([e['score'] for e in eg])])# / len([eg for eg in groups.values() if eg[0]['relational_word'] != 'none'])\n",
    "[(index, eg[0]['relational_word'], all([e['score'] for e in eg])) for index, eg in groups.items() if eg[0]['relational_word'] != 'none']\n",
    "# len([filter_dict(e) for e in examples if 'score' in e and not e['score'] and e['adjacent_ref']])\n",
    "# for e in examples:\n",
    "#     if e['index'] % 2 == 0:\n",
    "#         print(e['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['because' in e['sentence'] for e in examples]) + \\\n",
    "sum(['so ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['but ' in e['sentence'] for e in examples]) + \\\n",
    "sum(['though' in e['sentence'] for e in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('WSC_switched_label.json', 'w') as f:\n",
    "#     json.dump(examples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_attn_topk = 3\n",
    "\n",
    "def has_chinese_label(labels):\n",
    "    labels = [label.split('->')[0].strip() for label in labels]\n",
    "    r = sum([len(label) > 1 for label in labels if label not in ['BOS', 'EOS']]) * 1. / (len(labels) - 1)\n",
    "    return 0 < r < 0.5  # r == 0 means empty query labels used in self attention\n",
    "\n",
    "def _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col, color='b'):\n",
    "    assert len(query_labels) == attn.size(0)\n",
    "    assert len(key_labels) == attn.size(1)\n",
    "\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax2 = ax1.twinx()\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    pos = range(nlabels)\n",
    "    \n",
    "    if 'self' in attn_name and col < ncols - 1:\n",
    "        query_labels = ['' for _ in query_labels]\n",
    "\n",
    "    for ax, labels in [(ax1, key_labels), (ax2, query_labels)]:\n",
    "        ax.set_yticks(pos)\n",
    "        if has_chinese_label(labels):\n",
    "            ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "        else:\n",
    "            ax.set_yticklabels(labels)\n",
    "        ax.set_ylim([nlabels - 1, 0])\n",
    "        ax.tick_params(width=0, labelsize='xx-large')\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "#     mask, attn = filter_attn(attn)\n",
    "    for qi in range(attn.size(0)):\n",
    "#         if not mask[qi]:\n",
    "#             continue\n",
    "#         for ki in range(attn.size(1)):\n",
    "        for ki in attn[qi].topk(vis_attn_topk)[1]:\n",
    "            a = attn[qi, ki]\n",
    "            ax1.plot((-1, 1), (ki, qi), color, alpha=a)\n",
    "#     print(attn.mean(dim=0).topk(5)[0])\n",
    "#     ax1.barh(pos, attn.mean(dim=0).data.cpu().numpy())\n",
    "\n",
    "def plot_layer_attn(result_tuple, attn_name='dec_self_attns', layer=0, heads=None):\n",
    "    hypo, nheads, labels_dict = result_tuple\n",
    "    key_labels, query_labels = labels_dict[attn_name]\n",
    "    if heads is None:\n",
    "        heads = range(nheads)\n",
    "    else:\n",
    "        nheads = len(heads)\n",
    "    \n",
    "    stride = 2 if attn_name == 'dec_enc_attns' else 1\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    rcParams['figure.figsize'] = 20, int(round(nlabels * stride * nheads / 8 * 1.0))\n",
    "    \n",
    "    rows = nheads // ncols * stride\n",
    "    fig, axes = plt.subplots(rows, ncols)\n",
    "    \n",
    "    # for head in range(nheads):\n",
    "    for head_i, head in enumerate(heads):\n",
    "        row, col = head_i * stride // ncols, head_i * stride % ncols\n",
    "        ax1 = axes[row, col]\n",
    "        attn = hypo[attn_name][layer][head]\n",
    "        _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col)\n",
    "        if attn_name == 'dec_enc_attns':\n",
    "            col = col + 1\n",
    "            axes[row, col].axis('off')  # next subfig acts as blank place holder\n",
    "    # plt.suptitle('%s with %d heads, Layer %d' % (attn_name, nheads, layer), fontsize=20)\n",
    "    plt.show()  \n",
    "            \n",
    "ncols = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertSelfAttention' object has no attribute 'attention_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-13903ae0e550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enc_self_attns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-13903ae0e550>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enc_self_attns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mattn_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertSelfAttention' object has no attribute 'attention_probs'"
     ]
    }
   ],
   "source": [
    "attn_name = 'enc_self_attns'\n",
    "hypo = {attn_name: [model.bert.encoder.layer[i].attention.self.attention_probs[0] for i in range(config.num_hidden_layers)]}\n",
    "key_labels = query_labels = tokens\n",
    "labels_dict = {attn_name: (key_labels, query_labels)}\n",
    "result_tuple = (hypo, config.num_attention_heads, labels_dict)\n",
    "plot_layer_attn(result_tuple, attn_name=attn_name, layer=10, heads=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
