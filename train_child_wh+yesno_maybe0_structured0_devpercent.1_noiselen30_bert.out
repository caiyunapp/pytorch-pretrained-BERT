06/09/2019 19:28:36 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:28:36 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:28:36 - INFO - run_child_finetuning -   num_sent = 46080
06/09/2019 19:29:02 - INFO - run_child_finetuning -   num_train_steps = 7776
06/09/2019 19:29:03 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 19:29:03 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 19:29:03 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 19:29:06 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 19:29:08 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 19:29:08 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 19:30:06 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:30:06 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 19:30:06 - INFO - run_child_finetuning -   num_sent = 46080
06/09/2019 19:30:32 - INFO - run_child_finetuning -   num_train_steps = 7776
06/09/2019 19:30:37 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 19:30:37 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 19:30:37 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 19:30:40 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 19:30:45 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 19:30:45 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 19:30:45 - INFO - run_child_finetuning -   Evaluating on valid set...
Epoch:   0%|          | 0/6 [00:00<?, ?it/s]06/09/2019 19:41:36 - INFO - run_child_finetuning -   Epoch 1
06/09/2019 19:41:36 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 19:54:31 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 19:54:31 - INFO - run_child_finetuning -     eval_accuracy = 0.5042438271604939
06/09/2019 19:54:31 - INFO - run_child_finetuning -     eval_loss = 0.71469889415635
06/09/2019 19:54:31 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 19:55:57 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 19:55:57 - INFO - run_child_finetuning -     eval_accuracy = 0.5080295138888888
06/09/2019 19:55:57 - INFO - run_child_finetuning -     eval_loss = 0.7199578020307753
Epoch:  17%|█▋        | 1/6 [25:12<2:06:00, 1512.05s/it]06/09/2019 20:06:51 - INFO - run_child_finetuning -   Epoch 2
06/09/2019 20:06:51 - INFO - run_child_finetuning -   Evaluating on train set...
Traceback (most recent call last):
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/home/xd/projects/pytorch-pretrained-BERT/train_child.py", line 169, in <module>
    validate(model, train_dataset, device)
  File "/home/xd/projects/pytorch-pretrained-BERT/run_child_finetuning.py", line 486, in validate
    tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)
  File "/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py", line 761, in forward
    prediction_scores = self.cls(sequence_output)
  File "/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py", line 393, in forward
    prediction_scores = self.predictions(sequence_output)
  File "/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py", line 383, in forward
    hidden_states = self.decoder(hidden_states) + self.bias
RuntimeError: CUDA error: out of memory

Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.
Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.
Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Namespace(dev_percent=0.1, do_eval=True, do_lower_case=True, do_train=True, eval_batch_size=128, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=6.0, seed=42, train_batch_size=32, warmup_proportion=0.1)
num_sent = 46080 -> 46080
global_step 0, lr = 0.000000
global_step 1000, lr = 0.000026
global_step 2000, lr = 0.000022
06/09/2019 20:21:50 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 20:21:50 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 20:21:50 - INFO - run_child_finetuning -   num_sent = 46080
06/09/2019 20:22:16 - INFO - run_child_finetuning -   num_train_steps = 7776
06/09/2019 20:22:21 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 20:22:21 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 20:22:21 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 20:22:24 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 20:22:26 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 20:22:26 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 20:22:26 - INFO - run_child_finetuning -   Evaluating on valid set...
Epoch:   0%|          | 0/6 [00:00<?, ?it/s]06/09/2019 20:33:23 - INFO - run_child_finetuning -   Epoch 1
06/09/2019 20:33:23 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 20:46:05 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 20:46:05 - INFO - run_child_finetuning -     eval_accuracy = 0.5012056327160493
06/09/2019 20:46:05 - INFO - run_child_finetuning -     eval_loss = 0.776383133398162
06/09/2019 20:46:05 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 20:47:30 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 20:47:30 - INFO - run_child_finetuning -     eval_accuracy = 0.5006510416666666
06/09/2019 20:47:30 - INFO - run_child_finetuning -     eval_loss = 0.7749141405026118
Epoch:  17%|█▋        | 1/6 [25:03<2:05:19, 1503.96s/it]06/09/2019 20:58:27 - INFO - run_child_finetuning -   Epoch 2
06/09/2019 20:58:27 - INFO - run_child_finetuning -   Evaluating on train set...
Traceback (most recent call last):
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/qsj/miniconda3/lib/python3.6/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/home/xd/projects/pytorch-pretrained-BERT/train_child.py", line 169, in <module>
    validate(model, train_dataset, device)
  File "/home/xd/projects/pytorch-pretrained-BERT/run_child_finetuning.py", line 486, in validate
    tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)
  File "/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py", line 761, in forward
    prediction_scores = self.cls(sequence_output)
  File "/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py", line 393, in forward
    prediction_scores = self.predictions(sequence_output)
  File "/home/qsj/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xd/projects/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py", line 383, in forward
    hidden_states = self.decoder(hidden_states) + self.bias
RuntimeError: CUDA error: out of memory

Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.
Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.
Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Namespace(dev_percent=0.1, do_eval=True, do_lower_case=True, do_train=True, eval_batch_size=128, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=6.0, seed=42, train_batch_size=32, warmup_proportion=0.1)
num_sent = 46080 -> 46080
global_step 0, lr = 0.000000
global_step 1000, lr = 0.000026
global_step 2000, lr = 0.000022
06/09/2019 21:10:16 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 21:10:16 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 21:10:16 - INFO - run_child_finetuning -   num_sent = 46080
06/09/2019 21:10:38 - INFO - run_child_finetuning -   num_train_steps = 7776
06/09/2019 21:10:43 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 21:10:43 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 21:10:43 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 21:10:46 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 21:10:48 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 21:10:48 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 21:10:48 - INFO - run_child_finetuning -   Evaluating on valid set...
Epoch:   0%|          | 0/6 [00:00<?, ?it/s]06/09/2019 21:21:44 - INFO - run_child_finetuning -   Epoch 1
06/09/2019 21:21:44 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 21:31:18 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 21:31:18 - INFO - run_child_finetuning -     eval_accuracy = 0.4995659722222222
06/09/2019 21:31:18 - INFO - run_child_finetuning -     eval_loss = 0.7195569552757122
06/09/2019 21:31:18 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 21:32:22 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 21:32:22 - INFO - run_child_finetuning -     eval_accuracy = 0.5060763888888888
06/09/2019 21:32:22 - INFO - run_child_finetuning -     eval_loss = 0.7160747845967611
Epoch:  17%|█▋        | 1/6 [21:34<1:47:51, 1294.27s/it]06/09/2019 21:43:22 - INFO - run_child_finetuning -   Epoch 2
06/09/2019 21:43:22 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 21:53:00 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 21:53:00 - INFO - run_child_finetuning -     eval_accuracy = 0.5131896219135802
06/09/2019 21:53:00 - INFO - run_child_finetuning -     eval_loss = 0.6972262062776236
06/09/2019 21:53:00 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 21:54:04 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 21:54:04 - INFO - run_child_finetuning -     eval_accuracy = 0.4865451388888889
06/09/2019 21:54:04 - INFO - run_child_finetuning -     eval_loss = 0.7029400732782152
Epoch:  33%|███▎      | 2/6 [43:16<1:26:26, 1296.60s/it]06/09/2019 22:05:06 - INFO - run_child_finetuning -   Epoch 3
06/09/2019 22:05:06 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 22:14:37 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 22:14:37 - INFO - run_child_finetuning -     eval_accuracy = 0.5189525462962963
06/09/2019 22:14:37 - INFO - run_child_finetuning -     eval_loss = 0.6920798770439478
06/09/2019 22:14:37 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 22:15:40 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 22:15:40 - INFO - run_child_finetuning -     eval_accuracy = 0.4921875
06/09/2019 22:15:40 - INFO - run_child_finetuning -     eval_loss = 0.6966085980335871
Epoch:  50%|█████     | 3/6 [1:04:52<1:04:49, 1296.53s/it]06/09/2019 22:26:36 - INFO - run_child_finetuning -   Epoch 4
06/09/2019 22:26:36 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 22:36:07 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 22:36:07 - INFO - run_child_finetuning -     eval_accuracy = 0.51171875
06/09/2019 22:36:07 - INFO - run_child_finetuning -     eval_loss = 0.6979008471524274
06/09/2019 22:36:07 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 22:37:11 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 22:37:11 - INFO - run_child_finetuning -     eval_accuracy = 0.5032552083333334
06/09/2019 22:37:11 - INFO - run_child_finetuning -     eval_loss = 0.7015775607691871
Epoch:  67%|██████▋   | 4/6 [1:26:23<43:09, 1294.77s/it]  