06/09/2019 16:59:08 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 16:59:08 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 16:59:13 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 16:59:13 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 16:59:13 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 16:59:16 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 16:59:18 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 16:59:18 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 16:59:18 - INFO - run_child_finetuning -   Evaluating on valid set...
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]06/09/2019 17:00:00 - INFO - run_child_finetuning -   Epoch 1
06/09/2019 17:00:00 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 17:00:34 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:00:34 - INFO - run_child_finetuning -     eval_accuracy = 0.4959635416666667
06/09/2019 17:00:34 - INFO - run_child_finetuning -     eval_loss = 1.074929141998291
06/09/2019 17:00:34 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 17:01:06 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:01:06 - INFO - run_child_finetuning -     eval_accuracy = 0.5040364583333333
06/09/2019 17:01:06 - INFO - run_child_finetuning -     eval_loss = 1.073794287443161
Epoch:  33%|███▎      | 1/3 [01:48<03:36, 108.40s/it]06/09/2019 17:01:48 - INFO - run_child_finetuning -   Epoch 2
06/09/2019 17:01:48 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 17:02:20 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:02:20 - INFO - run_child_finetuning -     eval_accuracy = 0.73203125
06/09/2019 17:02:20 - INFO - run_child_finetuning -     eval_loss = 0.44871779829263686
06/09/2019 17:02:20 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 17:02:53 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:02:53 - INFO - run_child_finetuning -     eval_accuracy = 0.7317708333333334
06/09/2019 17:02:53 - INFO - run_child_finetuning -     eval_loss = 0.4502534622947375
Epoch:  67%|██████▋   | 2/3 [03:35<01:47, 107.88s/it]06/09/2019 17:03:34 - INFO - run_child_finetuning -   Epoch 3
06/09/2019 17:03:34 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 17:04:07 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:04:07 - INFO - run_child_finetuning -     eval_accuracy = 0.810546875
06/09/2019 17:04:07 - INFO - run_child_finetuning -     eval_loss = 0.3317147488395373
06/09/2019 17:04:07 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 17:04:39 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:04:39 - INFO - run_child_finetuning -     eval_accuracy = 0.8126302083333333
06/09/2019 17:04:39 - INFO - run_child_finetuning -     eval_loss = 0.32819243470827736
Epoch: 100%|██████████| 3/3 [05:21<00:00, 107.38s/it]
/home/qsj/miniconda3/bin/python: Error while finding module specification for 'train_child.py' (AttributeError: module 'train_child' has no attribute '__path__')
Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.
Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.
Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.
Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Namespace(dev_percent=0.5, do_eval=True, do_lower_case=True, do_train=True, eval_batch_size=128, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, seed=42, train_batch_size=32, warmup_proportion=0.1)
num_sent = 15360 -> 11520
num_train_steps = 720
global_step 0, lr = 0.000000
06/09/2019 17:05:29 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 17:05:30 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased-vocab.txt
06/09/2019 17:05:38 - INFO - run_child_finetuning -   device: cuda n_gpu: 1
06/09/2019 17:05:38 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-pytorch/bert-base-uncased
06/09/2019 17:05:38 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

06/09/2019 17:05:41 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
06/09/2019 17:05:45 - INFO - run_child_finetuning -   Epoch 0
06/09/2019 17:05:45 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 17:05:45 - INFO - run_child_finetuning -   Evaluating on valid set...
Epoch:   0%|          | 0/6 [00:00<?, ?it/s]06/09/2019 17:06:26 - INFO - run_child_finetuning -   Epoch 1
06/09/2019 17:06:26 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 17:06:58 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:06:58 - INFO - run_child_finetuning -     eval_accuracy = 0.4962239583333333
06/09/2019 17:06:58 - INFO - run_child_finetuning -     eval_loss = 1.0509962290525436
06/09/2019 17:06:58 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 17:07:31 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:07:31 - INFO - run_child_finetuning -     eval_accuracy = 0.5037760416666667
06/09/2019 17:07:31 - INFO - run_child_finetuning -     eval_loss = 1.0441355774799983
Epoch:  17%|█▋        | 1/6 [01:45<08:48, 105.72s/it]06/09/2019 17:08:11 - INFO - run_child_finetuning -   Epoch 2
06/09/2019 17:08:11 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 17:08:44 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:08:44 - INFO - run_child_finetuning -     eval_accuracy = 1.0
06/09/2019 17:08:44 - INFO - run_child_finetuning -     eval_loss = 7.717162370681762e-05
06/09/2019 17:08:44 - INFO - run_child_finetuning -   Evaluating on valid set...
06/09/2019 17:09:16 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:09:16 - INFO - run_child_finetuning -     eval_accuracy = 1.0
06/09/2019 17:09:16 - INFO - run_child_finetuning -     eval_loss = 7.584964235623678e-05
Epoch:  33%|███▎      | 2/6 [03:31<07:02, 105.71s/it]06/09/2019 17:09:57 - INFO - run_child_finetuning -   Epoch 3
06/09/2019 17:09:57 - INFO - run_child_finetuning -   Evaluating on train set...
06/09/2019 17:10:29 - INFO - run_child_finetuning -   ***** Eval results *****
06/09/2019 17:10:29 - INFO - run_child_finetuning -     eval_accuracy = 1.0
06/09/2019 17:10:29 - INFO - run_child_finetuning -     eval_loss = 1.903076966603597e-05
06/09/2019 17:10:29 - INFO - run_child_finetuning -   Evaluating on valid set...
